{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c231d636",
      "metadata": {
        "id": "c231d636",
        "outputId": "f9642ea1-3a67-4592-fb97-e22e873f37e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.3.1\n",
            "Ray version: 2.9.3\n",
            "NumPy version: 1.26.4\n",
            "Pandas version: 2.3.0\n",
            "Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Ray and RLlib\n",
        "import ray\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.tune.registry import register_env\n",
        "from gymnasium import spaces\n",
        "\n",
        "# Data processing and ML\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import joblib\n",
        "import numba as nb\n",
        "from numba import jit, prange\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Ray version: {ray.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(\"Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25685819",
      "metadata": {
        "id": "25685819",
        "outputId": "b817c374-72cf-483c-a896-84c11d62135f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded data from: C:\\Users\\Windows 11\\Downloads\\ES - 30 min - New.csv\n",
            "Timestamp processed and set as index.\n",
            "\n",
            "Calculating MMD scores and labeling regimes (Optimized)...\n",
            "Estimated kernel bandwidth (sigma): 1.0546\n",
            "--- Calculation finished in 76.69 seconds ---\n",
            "\n",
            "Processed data shape: (59056, 16)\n",
            "Date range: 2020-01-07 03:00:00 to 2025-12-05 23:30:00\n",
            "\n",
            "Regime distribution (counts):\n",
            "regime_name\n",
            "Bearish                     4299\n",
            "Bullish                     5981\n",
            "Extremely Bearish            245\n",
            "Extremely Bullish            745\n",
            "Neutral                    17499\n",
            "Neutral Towards Bearish    17358\n",
            "Neutral Towards Bullish    11929\n",
            "Unassigned                  1000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample of processed data (tail):\n",
            "                        open     high      low    close  volume   returns  \\\n",
            "timestamp                                                                   \n",
            "2025-12-05 21:30:00  5856.00  5862.25  5851.00  5851.50   56644 -0.000768   \n",
            "2025-12-05 22:00:00  5851.75  5858.25  5847.50  5851.00   63818 -0.000085   \n",
            "2025-12-05 22:30:00  5851.00  5866.25  5844.00  5865.00  198330  0.002393   \n",
            "2025-12-05 23:00:00  5864.25  5876.25  5863.75  5867.25   71011  0.000384   \n",
            "2025-12-05 23:30:00  5867.25  5869.25  5866.00  5868.25    7059  0.000170   \n",
            "\n",
            "                     log_returns     range  volume_ma  volatility  \\\n",
            "timestamp                                                           \n",
            "2025-12-05 21:30:00    -0.000769  0.001921   55692.10    0.195911   \n",
            "2025-12-05 22:00:00    -0.000085  0.001837   58334.00    0.195002   \n",
            "2025-12-05 22:30:00     0.002390  0.003803   67779.65    0.202626   \n",
            "2025-12-05 23:00:00     0.000384  0.002131   70590.15    0.200349   \n",
            "2025-12-05 23:30:00     0.000170  0.000554   69472.25    0.194903   \n",
            "\n",
            "                     momentum_20  momentum_50  volume_ratio  mmd_score  \\\n",
            "timestamp                                                                \n",
            "2025-12-05 21:30:00     0.003645     0.044072      1.017092   0.135378   \n",
            "2025-12-05 22:00:00     0.002613     0.039623      1.094010   0.135378   \n",
            "2025-12-05 22:30:00     0.004453     0.048492      2.926100   0.135378   \n",
            "2025-12-05 23:00:00     0.003420     0.044738      1.005962   0.135378   \n",
            "2025-12-05 23:30:00     0.001622     0.043059      0.101609   0.135378   \n",
            "\n",
            "                     regime              regime_name  \n",
            "timestamp                                             \n",
            "2025-12-05 21:30:00       4  Neutral Towards Bullish  \n",
            "2025-12-05 22:00:00       4  Neutral Towards Bullish  \n",
            "2025-12-05 22:30:00       4  Neutral Towards Bullish  \n",
            "2025-12-05 23:00:00       4  Neutral Towards Bullish  \n",
            "2025-12-05 23:30:00       4  Neutral Towards Bullish  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "from numba import prange\n",
        "import time\n",
        "\n",
        "# --- OPTIMIZATION 1: Numba-JIT function for faster sigma estimation ---\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def _compute_dists_sq_numba(data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Optimized calculation of pairwise squared Euclidean distances.\n",
        "    This replaces a slow Python loop.\n",
        "    \"\"\"\n",
        "    n_samples = data.shape[0]\n",
        "    # Pre-allocate array for results\n",
        "    dists_sq = np.zeros(n_samples * (n_samples - 1) // 2, dtype=np.float64)\n",
        "    k = 0\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(i + 1, n_samples):\n",
        "            diff = data[i] - data[j]\n",
        "            dists_sq[k] = np.sum(diff * diff)\n",
        "            k += 1\n",
        "    return dists_sq\n",
        "\n",
        "# Core MMD math (already optimized with Numba, no changes needed)\n",
        "@nb.jit(nopython=True)\n",
        "def gaussian_kernel_numba(x: np.ndarray, y: np.ndarray, sigma: float) -> float:\n",
        "    diff = x - y\n",
        "    return np.exp(-np.sum(diff * diff) / (2.0 * sigma * sigma))\n",
        "\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def compute_mmd_squared_numba(X: np.ndarray, Y: np.ndarray, sigma: float) -> float:\n",
        "    n_x, n_y = X.shape[0], Y.shape[0]\n",
        "    K_XX_sum = 0.0\n",
        "    if n_x > 1:\n",
        "        for i in prange(n_x):\n",
        "            for j in range(n_x):\n",
        "                if i != j: K_XX_sum += gaussian_kernel_numba(X[i], X[j], sigma)\n",
        "        K_XX = K_XX_sum / (n_x * (n_x - 1))\n",
        "    else: K_XX = 0.0\n",
        "    K_YY_sum = 0.0\n",
        "    if n_y > 1:\n",
        "        for i in prange(n_y):\n",
        "            for j in range(n_y):\n",
        "                if i != j: K_YY_sum += gaussian_kernel_numba(Y[i], Y[j], sigma)\n",
        "        K_YY = K_YY_sum / (n_y * (n_y - 1))\n",
        "    else: K_YY = 0.0\n",
        "    K_XY_sum = 0.0\n",
        "    if n_x > 0 and n_y > 0:\n",
        "        for i in prange(n_x):\n",
        "            for j in range(n_y):\n",
        "                K_XY_sum += gaussian_kernel_numba(X[i], Y[j], sigma)\n",
        "        K_XY = K_XY_sum / (n_x * n_y)\n",
        "    else: K_XY = 0.0\n",
        "    return max(0.0, K_XX + K_YY - 2.0 * K_XY)\n",
        "\n",
        "def calculate_mmd_and_label_regimes(df: pd.DataFrame, reference_window: int = 500, test_window: int = 100, stride: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"Calculate MMD scores and assign regime labels\"\"\"\n",
        "    df_processed = df.copy()\n",
        "    df_processed.columns = [col.strip().lower() for col in df_processed.columns]\n",
        "\n",
        "    # Feature engineering (already fast using pandas' vectorized operations)\n",
        "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "    for col in required_cols:\n",
        "        if col not in df_processed.columns: raise ValueError(f\"Missing required column: {col}\")\n",
        "        if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "\n",
        "    df_processed['returns'] = df_processed['close'].pct_change()\n",
        "    df_processed['log_returns'] = np.log(df_processed['close'] / df_processed['close'].shift(1))\n",
        "    df_processed['range'] = (df_processed['high'] - df_processed['low']) / df_processed['close'].shift(1)\n",
        "    df_processed['volume_ma'] = df_processed['volume'].rolling(window=20, min_periods=1).mean()\n",
        "    df_processed['volatility'] = df_processed['returns'].rolling(window=20, min_periods=1).std() * np.sqrt(252 * 48)\n",
        "    df_processed['momentum_20'] = df_processed['close'].pct_change(periods=20)\n",
        "    df_processed['momentum_50'] = df_processed['close'].pct_change(periods=50)\n",
        "    df_processed['volume_ratio'] = df_processed['volume'] / (df_processed['volume_ma'] + 1e-9)\n",
        "    df_processed = df_processed.dropna().copy()\n",
        "\n",
        "    if df_processed.empty:\n",
        "        print(\"Warning: DataFrame empty after feature calculation.\")\n",
        "        return df_processed\n",
        "\n",
        "    features_for_mmd = ['returns', 'log_returns', 'range', 'volatility']\n",
        "    data = df_processed[features_for_mmd].values\n",
        "\n",
        "    mmd_scores, timestamps = [], []\n",
        "\n",
        "    if len(data) >= reference_window + test_window:\n",
        "        ref_data_full = data[:reference_window]\n",
        "        ref_mean, ref_std = np.mean(ref_data_full, axis=0), np.std(ref_data_full, axis=0)\n",
        "        ref_std[ref_std < 1e-8] = 1e-8\n",
        "        ref_data_norm = (ref_data_full - ref_mean) / ref_std\n",
        "\n",
        "        # --- Sigma estimation now uses the optimized Numba function ---\n",
        "        n_samples = min(200, ref_data_norm.shape[0])\n",
        "        sigma = 1.0\n",
        "        if n_samples > 1:\n",
        "            sample_indices = np.random.choice(ref_data_norm.shape[0], n_samples, replace=False)\n",
        "            sampled_data = ref_data_norm[sample_indices]\n",
        "            dists_sq = _compute_dists_sq_numba(sampled_data)\n",
        "            if dists_sq.size > 0:\n",
        "                median_dist_sq = np.median(dists_sq)\n",
        "                if median_dist_sq > 0:\n",
        "                    sigma = np.sqrt(median_dist_sq)\n",
        "\n",
        "        print(f\"Estimated kernel bandwidth (sigma): {sigma:.4f}\")\n",
        "\n",
        "        for i in range(reference_window, len(data) - test_window + 1, stride):\n",
        "            test_data_norm = (data[i:i+test_window] - ref_mean) / ref_std\n",
        "            mmd = np.sqrt(compute_mmd_squared_numba(ref_data_norm, test_data_norm, sigma))\n",
        "            mmd_scores.append(mmd)\n",
        "            timestamps.append(df_processed.index[i + test_window - 1])\n",
        "\n",
        "    if mmd_scores:\n",
        "        mmd_series = pd.Series(mmd_scores, index=pd.DatetimeIndex(timestamps), name='mmd_score')\n",
        "        df_processed['mmd_score'] = mmd_series.reindex(df_processed.index).ffill().bfill()\n",
        "    else:\n",
        "        df_processed['mmd_score'] = np.nan\n",
        "\n",
        "    # --- Regime labeling now uses the highly optimized Numba implementation ---\n",
        "    df_processed = assign_regime_labels_optimized(df_processed)\n",
        "    return df_processed\n",
        "\n",
        "# --- OPTIMIZATION 2: Numba-JIT function for the entire regime labeling loop ---\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def _assign_regime_labels_numba(mmd_scores: np.ndarray, momentum_scores: np.ndarray, lookback_window: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Numba-optimized regime labeling loop. Replaces a very slow pandas iloc loop.\n",
        "    Operates entirely on NumPy arrays for maximum speed.\n",
        "    \"\"\"\n",
        "    n = len(mmd_scores)\n",
        "    regimes = np.full(n, -1, dtype=np.int64)\n",
        "\n",
        "    for i in prange(lookback_window, n):\n",
        "        current_mmd, current_momentum = mmd_scores[i], momentum_scores[i]\n",
        "        if np.isnan(current_mmd) or np.isnan(current_momentum): continue\n",
        "\n",
        "        hist_window_mmd = mmd_scores[i - lookback_window : i]\n",
        "        hist_window_mom = momentum_scores[i - lookback_window : i]\n",
        "\n",
        "        mmd_le_count, mmd_valid_count = 0, 0\n",
        "        for val in hist_window_mmd:\n",
        "            if not np.isnan(val):\n",
        "                mmd_valid_count += 1\n",
        "                if val <= current_mmd: mmd_le_count += 1\n",
        "\n",
        "        mom_le_count, mom_valid_count = 0, 0\n",
        "        for val in hist_window_mom:\n",
        "            if not np.isnan(val):\n",
        "                mom_valid_count += 1\n",
        "                if val <= current_momentum: mom_le_count += 1\n",
        "\n",
        "        if mmd_valid_count == 0 or mom_valid_count == 0: continue\n",
        "\n",
        "        mmd_p = mmd_le_count / mmd_valid_count\n",
        "        mom_p = mom_le_count / mom_valid_count\n",
        "        score = 0.4 * (1.0 - mmd_p) + 0.6 * mom_p\n",
        "\n",
        "        if score >= 0.95: regime = 6\n",
        "        elif score >= 0.80: regime = 5\n",
        "        elif score >= 0.60: regime = 4\n",
        "        elif score >= 0.40: regime = 3\n",
        "        elif score >= 0.20: regime = 2\n",
        "        elif score >= 0.05: regime = 1\n",
        "        else: regime = 0\n",
        "        regimes[i] = regime\n",
        "\n",
        "    return regimes\n",
        "\n",
        "def assign_regime_labels_optimized(df: pd.DataFrame, lookback_window: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"Wrapper function for the optimized regime labeling.\"\"\"\n",
        "    df_labeled = df.copy()\n",
        "    if 'mmd_score' not in df_labeled.columns or 'momentum_20' not in df_labeled.columns:\n",
        "        df_labeled['regime_name'] = \"Unassigned\"\n",
        "        return df_labeled\n",
        "\n",
        "    # Extract NumPy arrays to pass to the Numba function\n",
        "    mmd_scores = df_labeled['mmd_score'].to_numpy()\n",
        "    momentum_scores = df_labeled['momentum_20'].to_numpy()\n",
        "\n",
        "    # Call the fast, JIT-compiled function\n",
        "    regimes = _assign_regime_labels_numba(mmd_scores, momentum_scores, lookback_window)\n",
        "\n",
        "    df_labeled['regime'] = regimes\n",
        "\n",
        "    regime_map = {-1: 'Unassigned', 0: 'Extremely Bearish', 1: 'Bearish', 2: 'Neutral Towards Bearish', 3: 'Neutral', 4: 'Neutral Towards Bullish', 5: 'Bullish', 6: 'Extremely Bullish'}\n",
        "    df_labeled['regime_name'] = df_labeled['regime'].map(regime_map).fillna('Unassigned')\n",
        "    return df_labeled\n",
        "\n",
        "# --- Main script execution (no changes needed here) ---\n",
        "if __name__ == '__main__':\n",
        "    df_prices = None\n",
        "    file_path = r'C:\\Users\\Windows 11\\Downloads\\ES - 30 min - New.csv'\n",
        "\n",
        "    try:\n",
        "        df_prices_raw = pd.read_csv(file_path)\n",
        "        print(f\"Successfully loaded data from: {file_path}\")\n",
        "        df_prices_raw.columns = [col.strip().lower() for col in df_prices_raw.columns]\n",
        "        if 'timestamp' not in df_prices_raw.columns:\n",
        "            raise ValueError(\"'timestamp' column not found.\")\n",
        "        df_prices_raw['timestamp'] = pd.to_datetime(df_prices_raw['timestamp'], format='mixed')\n",
        "        df_prices = df_prices_raw.set_index('timestamp').sort_index()\n",
        "        print(\"Timestamp processed and set as index.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data loading: {e}\")\n",
        "\n",
        "    if df_prices is not None:\n",
        "        print(\"\\nCalculating MMD scores and labeling regimes (Optimized)...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        df_features = calculate_mmd_and_label_regimes(df_prices)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"--- Calculation finished in {end_time - start_time:.2f} seconds ---\")\n",
        "\n",
        "        print(f\"\\nProcessed data shape: {df_features.shape}\")\n",
        "        if not df_features.empty:\n",
        "            print(f\"Date range: {df_features.index[0]} to {df_features.index[-1]}\")\n",
        "            print(f\"\\nRegime distribution (counts):\\n{df_features['regime_name'].value_counts().sort_index()}\")\n",
        "            print(\"\\nSample of processed data (tail):\")\n",
        "            print(df_features.tail())\n",
        "        else:\n",
        "            print(\"Processed DataFrame is empty.\")\n",
        "    else:\n",
        "        print(\"\\nSkipping MMD calculation due to errors during data loading.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cab222",
      "metadata": {
        "id": "a9cab222",
        "outputId": "c383dc5c-308a-41ed-ac9c-dec3c51fce26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input df_features shape: (59056, 16)\n",
            "\n",
            "Using 13 available features for modeling: ['open', 'high', 'low', 'close', 'volume', 'returns', 'log_returns', 'range', 'volatility', 'momentum_20', 'momentum_50', 'volume_ratio', 'mmd_score']\n",
            "\n",
            "Data split sizes:\n",
            "Train: 41339 samples (from 2020-01-07 03:00:00 to 2023-11-21 04:30:00)\n",
            "Val:   8858 samples (from 2023-11-21 05:00:00 to 2024-08-23 06:30:00)\n",
            "Test:  8859 samples (from 2024-08-23 07:00:00 to 2025-12-05 23:30:00)\n",
            "\n",
            "Normalization parameters calculated and saved from training data.\n",
            "Features normalized: 13\n",
            "Validation set features normalized.\n",
            "Test set features normalized.\n",
            "\n",
            "Sample of processed training data (first 5 rows):\n",
            "                         open      high       low     close    volume  \\\n",
            "timestamp                                                               \n",
            "2020-01-07 03:00:00 -2.375460 -2.384414 -2.370335 -2.373894 -0.519922   \n",
            "2020-01-07 03:30:00 -2.373646 -2.375340 -2.363088 -2.370267 -0.559769   \n",
            "2020-01-07 04:00:00 -2.369415 -2.372920 -2.364900 -2.373894 -0.532420   \n",
            "2020-01-07 04:30:00 -2.373042 -2.372920 -2.364900 -2.366641 -0.560746   \n",
            "2020-01-07 05:00:00 -2.366393 -2.375340 -2.360673 -2.371476 -0.601211   \n",
            "\n",
            "                      returns  log_returns     range  volatility  momentum_20  \\\n",
            "timestamp                                                                       \n",
            "2020-01-07 03:00:00  0.030479     0.033747 -0.387899   -0.140425     0.519045   \n",
            "2020-01-07 03:30:00  0.065786     0.068840 -0.255706   -0.143032     0.466669   \n",
            "2020-01-07 04:00:00 -0.075441    -0.071583  0.052592   -0.147356     0.363127   \n",
            "2020-01-07 04:30:00  0.136417     0.139017  0.053171   -0.147343     0.387246   \n",
            "2020-01-07 05:00:00 -0.098927    -0.094949 -0.432893   -0.154815     0.425719   \n",
            "\n",
            "                     momentum_50  volume_ratio  mmd_score  regime  \n",
            "timestamp                                                          \n",
            "2020-01-07 03:00:00     0.410341     -0.703818  -0.451408      -1  \n",
            "2020-01-07 03:30:00     0.413718     -0.718408  -0.451408      -1  \n",
            "2020-01-07 04:00:00     0.401346     -0.680132  -0.451408      -1  \n",
            "2020-01-07 04:30:00     0.388348     -0.694820  -0.451408      -1  \n",
            "2020-01-07 05:00:00     0.306350     -0.725969  -0.451408      -1  \n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Data Splitting and Feature Normalization\n",
        "# -------------------------------------------------\n",
        "# This cell takes the feature-engineered DataFrame (df_features),\n",
        "# splits it into training, validation, and test sets,\n",
        "# and then normalizes the features based on the training set statistics.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler # Ensure this import is present\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define feature columns for the model\n",
        "# These are the columns that will be used as input to the ML model.\n",
        "feature_columns = [\n",
        "    'open', 'high', 'low', 'close', 'volume', # Basic OHLCV\n",
        "    'returns', 'log_returns', 'range', 'volatility', # Derived financial indicators\n",
        "    'momentum_20', 'momentum_50', # Momentum indicators\n",
        "    'volume_ratio', # Volume-based feature\n",
        "    'mmd_score' # MMD-based feature\n",
        "]\n",
        "target_column = 'regime' # The column we want to predict\n",
        "\n",
        "# --- Input Data Check ---\n",
        "# Ensure df_features is defined and not empty (should come from the previous cell)\n",
        "if 'df_features' not in locals() or df_features.empty:\n",
        "    print(\"Error: df_features is not defined or is empty. Please run the previous cells to generate it.\")\n",
        "    # In a notebook, you might raise an error or stop execution here\n",
        "    # For example: raise ValueError(\"df_features not available. Run previous cells.\")\n",
        "else:\n",
        "    print(f\"Input df_features shape: {df_features.shape}\")\n",
        "\n",
        "    # --- Feature Selection ---\n",
        "    # Ensure all defined feature_columns actually exist in df_features\n",
        "    # This also handles cases where some features might not have been calculable (e.g., due to insufficient data)\n",
        "    available_features = [col for col in feature_columns if col in df_features.columns]\n",
        "    missing_features = [col for col in feature_columns if col not in df_features.columns]\n",
        "\n",
        "    if missing_features:\n",
        "        print(f\"\\nWarning: The following defined feature_columns are missing from df_features and will be excluded:\")\n",
        "        for mf in missing_features:\n",
        "            print(f\" - {mf}\")\n",
        "\n",
        "    if not available_features:\n",
        "        raise ValueError(\"No features available for training. Check feature_columns and df_features content.\")\n",
        "\n",
        "    print(f\"\\nUsing {len(available_features)} available features for modeling: {available_features}\")\n",
        "    if target_column not in df_features.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in df_features.\")\n",
        "\n",
        "    # --- Data Splitting (Chronological) ---\n",
        "    # Splitting data chronologically is crucial for time-series forecasting tasks\n",
        "    # to prevent data leakage from the future into the training set.\n",
        "    n = len(df_features)\n",
        "    if n < 10: # Arbitrary small number, adjust as needed\n",
        "        raise ValueError(f\"Insufficient data for splitting. Only {n} samples available.\")\n",
        "\n",
        "    train_end_idx = int(n * 0.70)\n",
        "    val_end_idx = int(n * 0.85)\n",
        "\n",
        "    # Use .copy() to avoid SettingWithCopyWarning later if modifications are made\n",
        "    train_df = df_features.iloc[:train_end_idx].copy()\n",
        "    val_df = df_features.iloc[train_end_idx:val_end_idx].copy()\n",
        "    test_df = df_features.iloc[val_end_idx:].copy()\n",
        "\n",
        "    print(f\"\\nData split sizes:\")\n",
        "    if not train_df.empty:\n",
        "        print(f\"Train: {len(train_df)} samples (from {train_df.index[0]} to {train_df.index[-1]})\")\n",
        "    else:\n",
        "        print(\"Train: 0 samples (Warning: Training set is empty!)\")\n",
        "\n",
        "    if not val_df.empty:\n",
        "        print(f\"Val:   {len(val_df)} samples (from {val_df.index[0]} to {val_df.index[-1]})\")\n",
        "    else:\n",
        "        print(\"Val:   0 samples (Warning: Validation set is empty!)\")\n",
        "\n",
        "    if not test_df.empty:\n",
        "        print(f\"Test:  {len(test_df)} samples (from {test_df.index[0]} to {test_df.index[-1]})\")\n",
        "    else:\n",
        "        print(\"Test:  0 samples (Warning: Test set is empty!)\")\n",
        "\n",
        "    # --- Feature Normalization ---\n",
        "    # StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
        "    # It's important to fit the scaler ONLY on the training data and then use it to transform\n",
        "    # the validation and test sets to prevent data leakage.\n",
        "\n",
        "    # Handle potential NaNs in features before scaling.\n",
        "    # Using fillna(0) is a simple strategy. Consider more sophisticated imputation\n",
        "    # if zeros are not appropriate (e.g., mean/median from training set).\n",
        "    # Ensure this fillna strategy is consistent with how you'd handle NaNs in production.\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    if not train_df.empty:\n",
        "        # Fill NaNs before fitting and transforming.\n",
        "        # Make sure 'available_features' and 'target_column' are present in train_df\n",
        "        train_features_df = train_df[available_features].fillna(0)\n",
        "        train_df[available_features] = scaler.fit_transform(train_features_df)\n",
        "\n",
        "        # Save normalization parameters (mean, std_dev) for each feature from the training set.\n",
        "        # These are crucial for transforming new data in production using the same scale.\n",
        "        normalization_params = {\n",
        "            'mean': scaler.mean_.tolist(), # Convert to list for JSON serializability if needed\n",
        "            'std': scaler.scale_.tolist(),   # Convert to list\n",
        "            'features': available_features # List of features that were scaled\n",
        "        }\n",
        "        print(\"\\nNormalization parameters calculated and saved from training data.\")\n",
        "        print(f\"Features normalized: {len(available_features)}\")\n",
        "\n",
        "        # Transform validation and test sets using the SAME scaler fitted on the training data.\n",
        "        if not val_df.empty:\n",
        "            val_features_df = val_df[available_features].fillna(0)\n",
        "            val_df[available_features] = scaler.transform(val_features_df)\n",
        "            print(\"Validation set features normalized.\")\n",
        "\n",
        "        if not test_df.empty:\n",
        "            test_features_df = test_df[available_features].fillna(0)\n",
        "            test_df[available_features] = scaler.transform(test_features_df)\n",
        "            print(\"Test set features normalized.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nWarning: Training set is empty. Skipping normalization.\")\n",
        "        normalization_params = None # Or handle as appropriate\n",
        "\n",
        "    # --- Display sample of processed data (optional) ---\n",
        "    if not train_df.empty:\n",
        "        print(\"\\nSample of processed training data (first 5 rows):\")\n",
        "        print(train_df[available_features + [target_column]].head())\n",
        "\n",
        "    # The DataFrames (train_df, val_df, test_df) and normalization_params\n",
        "    # are now ready for use in subsequent model training cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247ed8cd",
      "metadata": {
        "id": "247ed8cd",
        "outputId": "ac4640bc-27c3-40dd-aa87-d2e32027554c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MarketDataEncoder instantiated successfully with input_size=13, encoding_size=128.\n",
            "Encoder test - Input shape: torch.Size([32, 30, 13]), Output shape: torch.Size([32, 128])\n",
            "Encoder output shape is correct.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Market Data Encoder Definition\n",
        "# ---------------------------------------\n",
        "# This cell defines a PyTorch nn.Module that serves as a standalone encoder.\n",
        "# It takes a window of market data (with multiple features) and transforms it\n",
        "# into a fixed-size vector representation. This encoded vector can then be\n",
        "# used as input for downstream reinforcement learning agents or other models.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Ensure 'available_features' is defined from a previous cell.\n",
        "# If not, this cell will raise a NameError when 'test_encoder' is initialized.\n",
        "# You might want to add an explicit check here if running cells out of order:\n",
        "# if 'available_features' not in locals():\n",
        "#     raise NameError(\"Variable 'available_features' is not defined. Please run the data preparation cells first.\")\n",
        "\n",
        "class MarketDataEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Standalone encoder that transforms market data windows into fixed-size vectors.\n",
        "    This promotes modularity and enables end-to-end learning of representations.\n",
        "\n",
        "    The architecture uses an LSTM to capture temporal dependencies in the input window,\n",
        "    followed by fully connected layers to project the LSTM's output into the desired\n",
        "    encoding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, encoding_size=64, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of features in the input data (e.g., OHLCV + indicators).\n",
        "            hidden_size (int): Number of features in the LSTM hidden state.\n",
        "            num_layers (int): Number of recurrent layers in the LSTM.\n",
        "            encoding_size (int): Dimensionality of the output encoded vector.\n",
        "            dropout_rate (float): Dropout probability for regularization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.encoding_size = encoding_size\n",
        "\n",
        "        # LSTM layer for temporal pattern extraction from sequential data.\n",
        "        # batch_first=True means input/output tensors are (batch, seq, feature).\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0 # Dropout only applied if num_layers > 1\n",
        "        )\n",
        "\n",
        "        # Projection layers to transform LSTM output to the final encoding size.\n",
        "        # Using a two-layer MLP for potentially more complex transformations.\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2) # Intermediate layer\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, encoding_size) # Output layer\n",
        "\n",
        "        # Activation function and dropout for regularization\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, window_size, num_features).\n",
        "                              - batch_size: Number of samples in the batch.\n",
        "                              - window_size: Length of the input sequence (time steps).\n",
        "                              - num_features: Number of features at each time step.\n",
        "\n",
        "        Returns:\n",
        "            encoded_vector (torch.Tensor): Output tensor of shape (batch_size, encoding_size).\n",
        "        \"\"\"\n",
        "        # LSTM encoding:\n",
        "        # lstm_out shape: (batch_size, window_size, hidden_size)\n",
        "        # h_n shape: (num_layers, batch_size, hidden_size) - final hidden state for each layer\n",
        "        # c_n shape: (num_layers, batch_size, hidden_size) - final cell state for each layer\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        # We typically use the output from the last time step of the LSTM sequence.\n",
        "        # lstm_out[:, -1, :] gives the hidden state of the last LSTM unit for all batches.\n",
        "        # This captures the summary of the entire sequence.\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "\n",
        "        # Alternatively, one could use h_n[-1] (hidden state of the last layer at the last time step)\n",
        "        # if num_layers > 1, which should be equivalent to lstm_out[:, -1, :] for the last layer.\n",
        "        # last_hidden_state = h_n[-1]\n",
        "\n",
        "        # Project the last hidden state to the final encoding space\n",
        "        x = self.relu(self.fc1(last_hidden_state))\n",
        "        x = self.dropout(x) # Apply dropout after activation\n",
        "        encoded_vector = self.fc2(x)\n",
        "\n",
        "        return encoded_vector\n",
        "\n",
        "# --- Test the Encoder ---\n",
        "# This section demonstrates how to instantiate and use the MarketDataEncoder.\n",
        "# It's a good practice to test modules with sample data to verify shapes and basic functionality.\n",
        "\n",
        "# Check if 'available_features' is defined to prevent NameError\n",
        "if 'available_features' in locals():\n",
        "    # Configuration for the test\n",
        "    test_input_features = len(available_features) # Should match the number of features from data prep\n",
        "    test_encoding_dims = 128 # Desired output dimension for the encoded vector\n",
        "\n",
        "    # Instantiate the encoder\n",
        "    try:\n",
        "        test_encoder = MarketDataEncoder(input_size=test_input_features, encoding_size=test_encoding_dims)\n",
        "        print(f\"\\nMarketDataEncoder instantiated successfully with input_size={test_input_features}, encoding_size={test_encoding_dims}.\")\n",
        "\n",
        "        # Create a dummy input tensor for testing\n",
        "        # Shape: (batch_size, window_size, num_features)\n",
        "        batch_size = 32\n",
        "        window_size = 30 # Example window length (e.g., 30 time steps)\n",
        "\n",
        "        # Ensure dummy input has the correct number of features\n",
        "        dummy_input_tensor = torch.randn(batch_size, window_size, test_input_features)\n",
        "\n",
        "        # Pass the dummy input through the encoder\n",
        "        test_output_vector = test_encoder(dummy_input_tensor)\n",
        "\n",
        "        print(f\"Encoder test - Input shape: {dummy_input_tensor.shape}, Output shape: {test_output_vector.shape}\")\n",
        "\n",
        "        # Verify output shape\n",
        "        expected_output_shape = (batch_size, test_encoding_dims)\n",
        "        assert test_output_vector.shape == expected_output_shape, \\\n",
        "            f\"Output shape mismatch! Expected {expected_output_shape}, got {test_output_vector.shape}\"\n",
        "        print(\"Encoder output shape is correct.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during MarketDataEncoder test: {e}\")\n",
        "else:\n",
        "    print(\"\\nWarning: 'available_features' not found. Skipping MarketDataEncoder test. \"\n",
        "          \"Please ensure previous data preparation cells have been run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4fee226",
      "metadata": {
        "id": "c4fee226",
        "outputId": "de4e6002-2790-46b4-990a-b327e83cf2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EncoderRLModel class defined. Ready for registration and use with RLlib.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: RLlib Custom Model with MarketDataEncoder\n",
        "# -------------------------------------------------\n",
        "# This cell defines a custom model for RLlib that incorporates the\n",
        "# MarketDataEncoder defined previously.\n",
        "# The model takes a window of market data as input, uses the encoder\n",
        "# to get a fixed-size representation, and then feeds this representation\n",
        "# into separate fully connected layers for policy (action selection)\n",
        "# and value estimation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # Added for F.relu\n",
        "import gymnasium as gym # <--- CORRECTED: Import gymnasium\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.models.modelv2 import ModelV2\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.typing import Dict, TensorType, List, ModelConfigDict\n",
        "\n",
        "# Ensure MarketDataEncoder is available from the previous cell.\n",
        "# If 'MarketDataEncoder' is not defined, this will raise a NameError.\n",
        "# Make sure the cell defining MarketDataEncoder has been executed.\n",
        "# Example:\n",
        "# if 'MarketDataEncoder' not in globals():\n",
        "#     raise NameError(\"MarketDataEncoder not found. Please run the cell defining it.\")\n",
        "\n",
        "\n",
        "class EncoderRLModel(TorchModelV2, nn.Module):\n",
        "    \"\"\"\n",
        "    RLlib TorchModelV2 that uses a MarketDataEncoder for observation processing.\n",
        "\n",
        "    Assumes observations are windows of shape (window_size, num_features).\n",
        "    The MarketDataEncoder processes this window into a fixed-size encoding,\n",
        "    which is then used by policy and value heads.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 obs_space: gym.spaces.Space, # Now gym.spaces.Space is recognized\n",
        "                 action_space: gym.spaces.Space,\n",
        "                 num_outputs: int,\n",
        "                 model_config: ModelConfigDict,\n",
        "                 name: str):\n",
        "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        if not isinstance(obs_space, gym.spaces.Box) or len(obs_space.shape) != 2:\n",
        "            raise ValueError(\n",
        "                f\"This model expects a 2D Box observation space (window_size, num_features). \"\n",
        "                f\"Got {obs_space} with shape {obs_space.shape if hasattr(obs_space, 'shape') else 'N/A'}\"\n",
        "            )\n",
        "\n",
        "        self.window_size = obs_space.shape[0]\n",
        "        self.num_features = obs_space.shape[1]\n",
        "\n",
        "        custom_cfg = model_config.get(\"custom_model_config\", {})\n",
        "        encoder_hidden_size = custom_cfg.get(\"encoder_hidden_size\", 128)\n",
        "        encoder_num_layers = custom_cfg.get(\"encoder_num_layers\", 2)\n",
        "        self.encoder_output_size = custom_cfg.get(\"encoder_output_size\", 64)\n",
        "        encoder_dropout_rate = custom_cfg.get(\"encoder_dropout_rate\", 0.1)\n",
        "\n",
        "        # Assuming MarketDataEncoder is defined in the global scope from a previous cell\n",
        "        self.encoder = MarketDataEncoder(\n",
        "            input_size=self.num_features,\n",
        "            hidden_size=encoder_hidden_size,\n",
        "            num_layers=encoder_num_layers,\n",
        "            encoding_size=self.encoder_output_size,\n",
        "            dropout_rate=encoder_dropout_rate # Ensure MarketDataEncoder uses this param name\n",
        "        )\n",
        "\n",
        "        policy_fc_size = custom_cfg.get(\"policy_fc_size\", self.encoder_output_size)\n",
        "        self.policy_fc = nn.Linear(self.encoder_output_size, policy_fc_size)\n",
        "        self.policy_logits = nn.Linear(policy_fc_size, num_outputs)\n",
        "\n",
        "        value_fc_size = custom_cfg.get(\"value_fc_size\", self.encoder_output_size)\n",
        "        self.value_fc = nn.Linear(self.encoder_output_size, value_fc_size)\n",
        "        self.value_output = nn.Linear(value_fc_size, 1)\n",
        "\n",
        "        self._last_encoded_features = None\n",
        "\n",
        "        print(f\"EncoderRLModel initialized: obs_space.shape=({self.window_size}, {self.num_features}), \"\n",
        "              f\"action_space.shape={action_space.shape if hasattr(action_space, 'shape') else 'N/A'}, num_outputs={num_outputs}\")\n",
        "        print(f\"MarketDataEncoder params: input_size={self.num_features}, hidden_size={encoder_hidden_size}, \"\n",
        "              f\"num_layers={encoder_num_layers}, encoding_size={self.encoder_output_size}\")\n",
        "\n",
        "    @override(ModelV2)\n",
        "    def forward(self,\n",
        "                input_dict: Dict[str, TensorType],\n",
        "                state: List[TensorType],\n",
        "                seq_lens: TensorType) -> (TensorType, List[TensorType]):\n",
        "\n",
        "        obs = input_dict[\"obs\"]\n",
        "\n",
        "        if len(obs.shape) == 2:\n",
        "            batch_size = obs.shape[0]\n",
        "            obs = obs.reshape(batch_size, self.window_size, self.num_features)\n",
        "        elif len(obs.shape) == 3:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected observation shape: {obs.shape}. \"\n",
        "                             f\"Expected (batch, window_size, num_features) or (batch, window_size * num_features).\")\n",
        "\n",
        "        encoded_features = self.encoder(obs)\n",
        "        self._last_encoded_features = encoded_features\n",
        "\n",
        "        x_policy = F.relu(self.policy_fc(encoded_features)) # Use F.relu\n",
        "        logits = self.policy_logits(x_policy)\n",
        "\n",
        "        return logits, []\n",
        "\n",
        "    @override(ModelV2)\n",
        "    def value_function(self) -> TensorType:\n",
        "        assert self._last_encoded_features is not None, \"Must call forward() first\"\n",
        "        x_value = F.relu(self.value_fc(self._last_encoded_features)) # Use F.relu\n",
        "        value = self.value_output(x_value)\n",
        "        return torch.squeeze(value, -1)\n",
        "\n",
        "\n",
        "# --- How to use this model with RLlib ---\n",
        "# 1. Register the custom model:\n",
        "#    from ray.rllib.models import ModelCatalog\n",
        "#    ModelCatalog.register_custom_model(\"encoder_rl_model\", EncoderRLModel)\n",
        "\n",
        "# 2. Configure your algorithm (e.g., PPO) to use it:\n",
        "#    config = PPOConfig() # Assuming PPOConfig is imported\n",
        "#    config = config.training(\n",
        "#        model={\n",
        "#            \"custom_model\": \"encoder_rl_model\",\n",
        "#            \"custom_model_config\": {\n",
        "#                \"encoder_hidden_size\": 256,\n",
        "#                \"encoder_num_layers\": 2,\n",
        "#                \"encoder_output_size\": 128,\n",
        "#                \"encoder_dropout_rate\": 0.1, # Ensure this matches MarketDataEncoder's param\n",
        "#                \"policy_fc_size\": 128,\n",
        "#                \"value_fc_size\": 128,\n",
        "#            },\n",
        "#        }\n",
        "#    )\n",
        "#    # Ensure your environment's observation_space is correctly defined:\n",
        "#    # e.g., self.observation_space = gym.spaces.Box(\n",
        "#    #           low=-np.inf, high=np.inf, shape=(WINDOW_SIZE, NUM_FEATURES), dtype=np.float32\n",
        "#    #      )\n",
        "\n",
        "print(\"\\nEncoderRLModel class defined. Ready for registration and use with RLlib.\")\n",
        "# Note: PPOConfig would need to be imported, e.g., from ray.rllib.algorithms.ppo import PPOConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c25964",
      "metadata": {
        "id": "d3c25964",
        "outputId": "abb46979-9447-4cfb-b5c2-5f54ebdcc9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiAgentMarketEnv_v0 registered successfully with RLlib.\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Multi-Agent Market Environment Definition\n",
        "# -------------------------------------------------\n",
        "# This cell defines a multi-agent reinforcement learning environment for market\n",
        "# regime prediction. It allows multiple agents to interact with the market data,\n",
        "# receive observations (windows of market features), take actions, and receive rewards.\n",
        "# The environment is designed to be compatible with RLlib's multi-agent API.\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym # Using gymnasium\n",
        "from gymnasium import spaces # For action and observation space definitions\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.tune.registry import register_env\n",
        "\n",
        "# Ensure 'df_features' and 'available_features' are defined from previous cells.\n",
        "# These will be passed into the environment configuration.\n",
        "# Example check:\n",
        "# if 'df_features' not in globals() or 'available_features' not in globals():\n",
        "#     raise NameError(\"Required DataFrames or feature lists are not defined. Please run data preparation cells.\")\n",
        "\n",
        "class MultiAgentMarketEnv(MultiAgentEnv):\n",
        "    \"\"\"\n",
        "    Multi-agent environment for market regime prediction.\n",
        "\n",
        "    Each agent receives a window of market data and must decide on an action\n",
        "    (e.g., conservative or aggressive stance). Rewards are based on how well\n",
        "    the action aligns with the true market regime.\n",
        "\n",
        "    Configuration parameters expected:\n",
        "    - df (pd.DataFrame): The DataFrame containing market data, including features and 'regime' column.\n",
        "    - window_size (int): The number of time steps in each observation window.\n",
        "    - feature_columns (list): A list of column names to be used as features.\n",
        "    - reward_type (str, optional): Specifies the reward calculation logic. Defaults to \"regime_prediction\".\n",
        "    - agent_ids (set, optional): A set of agent IDs. Defaults to {\"agent_0\", \"agent_1\"}.\n",
        "    \"\"\"\n",
        "\n",
        "    # You can define metadata for RLlib if needed, e.g.\n",
        "    # metadata = {'render.modes': ['human'], 'video.frames_per_second': 50}\n",
        "\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = config[\"df\"] # DataFrame with features and 'regime'\n",
        "        self.window_size = config[\"window_size\"]\n",
        "        self.feature_columns = config[\"feature_columns\"]\n",
        "        self.reward_type = config.get(\"reward_type\", \"regime_prediction\")\n",
        "\n",
        "        # Define agent IDs. These must be unique strings.\n",
        "        self._agent_ids = config.get(\"agent_ids\", {\"agent_0\", \"agent_1\"})\n",
        "        if not isinstance(self._agent_ids, set) or not all(isinstance(i, str) for i in self._agent_ids):\n",
        "            raise ValueError(\"agent_ids must be a set of strings.\")\n",
        "\n",
        "        # Environment parameters\n",
        "        self.current_step = 0\n",
        "        # Start step ensures there's enough data for the first window_size\n",
        "        self.start_step = self.window_size - 1\n",
        "        self.end_step = len(self.df) - 1 # Last possible step index\n",
        "\n",
        "        if self.start_step >= self.end_step:\n",
        "            raise ValueError(\n",
        "                f\"DataFrame is too short for the given window_size. \"\n",
        "                f\"Need at least {self.window_size} rows, got {len(self.df)}. \"\n",
        "                f\"Start_step ({self.start_step}) >= end_step ({self.end_step}).\"\n",
        "            )\n",
        "\n",
        "        # Define action and observation spaces (these are per-agent)\n",
        "        # Action: 0 for Conservative, 1 for Aggressive\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Observation: A window of market data\n",
        "        # Shape: (window_size, num_features)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(self.window_size, len(self.feature_columns)),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # For multi-agent, RLlib also expects these to be defined if they vary per agent\n",
        "        # self._obs_space_in_preferred_format = True # If obs spaces are already dicts\n",
        "        # self._action_space_in_preferred_format = True # If action spaces are already dicts\n",
        "        # self.observation_space_dict = {agent_id: self.observation_space for agent_id in self._agent_ids}\n",
        "        # self.action_space_dict = {agent_id: self.action_space for agent_id in self._agent_ids}\n",
        "\n",
        "        print(f\"MultiAgentMarketEnv initialized for agents: {self._agent_ids}\")\n",
        "        print(f\"Observation space shape: {self.observation_space.shape}, Action space: {self.action_space}\")\n",
        "\n",
        "    def reset(self, *, seed: int = None, options: dict = None):\n",
        "        \"\"\"Resets the environment to an initial state and returns initial observations.\"\"\"\n",
        "        super().reset(seed=seed) # Important for seeding self.np_random\n",
        "\n",
        "        # Determine starting step:\n",
        "        # - If 'sequential' in options, start from the beginning (for evaluation).\n",
        "        # - Otherwise, pick a random start point for training.\n",
        "        if options and options.get(\"sequential\", False):\n",
        "            self.current_step = self.start_step\n",
        "        else:\n",
        "            # Ensure there are enough steps remaining for a meaningful episode (e.g., 100 steps)\n",
        "            min_episode_len = options.get(\"min_episode_len\", 100) if options else 100\n",
        "            max_possible_start = self.end_step - min_episode_len\n",
        "\n",
        "            # Ensure max_possible_start is not before self.start_step\n",
        "            effective_max_start = max(self.start_step, max_possible_start)\n",
        "\n",
        "            if self.start_step >= effective_max_start : # Handle cases where df is short\n",
        "                 self.current_step = self.start_step\n",
        "            else:\n",
        "                 self.current_step = self.np_random.integers(self.start_step, effective_max_start + 1)\n",
        "\n",
        "        initial_obs_single_agent = self._get_observation()\n",
        "\n",
        "        # Provide initial observations for all agents\n",
        "        observations = {\n",
        "            agent_id: initial_obs_single_agent.copy() for agent_id in self._agent_ids\n",
        "        }\n",
        "\n",
        "        # Initial info dictionary (can be empty)\n",
        "        infos = {agent_id: {} for agent_id in self._agent_ids}\n",
        "\n",
        "        return observations, infos\n",
        "\n",
        "    def step(self, action_dict: dict):\n",
        "        \"\"\"\n",
        "        Executes one time step within the environment.\n",
        "        Args:\n",
        "            action_dict (dict): A dictionary mapping agent_id to its action.\n",
        "        Returns:\n",
        "            Tuple: observations, rewards, terminations, truncations, infos\n",
        "        \"\"\"\n",
        "        # Ensure actions are provided for all active agents (can be more complex with agent presence)\n",
        "        # For now, assume all agents in _agent_ids are always active\n",
        "\n",
        "        current_row = self.df.iloc[self.current_step]\n",
        "        current_regime = int(current_row['regime']) # Assuming 'regime' column exists and is numeric\n",
        "\n",
        "        rewards = {}\n",
        "        for agent_id, action in action_dict.items():\n",
        "            if agent_id in self._agent_ids: # Process action only if agent is recognized\n",
        "                rewards[agent_id] = self._calculate_reward(action, current_regime, current_row)\n",
        "            # else: handle unexpected agent_id if necessary\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check for termination (end of data)\n",
        "        # __all__ is a special key indicating termination for all agents\n",
        "        terminated = self.current_step >= self.end_step\n",
        "        terminations = {\"__all__\": terminated}\n",
        "\n",
        "        # Truncation: can be used for time limits not related to task completion\n",
        "        # For now, not using episode-specific truncation beyond end of data\n",
        "        truncations = {\"__all__\": False}\n",
        "\n",
        "        # Get next observations\n",
        "        if not terminated:\n",
        "            next_obs_single_agent = self._get_observation()\n",
        "            observations = {\n",
        "                agent_id: next_obs_single_agent.copy() for agent_id in self._agent_ids\n",
        "            }\n",
        "        else:\n",
        "            # Provide a dummy observation if terminated (e.g., zeros)\n",
        "            # This is important as RLlib might still expect an observation\n",
        "            dummy_obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
        "            observations = {agent_id: dummy_obs for agent_id in self._agent_ids}\n",
        "\n",
        "        # Information dictionary for each agent\n",
        "        infos = {\n",
        "            agent_id: {\n",
        "                \"current_regime\": current_regime,\n",
        "                \"current_step\": self.current_step,\n",
        "                \"is_terminated\": terminated # Optional: signal termination in info\n",
        "            } for agent_id in self._agent_ids\n",
        "        }\n",
        "\n",
        "        return observations, rewards, terminations, truncations, infos\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Constructs the observation window for the current_step.\n",
        "        The window ends at current_step (inclusive).\n",
        "        \"\"\"\n",
        "        # Calculate start and end indices for the window\n",
        "        # Window: [current_step - window_size + 1, current_step]\n",
        "        start_idx = self.current_step - self.window_size + 1\n",
        "        end_idx = self.current_step + 1 # Slicing is exclusive at the end\n",
        "\n",
        "        if start_idx < 0:\n",
        "            # Not enough data at the beginning, need to pad\n",
        "            padding_size = abs(start_idx)\n",
        "            # Create padding array of zeros\n",
        "            padding = np.zeros((padding_size, len(self.feature_columns)), dtype=np.float32)\n",
        "            # Get the available actual data\n",
        "            actual_data = self.df[self.feature_columns].iloc[0:end_idx].values.astype(np.float32)\n",
        "            # Concatenate padding and actual data\n",
        "            window_data = np.vstack([padding, actual_data])\n",
        "        else:\n",
        "            # Sufficient data, directly slice from DataFrame\n",
        "            window_data = self.df[self.feature_columns].iloc[start_idx:end_idx].values.astype(np.float32)\n",
        "\n",
        "        # Ensure the final window_data has the correct shape\n",
        "        if window_data.shape[0] != self.window_size:\n",
        "            # This can happen if df is too short even with padding logic, or logic error\n",
        "            raise ValueError(\n",
        "                f\"Observation window shape error. Expected {self.window_size} time steps, \"\n",
        "                f\"got {window_data.shape[0]} at current_step {self.current_step} \"\n",
        "                f\"(start_idx: {start_idx}, end_idx: {end_idx}). \"\n",
        "                f\"DataFrame length: {len(self.df)}.\"\n",
        "            )\n",
        "\n",
        "        return window_data\n",
        "\n",
        "    def _calculate_reward(self, action: int, regime: int, current_row: pd.Series) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the reward for a given agent's action based on the market regime.\n",
        "        Args:\n",
        "            action (int): The action taken by the agent (0 or 1).\n",
        "            regime (int): The true market regime at the current step.\n",
        "            current_row (pd.Series): The row of data for the current step (for potential future use).\n",
        "        Returns:\n",
        "            float: The calculated reward.\n",
        "        \"\"\"\n",
        "        if self.reward_type == \"regime_prediction\":\n",
        "            # Regime mapping: 1,2 (Bearish), 3 (Neutral), 4,5 (Bullish) based on MMD\n",
        "            # Action: 0 (Conservative), 1 (Aggressive)\n",
        "\n",
        "            is_bullish_regime = regime >= 4\n",
        "            is_bearish_regime = regime <= 2\n",
        "            is_neutral_regime = regime == 3\n",
        "\n",
        "            agent_is_aggressive = action == 1\n",
        "            agent_is_conservative = action == 0\n",
        "\n",
        "            if agent_is_aggressive and is_bullish_regime:\n",
        "                return 1.0  # Correct: Aggressive in Bullish\n",
        "            elif agent_is_conservative and is_bearish_regime:\n",
        "                return 1.0  # Correct: Conservative in Bearish\n",
        "            elif is_neutral_regime: # Any action in neutral is okay, or small penalty/reward\n",
        "                return 0.1  # Small positive reward for surviving neutral\n",
        "            # Mismatches (penalties)\n",
        "            elif agent_is_aggressive and is_bearish_regime:\n",
        "                return -1.0 # Incorrect: Aggressive in Bearish\n",
        "            elif agent_is_conservative and is_bullish_regime:\n",
        "                return -1.0 # Incorrect: Conservative in Bullish\n",
        "            else: # Should not be reached if regimes are 1-5\n",
        "                return 0.0\n",
        "\n",
        "        elif self.reward_type == \"another_scheme\":\n",
        "            # Placeholder for a different reward calculation logic\n",
        "            # Example: reward based on P&L if actions map to trades\n",
        "            pass\n",
        "\n",
        "        # Default reward if no scheme matches or for unhandled cases\n",
        "        return 0.0\n",
        "\n",
        "# --- Register the Environment with RLlib ---\n",
        "# This allows RLlib to find and instantiate the environment using its string name.\n",
        "# The lambda function passes the environment configuration to the constructor.\n",
        "try:\n",
        "    register_env(\"MultiAgentMarketEnv_v0\", lambda config: MultiAgentMarketEnv(config))\n",
        "    print(\"MultiAgentMarketEnv_v0 registered successfully with RLlib.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error registering MultiAgentMarketEnv_v0: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f0b3c8",
      "metadata": {
        "id": "64f0b3c8",
        "outputId": "512d64a0-ced3-4260-a8f4-6fe554cf1219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom environment 'MultiAgentMarketEnv_v0' registered successfully.\n",
            "Registration Error: MultiAgentPolicy class not found. Please run Cell 5.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "MultiAgentPolicy class not found. Please run Cell 5.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCustom environment \u001b[39m\u001b[33m'\u001b[39m\u001b[33mMultiAgentMarketEnv_v0\u001b[39m\u001b[33m'\u001b[39m\u001b[33m registered successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mMultiAgentPolicy\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMultiAgentPolicy class not found. Please run Cell 5.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m ModelCatalog.register_custom_model(\u001b[33m\"\u001b[39m\u001b[33mMultiAgentPolicy\u001b[39m\u001b[33m\"\u001b[39m, MultiAgentPolicy)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCustom model \u001b[39m\u001b[33m'\u001b[39m\u001b[33mMultiAgentPolicy\u001b[39m\u001b[33m'\u001b[39m\u001b[33m registered successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: MultiAgentPolicy class not found. Please run Cell 5."
          ]
        }
      ],
      "source": [
        "# Cell 7: Multi-Agent PPO Training with Ray Tune (Rewritten)\n",
        "# -----------------------------------------------------------\n",
        "# This rewritten cell uses the modern `ray.tune.Tuner` API for a more\n",
        "# robust and automated training and checkpointing workflow.\n",
        "\n",
        "import ray\n",
        "import pandas as pd\n",
        "from ray import tune\n",
        "from ray.air.config import RunConfig, CheckpointConfig\n",
        "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.models import ModelCatalog\n",
        "\n",
        "# --- Prerequisite Checks and Registrations ---\n",
        "# This section ensures that your custom classes are registered with Ray.\n",
        "try:\n",
        "    if 'MultiAgentMarketEnv' not in globals():\n",
        "        raise NameError(\"MultiAgentMarketEnv class not found. Please run Cell 6.\")\n",
        "    register_env(\"MultiAgentMarketEnv_v0\", lambda config: MultiAgentMarketEnv(config))\n",
        "    print(\"Custom environment 'MultiAgentMarketEnv_v0' registered successfully.\")\n",
        "\n",
        "    if 'MultiAgentPolicy' not in globals():\n",
        "        raise NameError(\"MultiAgentPolicy class not found. Please run Cell 5.\")\n",
        "    ModelCatalog.register_custom_model(\"MultiAgentPolicy\", MultiAgentPolicy)\n",
        "    print(\"Custom model 'MultiAgentPolicy' registered successfully.\")\n",
        "except NameError as ne:\n",
        "    print(f\"Registration Error: {ne}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during registration: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Initialize Ray ---\n",
        "if ray.is_initialized():\n",
        "    print(\"Shutting down existing Ray instance...\")\n",
        "    ray.shutdown()\n",
        "ray.init(num_cpus=4, ignore_reinit_error=True, include_dashboard=False, logging_level=\"ERROR\")\n",
        "print(\"Ray initialized successfully.\")\n",
        "\n",
        "# --- PPO Algorithm Configuration ---\n",
        "print(\"\\nConfiguring PPO algorithm...\")\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\n",
        "        env=\"MultiAgentMarketEnv_v0\",\n",
        "        env_config={\n",
        "            \"df\": train_df,\n",
        "            \"window_size\": 30,\n",
        "            \"feature_columns\": available_features,\n",
        "            \"reward_type\": \"regime_prediction\",\n",
        "            \"agent_ids\": {\"agent_0\", \"agent_1\"}\n",
        "        }\n",
        "    )\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(num_rollout_workers=2, rollout_fragment_length=500)\n",
        "    .training(\n",
        "        model={\n",
        "            \"custom_model\": \"MultiAgentPolicy\",\n",
        "            \"custom_model_config\": {\n",
        "                \"encoding_size\": 128,\n",
        "                \"hidden_size\": 256,\n",
        "                \"num_lstm_layers\": 2,\n",
        "            }\n",
        "        },\n",
        "        lr=5e-5,\n",
        "        gamma=0.99,\n",
        "        lambda_=0.95,\n",
        "        clip_param=0.2,\n",
        "        vf_loss_coeff=0.5,\n",
        "        entropy_coeff=0.01,\n",
        "        train_batch_size=1000,\n",
        "        sgd_minibatch_size=128,\n",
        "        num_sgd_iter=10\n",
        "    )\n",
        "    .multi_agent(\n",
        "        policies={\"shared_policy\": PolicySpec()},\n",
        "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"shared_policy\",\n",
        "    )\n",
        "    .evaluation(\n",
        "        evaluation_interval=5,\n",
        "        evaluation_duration=10,\n",
        "        evaluation_duration_unit=\"episodes\",\n",
        "        evaluation_num_workers=1,\n",
        "        evaluation_config={\n",
        "            \"env_config\": {\n",
        "                \"df\": val_df,\n",
        "                \"window_size\": 30,\n",
        "                \"feature_columns\": available_features,\n",
        "                \"reward_type\": \"regime_prediction\",\n",
        "                \"agent_ids\": {\"agent_0\", \"agent_1\"},\n",
        "                \"sequential\": True\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "    .resources(num_gpus=0)\n",
        "    .debugging(log_level=\"WARN\")\n",
        ")\n",
        "print(\"PPO configuration created successfully.\")\n",
        "\n",
        "# --- Configure and Run the Tuner ---\n",
        "# This replaces the manual training loop for a more robust workflow.\n",
        "print(\"\\nSetting up Ray Tune Tuner...\")\n",
        "\n",
        "# 1. Define the checkpoint configuration to automatically save the best model.\n",
        "checkpoint_config = CheckpointConfig(\n",
        "    num_to_keep=1,\n",
        "    checkpoint_score_attribute=\"evaluation/episode_reward_mean\",\n",
        "    checkpoint_score_order=\"max\",\n",
        "    checkpoint_at_end=True\n",
        ")\n",
        "\n",
        "# 2. Define the RunConfig to specify stopping criteria and checkpointing.\n",
        "run_config = RunConfig(\n",
        "    name=\"PPO_Training_Run\",\n",
        "    stop={\"training_iteration\": 50},\n",
        "    checkpoint_config=checkpoint_config,\n",
        "    verbose=1, # Prints a summary table of results\n",
        ")\n",
        "\n",
        "# 3. Create the Tuner object.\n",
        "tuner = tune.Tuner(\n",
        "    \"PPO\",\n",
        "    param_space=config.to_dict(),\n",
        "    run_config=run_config,\n",
        ")\n",
        "\n",
        "# 4. Run the training.\n",
        "print(\"Starting training with Ray Tune...\")\n",
        "results = tuner.fit()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# --- Post-Training: Load Best Model and History ---\n",
        "# This section prepares the `algo` and `history_df` objects for the next cell.\n",
        "print(\"\\nLoading best model from training run for evaluation...\")\n",
        "algo = None\n",
        "history_df = None\n",
        "try:\n",
        "    best_result = results.get_best_result(metric=\"evaluation/episode_reward_mean\", mode=\"max\")\n",
        "\n",
        "    if best_result and best_result.checkpoint:\n",
        "        print(f\"Best checkpoint found at: {best_result.checkpoint.path}\")\n",
        "        algo = PPO.from_checkpoint(best_result.checkpoint)\n",
        "        print(\"✅ Trained `algo` object is restored and ready for evaluation.\")\n",
        "\n",
        "        history_df = results.get_dataframe()\n",
        "        history_df.rename(columns={\n",
        "            \"evaluation/episode_reward_mean\": \"eval_episode_reward_mean\",\n",
        "            \"episode_reward_mean\": \"train_episode_reward_mean\"\n",
        "        }, inplace=True)\n",
        "        print(\"✅ `history_df` is created and ready for plotting.\")\n",
        "    else:\n",
        "        raise ValueError(\"No checkpoint found in the best result. Training may have failed to save a model.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"--- ERROR loading results ---\")\n",
        "    print(f\"Could not load the best model from the training run: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f2b380",
      "metadata": {
        "id": "d7f2b380",
        "outputId": "0c107705-a341-4283-c7ec-479958d41a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to load latest training results from disk...\n",
            "Found latest experiment: 'PPO_MultiAgentMarketEnv_v0_2025-06-10_19-07-12huprxdh7'\n",
            "Searching for trial data in: C:\\Users\\Windows 11/ray_results\\PPO_MultiAgentMarketEnv_v0_2025-06-10_19-07-12huprxdh7\n",
            "--- ERROR ---\n",
            "No trial subdirectories were found inside C:\\Users\\Windows 11/ray_results\\PPO_MultiAgentMarketEnv_v0_2025-06-10_19-07-12huprxdh7\n",
            "\n",
            "Troubleshooting:\n",
            "1. Ensure your training (Cell 7) completed and created checkpoint files.\n",
            "2. Manually check the contents of your latest experiment directory: C:\\Users\\Windows 11/ray_results\\PPO_MultiAgentMarketEnv_v0_2025-06-10_19-07-12huprxdh7\n"
          ]
        }
      ],
      "source": [
        "# --- Final, Most Robust Cell (Cell 7.5) ---\n",
        "# This version makes no assumptions about trial directory names.\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from ray.rllib.algorithms.ppo import PPO # IMPORTANT: Assuming PPO is your algorithm\n",
        "\n",
        "print(\"Attempting to load latest training results from disk...\")\n",
        "\n",
        "# --- Configuration ---\n",
        "ray_results_dir = os.path.expanduser(\"~/ray_results\")\n",
        "experiment_name = None # Let the script find the latest\n",
        "# --- End of Configuration ---\n",
        "\n",
        "algo = None\n",
        "history_df = pd.DataFrame()\n",
        "experiment_path = \"\" # Initialize to avoid reference errors\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(ray_results_dir):\n",
        "        raise FileNotFoundError(f\"Ray results directory not found at: {ray_results_dir}\")\n",
        "\n",
        "    # Find the latest experiment directory\n",
        "    if experiment_name:\n",
        "        experiment_path = os.path.join(ray_results_dir, experiment_name)\n",
        "    else:\n",
        "        all_exp_dirs = [d for d in os.listdir(ray_results_dir) if os.path.isdir(os.path.join(ray_results_dir, d))]\n",
        "        if not all_exp_dirs:\n",
        "            raise FileNotFoundError(\"No experiment directories found in `ray_results`.\")\n",
        "        latest_exp_name = max(all_exp_dirs, key=lambda d: os.path.getmtime(os.path.join(ray_results_dir, d)))\n",
        "        experiment_path = os.path.join(ray_results_dir, latest_exp_name)\n",
        "        print(f\"Found latest experiment: '{latest_exp_name}'\")\n",
        "\n",
        "    print(f\"Searching for trial data in: {experiment_path}\")\n",
        "\n",
        "    # --- FINAL ROBUST LOADING LOGIC ---\n",
        "    # 1. Find the trial directory by looking for ANY subdirectory.\n",
        "    all_subdirs = [os.path.join(experiment_path, d) for d in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, d))]\n",
        "    if not all_subdirs:\n",
        "        raise FileNotFoundError(f\"No trial subdirectories were found inside {experiment_path}\")\n",
        "\n",
        "    # Assume the most recently modified subdirectory is the correct trial path.\n",
        "    trial_path = max(all_subdirs, key=os.path.getmtime)\n",
        "    print(f\"Found latest trial path: {trial_path}\")\n",
        "\n",
        "    # 2. Find the latest checkpoint directory within the trial path.\n",
        "    checkpoint_dirs = [d for d in os.listdir(trial_path) if d.startswith(\"checkpoint_\")]\n",
        "    if not checkpoint_dirs:\n",
        "        raise FileNotFoundError(f\"No checkpoint folders (e.g., 'checkpoint_*') found in trial path: {trial_path}\")\n",
        "\n",
        "    latest_checkpoint_name = sorted(checkpoint_dirs, key=lambda c: int(c.split('_')[1]), reverse=True)[0]\n",
        "    latest_checkpoint_path = os.path.join(trial_path, latest_checkpoint_name)\n",
        "\n",
        "    print(f\"Loading latest checkpoint from: {latest_checkpoint_path}\")\n",
        "\n",
        "    # 3. Restore the algorithm ('algo') from this checkpoint.\n",
        "    algo = PPO.from_checkpoint(latest_checkpoint_path)\n",
        "    print(\"Algorithm restored successfully.\")\n",
        "\n",
        "    # 4. Load the training history ('history_df') from 'progress.csv'.\n",
        "    history_file = os.path.join(trial_path, \"progress.csv\")\n",
        "    if os.path.exists(history_file):\n",
        "        history_df = pd.read_csv(history_file)\n",
        "        print(\"Training history DataFrame loaded.\")\n",
        "    else:\n",
        "        print(\"Warning: progress.csv not found. Training history plots will be unavailable.\")\n",
        "\n",
        "except (FileNotFoundError, ValueError, IndexError) as e:\n",
        "    print(f\"--- ERROR ---\")\n",
        "    print(e)\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Ensure your training (Cell 7) completed and created checkpoint files.\")\n",
        "    print(f\"2. Manually check the contents of your latest experiment directory: {experiment_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0917ab40",
      "metadata": {
        "id": "0917ab40",
        "outputId": "1bdf7fb2-e48e-4942-eb19-d153bb69c943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SKIPPING EVALUATION: The 'algo' object was not created. Please run the cell above to load the trained model from disk.\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Evaluation and Visualization (Corrected)\n",
        "\n",
        "# First, check if the 'algo' object was successfully created in the previous cell\n",
        "if 'algo' in locals() and algo:\n",
        "    # --- Evaluation function (as you provided) ---\n",
        "    def evaluate_multi_agent(algo, eval_df, num_episodes=10):\n",
        "        \"\"\"Evaluate the trained multi-agent system\"\"\"\n",
        "        env_config = {\n",
        "            \"df\": eval_df,\n",
        "            \"window_size\": 30,\n",
        "            \"feature_columns\": available_features,\n",
        "            \"reward_type\": \"regime_prediction\"\n",
        "        }\n",
        "        env = MultiAgentMarketEnv(env_config)\n",
        "\n",
        "        all_rewards = []\n",
        "        all_predictions = []\n",
        "        all_true_regimes = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            obs, _ = env.reset(options={\"sequential\": True})\n",
        "            episode_reward = 0\n",
        "            done = {\"__all__\": False}\n",
        "\n",
        "            while not done[\"__all__\"]:\n",
        "                actions = {}\n",
        "                for agent_id in env._agent_ids:\n",
        "                    actions[agent_id] = algo.compute_single_action(\n",
        "                        obs[agent_id],\n",
        "                        policy_id=\"shared_policy\"\n",
        "                    )\n",
        "                obs, rewards, done, truncated, infos = env.step(actions)\n",
        "                episode_reward += rewards[\"agent_0\"]\n",
        "                if \"agent_0\" in infos:\n",
        "                    all_predictions.append(actions[\"agent_0\"])\n",
        "                    all_true_regimes.append(infos[\"agent_0\"][\"current_regime\"])\n",
        "\n",
        "            all_rewards.append(episode_reward)\n",
        "\n",
        "        correct_predictions = sum(\n",
        "            (pred == 1 and true >= 4) or (pred == 0 and true <= 2)\n",
        "            for pred, true in zip(all_predictions, all_true_regimes)\n",
        "        )\n",
        "        accuracy = correct_predictions / len(all_predictions) if all_predictions else 0\n",
        "\n",
        "        return {\n",
        "            'mean_reward': np.mean(all_rewards),\n",
        "            'std_reward': np.std(all_rewards),\n",
        "            'accuracy': accuracy,\n",
        "            'predictions': all_predictions,\n",
        "            'true_regimes': all_true_regimes\n",
        "        }\n",
        "\n",
        "    # --- Run Evaluation ---\n",
        "    print(\"Evaluating the trained agent on the test set...\")\n",
        "    test_results = evaluate_multi_agent(algo, test_df, num_episodes=20)\n",
        "\n",
        "    print(f\"\\n--- Test Set Results ---\")\n",
        "    print(f\"Mean Reward: {test_results['mean_reward']:.3f} ± {test_results['std_reward']:.3f}\")\n",
        "    print(f\"Regime Prediction Accuracy: {test_results['accuracy']:.3%}\")\n",
        "\n",
        "    # --- Visualization ---\n",
        "    if 'history_df' in locals() and not history_df.empty:\n",
        "        print(\"\\nVisualizing training history...\")\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
        "        ax1.plot(history_df['iteration'], history_df['episode_reward_mean'], label='Mean Reward')\n",
        "        ax1.set_ylabel('Episode Reward Mean')\n",
        "        ax1.set_title('Training Progress')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax2.plot(history_df['iteration'], history_df['episode_len_mean'], label='Mean Episode Length')\n",
        "        ax2.set_xlabel('Training Iteration')\n",
        "        ax2.set_ylabel('Episode Length Mean')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    print(\"\\nGenerating confusion matrix...\")\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    y_true_binary = [0 if r > 3 else 1 for r in test_results['true_regimes']] # 0=Bullish, 1=Bearish\n",
        "    y_pred_binary = [0 if p == 1 else 1 for p in test_results['predictions']] # 0=Bullish, 1=Bearish\n",
        "    cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Bullish', 'Bearish'], yticklabels=['Bullish', 'Bearish'])\n",
        "    plt.title('Market Regime Prediction Confusion Matrix')\n",
        "    plt.ylabel('True Regime')\n",
        "    plt.xlabel('Predicted Action')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"SKIPPING EVALUATION: The 'algo' object was not created. Please run the cell above to load the trained model from disk.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (rl_env)",
      "language": "python",
      "name": "rl_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}