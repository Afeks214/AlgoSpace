{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 3: NW-RQK → MLMI → FVG Trading Strategy\n",
    "\n",
    "## Ultra-High Performance Implementation with VectorBT and Numba\n",
    "\n",
    "This notebook implements the third synergy pattern where:\n",
    "1. **NW-RQK** (Nadaraya-Watson Rational Quadratic Kernel) provides the initial trend signal\n",
    "2. **MLMI** (Machine Learning Market Intelligence) confirms the market regime\n",
    "3. **FVG** (Fair Value Gap) validates the final entry zone\n",
    "\n",
    "### Key Features:\n",
    "- Ultra-fast execution using Numba JIT compilation with parallel processing\n",
    "- VectorBT for lightning-fast vectorized backtesting\n",
    "- Natural trade generation (2,500-4,500 trades over 5 years)\n",
    "- Professional visualizations and comprehensive metrics\n",
    "- Sub-10 second full backtest execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport vectorbt as vbt\nfrom numba import njit, prange, float64, int64, boolean\nfrom numba.typed import List\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nfrom datetime import datetime, timedelta\nimport time\nfrom scipy import stats\nimport logging\nimport os\nimport sys\nfrom typing import Dict, Any, Tuple, Optional\nimport json\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\n# Configure VectorBT\nvbt.settings.set_theme('dark')\nvbt.settings['plotting']['layout']['width'] = 1200\nvbt.settings['plotting']['layout']['height'] = 800\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout),\n        logging.FileHandler('synergy_3_strategy.log')\n    ]\n)\nlogger = logging.getLogger('Synergy3Strategy')\n\n# Configuration Management\nclass StrategyConfig:\n    \"\"\"Centralized configuration for the strategy\"\"\"\n    \n    # Data Configuration\n    DATA_PATH_30M = '/home/QuantNova/AlgoSpace-8/data/BTC-USD-30m.csv'\n    DATA_PATH_5M = '/home/QuantNova/AlgoSpace-8/data/BTC-USD-5m.csv'\n    DATETIME_FORMATS = ['%Y-%m-%d %H:%M:%S%z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']\n    \n    # NW-RQK Configuration\n    NWRQK_WINDOW = 30\n    NWRQK_N_KERNELS = 3\n    NWRQK_ALPHAS = [0.3, 0.5, 0.7]\n    NWRQK_LENGTH_SCALES = [30.0, 50.0, 70.0]\n    NWRQK_THRESHOLD = 0.002\n    NWRQK_VOLATILITY_ADAPTIVE = True\n    \n    # MLMI Configuration\n    MLMI_WINDOW = 10\n    MLMI_K_NEIGHBORS = 5\n    MLMI_FEATURE_WINDOW = 3\n    MLMI_LOOKBACK = 100\n    MLMI_RSI_PERIOD = 14\n    MLMI_VOLATILITY_WINDOW = 20\n    MLMI_VOLATILITY_SCALE = 2.0\n    MLMI_BULL_THRESHOLD = 0.65\n    MLMI_BEAR_THRESHOLD = 0.35\n    MLMI_CONFIDENCE_THRESHOLD = 0.3\n    \n    # FVG Configuration\n    FVG_MIN_GAP_PCT = 0.001\n    FVG_VOLUME_FACTOR = 1.2\n    FVG_VOLUME_WINDOW = 20\n    \n    # Synergy Configuration\n    SYNERGY_WINDOW = 30\n    SYNERGY_NWRQK_STRENGTH_THRESHOLD = 0.5\n    SYNERGY_MLMI_CONFIDENCE_THRESHOLD = 0.3\n    SYNERGY_STATE_DECAY_WINDOW = 30\n    \n    # Risk Management\n    POSITION_SIZE_BASE = 0.1\n    STOP_LOSS_PCT = 0.02\n    TAKE_PROFIT_PCT = 0.03\n    MAX_DRAWDOWN_LIMIT = 0.15\n    MAX_DAILY_LOSS = 0.05\n    \n    # Backtesting\n    INITIAL_CAPITAL = 100000\n    TRADING_FEES = 0.001\n    SLIPPAGE = 0.0005\n    \n    # Validation\n    MAX_MISSING_DATA_PCT = 0.05\n    OUTLIER_STD_THRESHOLD = 10\n    MIN_DATA_POINTS = 1000\n    \n    @classmethod\n    def validate(cls):\n        \"\"\"Validate configuration parameters\"\"\"\n        if cls.NWRQK_WINDOW < 10:\n            logger.warning(\"NW-RQK window < 10 may produce unstable results\")\n        \n        if len(cls.NWRQK_ALPHAS) != cls.NWRQK_N_KERNELS:\n            raise ValueError(\"Number of alphas must match n_kernels\")\n        \n        if cls.MLMI_BULL_THRESHOLD <= cls.MLMI_BEAR_THRESHOLD:\n            raise ValueError(\"Bull threshold must be greater than bear threshold\")\n        \n        logger.info(\"Configuration validated successfully\")\n\n# Validate configuration on import\nStrategyConfig.validate()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ultra-Fast Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data with optimized parsing and comprehensive error handling\ndef load_data():\n    \"\"\"Load and preprocess data with ultra-fast parsing and robust error handling\"\"\"\n    logger.info(\"Starting data loading process...\")\n    start_time = time.time()\n    \n    try:\n        # Validate file existence\n        if not os.path.exists(StrategyConfig.DATA_PATH_30M):\n            raise FileNotFoundError(f\"30m data file not found: {StrategyConfig.DATA_PATH_30M}\")\n        if not os.path.exists(StrategyConfig.DATA_PATH_5M):\n            raise FileNotFoundError(f\"5m data file not found: {StrategyConfig.DATA_PATH_5M}\")\n        \n        # Load 30-minute data with error handling\n        logger.info(f\"Loading 30m data from {StrategyConfig.DATA_PATH_30M}\")\n        df_30m = load_single_timeframe(StrategyConfig.DATA_PATH_30M, '30m')\n        \n        # Load 5-minute data with error handling\n        logger.info(f\"Loading 5m data from {StrategyConfig.DATA_PATH_5M}\")\n        df_5m = load_single_timeframe(StrategyConfig.DATA_PATH_5M, '5m')\n        \n        # Validate data quality\n        df_30m = validate_and_clean_data(df_30m, '30m')\n        df_5m = validate_and_clean_data(df_5m, '5m')\n        \n        # Add derived features\n        df_30m = add_robust_features(df_30m, '30m')\n        df_5m = add_robust_features(df_5m, '5m')\n        \n        # Align data timeframes\n        df_30m, df_5m = align_timeframes(df_30m, df_5m)\n        \n        logger.info(f\"Data loading completed in {time.time() - start_time:.2f} seconds\")\n        logger.info(f\"30m data: {len(df_30m)} bars from {df_30m.index[0]} to {df_30m.index[-1]}\")\n        logger.info(f\"5m data: {len(df_5m)} bars from {df_5m.index[0]} to {df_5m.index[-1]}\")\n        \n        return df_30m, df_5m\n        \n    except Exception as e:\n        logger.error(f\"Critical error in data loading: {str(e)}\")\n        raise\n\ndef load_single_timeframe(file_path: str, timeframe: str) -> pd.DataFrame:\n    \"\"\"Load data for a single timeframe with robust error handling\"\"\"\n    try:\n        # Load CSV with error handling\n        df = pd.read_csv(file_path, na_values=['', 'null', 'NULL', 'NaN'])\n        \n        # Check for required columns\n        required_columns = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns in {timeframe} data: {missing_columns}\")\n        \n        # Flexible datetime parsing\n        datetime_parsed = False\n        for fmt in StrategyConfig.DATETIME_FORMATS:\n            try:\n                df['datetime'] = pd.to_datetime(df['datetime'], format=fmt, errors='coerce')\n                if df['datetime'].notna().sum() > len(df) * 0.95:  # At least 95% parsed successfully\n                    datetime_parsed = True\n                    logger.debug(f\"Successfully parsed datetime with format: {fmt}\")\n                    break\n            except Exception:\n                continue\n        \n        if not datetime_parsed:\n            # Fallback to pandas automatic parsing\n            df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n            logger.warning(f\"Used automatic datetime parsing for {timeframe} data\")\n        \n        # Remove rows with invalid datetime\n        invalid_datetime = df['datetime'].isna()\n        if invalid_datetime.any():\n            logger.warning(f\"Removing {invalid_datetime.sum()} rows with invalid datetime in {timeframe} data\")\n            df = df[~invalid_datetime]\n        \n        # Set index and sort\n        df = df.set_index('datetime').sort_index()\n        \n        # Remove duplicate timestamps\n        duplicates = df.index.duplicated()\n        if duplicates.any():\n            logger.warning(f\"Removing {duplicates.sum()} duplicate timestamps in {timeframe} data\")\n            df = df[~duplicates]\n        \n        # Ensure numeric data types\n        numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n        for col in numeric_columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n        \n        # Check minimum data requirement\n        if len(df) < StrategyConfig.MIN_DATA_POINTS:\n            raise ValueError(f\"Insufficient data: {len(df)} rows, need at least {StrategyConfig.MIN_DATA_POINTS}\")\n        \n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error loading {timeframe} data: {str(e)}\")\n        raise\n\ndef validate_and_clean_data(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n    \"\"\"Validate data quality and handle issues robustly\"\"\"\n    logger.info(f\"Validating {timeframe} data...\")\n    \n    try:\n        # Check for missing values\n        missing_count = df.isnull().sum()\n        total_missing = missing_count.sum()\n        \n        if total_missing > 0:\n            missing_pct = total_missing / (len(df) * len(df.columns))\n            logger.warning(f\"Found {total_missing} missing values ({missing_pct:.2%}) in {timeframe} data\")\n            \n            if missing_pct > StrategyConfig.MAX_MISSING_DATA_PCT:\n                logger.error(f\"Too many missing values: {missing_pct:.2%}\")\n                # Try to recover by forward/backward filling\n                df = df.fillna(method='ffill').fillna(method='bfill')\n                \n                # Check again\n                remaining_missing = df.isnull().sum().sum()\n                if remaining_missing > 0:\n                    logger.warning(f\"Still have {remaining_missing} missing values after filling\")\n                    # Fill remaining with reasonable defaults\n                    df['volume'] = df['volume'].fillna(0)\n                    for col in ['open', 'high', 'low', 'close']:\n                        df[col] = df[col].fillna(df[col].mean())\n        \n        # Validate price relationships\n        invalid_candles = (\n            (df['high'] < df['low']) | \n            (df['high'] < df['open']) | \n            (df['high'] < df['close']) |\n            (df['low'] > df['open']) | \n            (df['low'] > df['close'])\n        )\n        \n        if invalid_candles.any():\n            n_invalid = invalid_candles.sum()\n            logger.warning(f\"Found {n_invalid} invalid candles in {timeframe} data, fixing...\")\n            \n            # Fix invalid candles\n            df.loc[invalid_candles, 'high'] = df.loc[invalid_candles, ['open', 'close', 'high']].max(axis=1)\n            df.loc[invalid_candles, 'low'] = df.loc[invalid_candles, ['open', 'close', 'low']].min(axis=1)\n        \n        # Check for outliers\n        for col in ['open', 'high', 'low', 'close']:\n            # Calculate rolling statistics\n            rolling_mean = df[col].rolling(window=100, min_periods=10).mean()\n            rolling_std = df[col].rolling(window=100, min_periods=10).std()\n            \n            # Identify outliers\n            z_scores = np.abs((df[col] - rolling_mean) / rolling_std)\n            outliers = z_scores > StrategyConfig.OUTLIER_STD_THRESHOLD\n            \n            if outliers.any():\n                n_outliers = outliers.sum()\n                logger.warning(f\"Found {n_outliers} outliers in {col} for {timeframe} data\")\n                \n                # Cap outliers at threshold\n                df.loc[outliers, col] = rolling_mean[outliers] + np.sign(\n                    df.loc[outliers, col] - rolling_mean[outliers]\n                ) * StrategyConfig.OUTLIER_STD_THRESHOLD * rolling_std[outliers]\n        \n        # Ensure no negative prices\n        negative_prices = (df[['open', 'high', 'low', 'close']] < 0).any(axis=1)\n        if negative_prices.any():\n            logger.error(f\"Found {negative_prices.sum()} rows with negative prices, removing...\")\n            df = df[~negative_prices]\n        \n        # Ensure no zero prices\n        zero_prices = (df[['open', 'high', 'low', 'close']] == 0).any(axis=1)\n        if zero_prices.any():\n            logger.warning(f\"Found {zero_prices.sum()} rows with zero prices, interpolating...\")\n            for col in ['open', 'high', 'low', 'close']:\n                df[col] = df[col].replace(0, np.nan).interpolate(method='linear')\n        \n        # Check for time gaps\n        time_diff = df.index.to_series().diff()\n        expected_freq = pd.Timedelta(timeframe)\n        large_gaps = time_diff > expected_freq * 2\n        \n        if large_gaps.any():\n            logger.warning(f\"Found {large_gaps.sum()} time gaps larger than expected in {timeframe} data\")\n        \n        logger.info(f\"Validation completed for {timeframe} data\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error in data validation for {timeframe}: {str(e)}\")\n        raise\n\ndef add_robust_features(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n    \"\"\"Add derived features with error handling\"\"\"\n    try:\n        # Calculate returns with handling for division by zero\n        df['returns'] = df['close'].pct_change().fillna(0)\n        \n        # Cap extreme returns\n        extreme_returns = np.abs(df['returns']) > 0.5  # 50% moves\n        if extreme_returns.any():\n            logger.warning(f\"Capping {extreme_returns.sum()} extreme returns in {timeframe} data\")\n            df.loc[extreme_returns, 'returns'] = np.sign(df.loc[extreme_returns, 'returns']) * 0.5\n        \n        # Calculate log returns safely\n        df['log_returns'] = np.log1p(df['returns'])  # log1p is more stable for small values\n        \n        # Calculate volatility with minimum periods\n        df['volatility'] = df['returns'].rolling(window=20, min_periods=5).std().fillna(0)\n        \n        # Volume metrics with safety checks\n        df['volume_sma'] = df['volume'].rolling(window=20, min_periods=5).mean().fillna(df['volume'])\n        df['volume_ratio'] = np.where(\n            df['volume_sma'] > 0,\n            df['volume'] / df['volume_sma'],\n            1.0\n        )\n        \n        # Price ranges with safety checks\n        df['high_low_range'] = np.where(\n            df['close'] > 0,\n            (df['high'] - df['low']) / df['close'],\n            0\n        )\n        df['close_open_range'] = np.where(\n            df['open'] > 0,\n            (df['close'] - df['open']) / df['open'],\n            0\n        )\n        \n        # VWAP calculation with cumulative approach\n        typical_price = (df['high'] + df['low'] + df['close']) / 3\n        df['vwap'] = (typical_price * df['volume']).cumsum() / df['volume'].cumsum()\n        df['vwap'] = df['vwap'].fillna(typical_price)  # Handle initial NaN values\n        \n        # Add trend indicators\n        df['sma_20'] = df['close'].rolling(window=20, min_periods=5).mean()\n        df['sma_50'] = df['close'].rolling(window=50, min_periods=10).mean()\n        df['price_position'] = (df['close'] - df['sma_20']) / df['sma_20']\n        \n        logger.info(f\"Added features to {timeframe} data\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error adding features to {timeframe} data: {str(e)}\")\n        raise\n\ndef align_timeframes(df_30m: pd.DataFrame, df_5m: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Align data timeframes with validation\"\"\"\n    try:\n        # Find common time range\n        start_time = max(df_30m.index[0], df_5m.index[0])\n        end_time = min(df_30m.index[-1], df_5m.index[-1])\n        \n        logger.info(f\"Aligning data from {start_time} to {end_time}\")\n        \n        # Filter to common range\n        df_30m = df_30m[(df_30m.index >= start_time) & (df_30m.index <= end_time)]\n        df_5m = df_5m[(df_5m.index >= start_time) & (df_5m.index <= end_time)]\n        \n        # Validate alignment\n        expected_ratio = 6  # 30min / 5min\n        actual_ratio = len(df_5m) / len(df_30m)\n        \n        if abs(actual_ratio - expected_ratio) > 1:\n            logger.warning(f\"Unexpected data ratio: {actual_ratio:.2f} (expected ~{expected_ratio})\")\n        \n        return df_30m, df_5m\n        \n    except Exception as e:\n        logger.error(f\"Error aligning timeframes: {str(e)}\")\n        raise\n\n# Load the data with error handling\ntry:\n    df_30m, df_5m = load_data()\n    logger.info(\"Data loading successful!\")\nexcept Exception as e:\n    logger.error(f\"Failed to load data: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced NW-RQK Implementation with Multi-Kernel Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(fastmath=True, cache=True)\ndef rational_quadratic_kernel(x1, x2, alpha=0.5, length_scale=50.0):\n    \"\"\"Rational Quadratic Kernel for NW-RQK\"\"\"\n    # Validate inputs\n    if alpha <= 0 or alpha >= 1:\n        alpha = 0.5  # Default safe value\n    if length_scale <= 0:\n        length_scale = 50.0  # Default safe value\n    \n    diff = x1 - x2\n    return (1.0 + (diff * diff) / (2.0 * alpha * length_scale * length_scale)) ** (-alpha)\n\n@njit(fastmath=True, cache=True)\ndef gaussian_kernel(x1, x2, length_scale=50.0):\n    \"\"\"Gaussian Kernel for ensemble\"\"\"\n    if length_scale <= 0:\n        length_scale = 50.0\n    \n    diff = x1 - x2\n    return np.exp(-0.5 * (diff * diff) / (length_scale * length_scale))\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef nwrqk_ensemble(prices, window=30, n_kernels=3):\n    \"\"\"Multi-kernel ensemble NW-RQK implementation with validation\"\"\"\n    n = len(prices)\n    nwrqk_values = np.zeros(n)\n    \n    # Input validation\n    if window < 10:\n        window = 10\n    if n_kernels < 1:\n        n_kernels = 3\n    \n    # Kernel parameters for ensemble\n    alphas = np.array([0.3, 0.5, 0.7])\n    length_scales = np.array([30.0, 50.0, 70.0])\n    \n    # Initialize with actual prices for the first window\n    for i in range(min(window, n)):\n        nwrqk_values[i] = prices[i] if i < len(prices) else prices[-1]\n    \n    for i in prange(window, n):\n        # Window data\n        window_prices = prices[i-window:i]\n        \n        # Check for valid window data\n        if np.all(np.isnan(window_prices)) or np.all(window_prices == 0):\n            nwrqk_values[i] = nwrqk_values[i-1] if i > 0 else prices[i]\n            continue\n        \n        # Ensemble predictions\n        predictions = np.zeros(n_kernels)\n        valid_predictions = 0\n        \n        for k in range(n_kernels):\n            # Calculate weights using RQ kernel\n            weights = np.zeros(window)\n            for j in range(window):\n                weights[j] = rational_quadratic_kernel(\n                    float(i), float(i-window+j), \n                    alphas[k % len(alphas)], \n                    length_scales[k % len(length_scales)]\n                )\n            \n            # Normalize weights\n            weight_sum = np.sum(weights)\n            if weight_sum > 1e-10:  # Avoid division by very small numbers\n                weights /= weight_sum\n                predictions[k] = np.sum(weights * window_prices)\n                valid_predictions += 1\n            else:\n                predictions[k] = np.nan\n        \n        # Weighted ensemble - only use valid predictions\n        if valid_predictions > 0:\n            valid_mask = ~np.isnan(predictions)\n            nwrqk_values[i] = np.mean(predictions[valid_mask])\n        else:\n            # Fallback to simple moving average\n            nwrqk_values[i] = np.mean(window_prices)\n    \n    return nwrqk_values\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef calculate_nwrqk_signals(prices, nwrqk_values, threshold=0.002):\n    \"\"\"Generate NW-RQK trend signals with adaptive thresholds and validation\"\"\"\n    n = len(prices)\n    bull_signals = np.zeros(n, dtype=np.bool_)\n    bear_signals = np.zeros(n, dtype=np.bool_)\n    signal_strength = np.zeros(n)\n    \n    # Validate threshold\n    if threshold <= 0:\n        threshold = 0.002\n    \n    for i in prange(1, n):\n        # Skip if invalid data\n        if nwrqk_values[i] <= 0 or prices[i] <= 0 or np.isnan(nwrqk_values[i]) or np.isnan(prices[i]):\n            continue\n        \n        # Price relative to NW-RQK\n        deviation = (prices[i] - nwrqk_values[i]) / nwrqk_values[i]\n        \n        # NW-RQK slope calculation with safety checks\n        if i > 5 and nwrqk_values[i-5] > 0:\n            slope = (nwrqk_values[i] - nwrqk_values[i-5]) / nwrqk_values[i-5]\n            \n            # Adaptive threshold based on volatility\n            vol_window = 20\n            adaptive_threshold = threshold\n            \n            if i > vol_window:\n                returns = np.zeros(vol_window)\n                valid_returns = 0\n                \n                for j in range(vol_window):\n                    if i-j-1 >= 0 and prices[i-j-1] > 0 and prices[i-j] > 0:\n                        returns[valid_returns] = (prices[i-j] - prices[i-j-1]) / prices[i-j-1]\n                        valid_returns += 1\n                \n                if valid_returns > 5:  # Need minimum returns for volatility\n                    volatility = np.std(returns[:valid_returns])\n                    # Cap volatility adjustment to prevent extreme thresholds\n                    volatility = min(volatility, 0.1)  # Cap at 10% volatility\n                    adaptive_threshold = threshold * (1 + volatility * 10)\n            \n            # Strong trend signals with deviation bounds\n            if slope > adaptive_threshold and -0.05 < deviation < 0.05:  # Within 5% of NW-RQK\n                bull_signals[i] = True\n                signal_strength[i] = min(slope / adaptive_threshold, 2.0)\n            elif slope < -adaptive_threshold and -0.05 < deviation < 0.05:\n                bear_signals[i] = True\n                signal_strength[i] = min(abs(slope) / adaptive_threshold, 2.0)\n    \n    return bull_signals, bear_signals, signal_strength\n\n# Add validation wrapper for NW-RQK calculation\ndef calculate_nwrqk_with_validation(prices, config=None):\n    \"\"\"Calculate NW-RQK with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input\n        if len(prices) < config.NWRQK_WINDOW * 2:\n            raise ValueError(f\"Insufficient data for NW-RQK: need at least {config.NWRQK_WINDOW * 2} points\")\n        \n        # Check for valid prices\n        valid_prices = prices[~np.isnan(prices) & (prices > 0)]\n        if len(valid_prices) < len(prices) * 0.9:\n            logger.warning(f\"More than 10% invalid prices in NW-RQK input\")\n        \n        # Calculate NW-RQK\n        logger.info(\"Calculating NW-RQK ensemble...\")\n        nwrqk_values = nwrqk_ensemble(\n            prices,\n            window=config.NWRQK_WINDOW,\n            n_kernels=config.NWRQK_N_KERNELS\n        )\n        \n        # Validate output\n        if np.all(np.isnan(nwrqk_values)) or np.all(nwrqk_values == 0):\n            raise ValueError(\"NW-RQK calculation failed - all values invalid\")\n        \n        # Calculate signals\n        bull_signals, bear_signals, signal_strength = calculate_nwrqk_signals(\n            prices, \n            nwrqk_values,\n            threshold=config.NWRQK_THRESHOLD\n        )\n        \n        # Log statistics\n        logger.info(f\"NW-RQK calculation complete - Bull: {bull_signals.sum()}, Bear: {bear_signals.sum()}\")\n        \n        return nwrqk_values, bull_signals, bear_signals, signal_strength\n        \n    except Exception as e:\n        logger.error(f\"Error in NW-RQK calculation: {str(e)}\")\n        # Return safe defaults\n        n = len(prices)\n        return prices.copy(), np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced MLMI with Volatility-Adaptive KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(fastmath=True, cache=True)\ndef calculate_rsi(prices, period=14):\n    \"\"\"Ultra-fast RSI calculation with validation\"\"\"\n    n = len(prices)\n    rsi = np.zeros(n)\n    \n    # Validate period\n    if period < 2:\n        period = 14\n    \n    if n < period + 1:\n        return rsi\n    \n    # Calculate price changes\n    deltas = np.zeros(n)\n    for i in range(1, n):\n        if prices[i-1] > 0 and not np.isnan(prices[i]) and not np.isnan(prices[i-1]):\n            deltas[i] = prices[i] - prices[i-1]\n    \n    # Initial averages\n    avg_gain = 0.0\n    avg_loss = 0.0\n    valid_deltas = 0\n    \n    for i in range(1, min(period + 1, n)):\n        if not np.isnan(deltas[i]):\n            if deltas[i] > 0:\n                avg_gain += deltas[i]\n            else:\n                avg_loss -= deltas[i]\n            valid_deltas += 1\n    \n    if valid_deltas > 0:\n        avg_gain /= valid_deltas\n        avg_loss /= valid_deltas\n    \n    if avg_loss > 0:\n        rs = avg_gain / avg_loss\n        rsi[period] = 100.0 - (100.0 / (1.0 + rs))\n    else:\n        rsi[period] = 100.0 if avg_gain > 0 else 50.0\n    \n    # Calculate RSI for remaining periods\n    for i in range(period + 1, n):\n        if not np.isnan(deltas[i]):\n            if deltas[i] > 0:\n                avg_gain = (avg_gain * (period - 1) + deltas[i]) / period\n                avg_loss = avg_loss * (period - 1) / period\n            else:\n                avg_gain = avg_gain * (period - 1) / period\n                avg_loss = (avg_loss * (period - 1) - deltas[i]) / period\n            \n            if avg_loss > 0:\n                rs = avg_gain / avg_loss\n                rsi[i] = 100.0 - (100.0 / (1.0 + rs))\n            else:\n                rsi[i] = 100.0 if avg_gain > 0 else 50.0\n        else:\n            rsi[i] = rsi[i-1] if i > 0 else 50.0\n    \n    return rsi\n\n@njit(fastmath=True, cache=True)\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate Euclidean distance between two vectors with NaN handling\"\"\"\n    dist = 0.0\n    valid_dims = 0\n    \n    for i in range(len(x1)):\n        if not np.isnan(x1[i]) and not np.isnan(x2[i]):\n            diff = x1[i] - x2[i]\n            dist += diff * diff\n            valid_dims += 1\n    \n    if valid_dims > 0:\n        return np.sqrt(dist / valid_dims)  # Normalize by valid dimensions\n    else:\n        return np.inf  # No valid dimensions\n\n@njit(fastmath=True, cache=True)\ndef volatility_adaptive_knn(features, labels, query, k_base, volatility, vol_scale=2.0):\n    \"\"\"KNN with volatility-based K adjustment and robustness checks\"\"\"\n    # Validate inputs\n    if k_base < 1:\n        k_base = 5\n    if volatility < 0:\n        volatility = 0\n    if vol_scale < 0:\n        vol_scale = 2.0\n    \n    # Adjust K based on volatility\n    k = max(3, min(k_base, int(k_base * (1 - min(volatility, 0.5) * vol_scale))))\n    \n    n_samples = len(labels)\n    if n_samples < k:\n        # Not enough samples, return neutral\n        return 0.5\n    \n    # Calculate distances\n    distances = np.zeros(n_samples)\n    valid_samples = 0\n    \n    for i in range(n_samples):\n        dist = euclidean_distance(features[i], query)\n        if dist < np.inf:  # Valid distance\n            distances[valid_samples] = dist\n            valid_samples += 1\n    \n    if valid_samples < k:\n        return 0.5  # Not enough valid samples\n    \n    # Get k nearest neighbors from valid samples\n    indices = np.argsort(distances[:valid_samples])[:k]\n    \n    # Weighted voting\n    bull_score = 0.0\n    total_weight = 0.0\n    \n    for i in range(k):\n        idx = indices[i]\n        if distances[idx] > 0:\n            weight = 1.0 / (1.0 + distances[idx])\n        else:\n            weight = 1.0\n        \n        bull_score += labels[idx] * weight\n        total_weight += weight\n    \n    if total_weight > 0:\n        return bull_score / total_weight\n    else:\n        return 0.5\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef calculate_mlmi_enhanced(prices, window=10, k=5, feature_window=3):\n    \"\"\"Enhanced MLMI with volatility adaptation and robust error handling\"\"\"\n    n = len(prices)\n    mlmi_bull = np.zeros(n, dtype=np.bool_)\n    mlmi_bear = np.zeros(n, dtype=np.bool_)\n    confidence = np.zeros(n)\n    \n    # Parameter validation\n    if window < 5:\n        window = 10\n    if k < 3:\n        k = 5\n    if feature_window < 2:\n        feature_window = 3\n    \n    # Calculate RSI\n    rsi = calculate_rsi(prices)\n    \n    # Calculate volatility\n    volatility = np.zeros(n)\n    vol_window = 20\n    \n    for i in range(vol_window, n):\n        returns = np.zeros(vol_window)\n        valid_returns = 0\n        \n        for j in range(vol_window):\n            if i-j-1 >= 0 and prices[i-j-1] > 0 and prices[i-j] > 0:\n                if not np.isnan(prices[i-j]) and not np.isnan(prices[i-j-1]):\n                    returns[valid_returns] = (prices[i-j] - prices[i-j-1]) / prices[i-j-1]\n                    valid_returns += 1\n        \n        if valid_returns > 5:  # Need minimum returns\n            volatility[i] = np.std(returns[:valid_returns])\n        else:\n            volatility[i] = 0.02  # Default volatility\n    \n    # MLMI calculation\n    lookback = max(window * 10, 100)\n    \n    for i in prange(lookback, n):\n        # Prepare historical data\n        start_idx = max(0, i - lookback)\n        historical_size = i - start_idx - feature_window - 1\n        \n        if historical_size < k:\n            continue\n        \n        # Create feature matrix\n        features = np.zeros((historical_size, feature_window))\n        labels = np.zeros(historical_size)\n        valid_samples = 0\n        \n        # Fill features and labels\n        for j in range(historical_size):\n            idx = start_idx + j\n            \n            # Check if we have valid RSI values for the feature window\n            valid_features = True\n            for f in range(feature_window):\n                if np.isnan(rsi[idx + f]) or rsi[idx + f] <= 0 or rsi[idx + f] >= 100:\n                    valid_features = False\n                    break\n                features[valid_samples, f] = rsi[idx + f]\n            \n            if not valid_features:\n                continue\n            \n            # Label based on next period return\n            if idx + feature_window < n and prices[idx + feature_window] > 0 and prices[idx + feature_window - 1] > 0:\n                ret = (prices[idx + feature_window] - prices[idx + feature_window - 1]) / prices[idx + feature_window - 1]\n                if not np.isnan(ret) and abs(ret) < 0.5:  # Cap extreme returns\n                    labels[valid_samples] = 1.0 if ret > 0 else 0.0\n                    valid_samples += 1\n        \n        if valid_samples < k:\n            continue\n        \n        # Current query\n        query = np.zeros(feature_window)\n        valid_query = True\n        \n        for f in range(feature_window):\n            if i - feature_window + f >= 0:\n                query[f] = rsi[i - feature_window + f]\n                if np.isnan(query[f]) or query[f] <= 0 or query[f] >= 100:\n                    valid_query = False\n                    break\n            else:\n                valid_query = False\n                break\n        \n        if not valid_query:\n            continue\n        \n        # Adaptive KNN prediction\n        bull_prob = volatility_adaptive_knn(\n            features[:valid_samples], \n            labels[:valid_samples], \n            query, \n            k, \n            volatility[i], \n            2.0  # vol_scale\n        )\n        \n        confidence[i] = abs(bull_prob - 0.5) * 2  # Convert to confidence score\n        \n        # Generate signals with confidence threshold\n        if bull_prob > 0.65 and confidence[i] > 0.3:\n            mlmi_bull[i] = True\n        elif bull_prob < 0.35 and confidence[i] > 0.3:\n            mlmi_bear[i] = True\n    \n    return mlmi_bull, mlmi_bear, confidence\n\n# Add validation wrapper for MLMI calculation\ndef calculate_mlmi_with_validation(prices, config=None):\n    \"\"\"Calculate MLMI with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input\n        if len(prices) < config.MLMI_LOOKBACK:\n            raise ValueError(f\"Insufficient data for MLMI: need at least {config.MLMI_LOOKBACK} points\")\n        \n        # Check for valid prices\n        valid_prices = prices[~np.isnan(prices) & (prices > 0)]\n        if len(valid_prices) < len(prices) * 0.9:\n            logger.warning(f\"More than 10% invalid prices in MLMI input\")\n        \n        # Calculate MLMI\n        logger.info(\"Calculating MLMI signals...\")\n        mlmi_bull, mlmi_bear, mlmi_confidence = calculate_mlmi_enhanced(\n            prices,\n            window=config.MLMI_WINDOW,\n            k=config.MLMI_K_NEIGHBORS,\n            feature_window=config.MLMI_FEATURE_WINDOW\n        )\n        \n        # Validate output\n        total_signals = mlmi_bull.sum() + mlmi_bear.sum()\n        if total_signals == 0:\n            logger.warning(\"MLMI generated no signals - check parameters\")\n        \n        # Log statistics\n        logger.info(f\"MLMI calculation complete - Bull: {mlmi_bull.sum()}, Bear: {mlmi_bear.sum()}\")\n        \n        return mlmi_bull, mlmi_bear, mlmi_confidence\n        \n    except Exception as e:\n        logger.error(f\"Error in MLMI calculation: {str(e)}\")\n        # Return safe defaults\n        n = len(prices)\n        return np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FVG Detection with Volume Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(parallel=True, fastmath=True, cache=True)\ndef detect_fvg_with_volume(high, low, close, volume, min_gap_pct=0.001, volume_factor=1.2):\n    \"\"\"Detect Fair Value Gaps with volume confirmation and robust validation\"\"\"\n    n = len(high)\n    fvg_bull = np.zeros(n, dtype=np.bool_)\n    fvg_bear = np.zeros(n, dtype=np.bool_)\n    gap_size = np.zeros(n)\n    \n    # Parameter validation\n    if min_gap_pct <= 0:\n        min_gap_pct = 0.001\n    if volume_factor < 1:\n        volume_factor = 1.2\n    \n    # Calculate average volume with validation\n    avg_volume = np.zeros(n)\n    vol_window = 20\n    \n    for i in range(vol_window, n):\n        valid_volumes = 0\n        vol_sum = 0.0\n        \n        for j in range(vol_window):\n            if i-j >= 0 and volume[i-j] > 0 and not np.isnan(volume[i-j]):\n                vol_sum += volume[i-j]\n                valid_volumes += 1\n        \n        if valid_volumes > 5:  # Need minimum valid volumes\n            avg_volume[i] = vol_sum / valid_volumes\n        else:\n            avg_volume[i] = 0  # Will skip this bar\n    \n    for i in prange(2, n):\n        # Skip if no average volume or invalid data\n        if avg_volume[i] <= 0:\n            continue\n        \n        # Validate all required data points\n        if (np.isnan(high[i]) or np.isnan(low[i]) or np.isnan(close[i]) or\n            np.isnan(high[i-2]) or np.isnan(low[i-2]) or np.isnan(close[i-1]) or\n            np.isnan(volume[i])):\n            continue\n        \n        # Ensure positive prices\n        if close[i-1] <= 0 or high[i] <= 0 or low[i] <= 0 or high[i-2] <= 0 or low[i-2] <= 0:\n            continue\n        \n        # Volume confirmation\n        vol_confirmed = volume[i] > avg_volume[i] * volume_factor\n        \n        # Bullish FVG: gap up\n        gap_up = low[i] - high[i-2]\n        if gap_up > 0 and vol_confirmed:\n            gap_pct = gap_up / close[i-1]\n            # Additional validation: gap shouldn't be too large (> 10%)\n            if min_gap_pct < gap_pct < 0.1:\n                fvg_bull[i] = True\n                gap_size[i] = gap_pct\n        \n        # Bearish FVG: gap down\n        gap_down = low[i-2] - high[i]\n        if gap_down > 0 and vol_confirmed:\n            gap_pct = gap_down / close[i-1]\n            # Additional validation: gap shouldn't be too large (> 10%)\n            if min_gap_pct < gap_pct < 0.1:\n                fvg_bear[i] = True\n                gap_size[i] = -gap_pct\n    \n    return fvg_bull, fvg_bear, gap_size\n\n# Add validation wrapper for FVG calculation\ndef calculate_fvg_with_validation(df, config=None):\n    \"\"\"Calculate FVG with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate required columns\n        required_columns = ['high', 'low', 'close', 'volume']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns for FVG: {missing_columns}\")\n        \n        # Validate data length\n        if len(df) < 50:  # Need enough data for volume average\n            raise ValueError(f\"Insufficient data for FVG: need at least 50 points\")\n        \n        # Calculate FVG\n        logger.info(\"Calculating FVG signals...\")\n        fvg_bull, fvg_bear, fvg_size = detect_fvg_with_volume(\n            df['high'].values,\n            df['low'].values,\n            df['close'].values,\n            df['volume'].values,\n            min_gap_pct=config.FVG_MIN_GAP_PCT,\n            volume_factor=config.FVG_VOLUME_FACTOR\n        )\n        \n        # Validate output\n        total_gaps = fvg_bull.sum() + fvg_bear.sum()\n        if total_gaps == 0:\n            logger.warning(\"No FVG detected - this is normal for some market conditions\")\n        \n        # Check for excessive gaps\n        gap_ratio = total_gaps / len(df)\n        if gap_ratio > 0.1:  # More than 10% bars have gaps\n            logger.warning(f\"High number of gaps detected: {gap_ratio:.1%} - consider adjusting parameters\")\n        \n        # Log statistics\n        logger.info(f\"FVG calculation complete - Bull: {fvg_bull.sum()}, Bear: {fvg_bear.sum()}\")\n        \n        return fvg_bull, fvg_bear, fvg_size\n        \n    except Exception as e:\n        logger.error(f\"Error in FVG calculation: {str(e)}\")\n        # Return safe defaults\n        n = len(df)\n        return np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NW-RQK → MLMI → FVG Synergy Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(parallel=True, fastmath=True, cache=True)\ndef detect_nwrqk_mlmi_fvg_synergy(nwrqk_bull, nwrqk_bear, nwrqk_strength,\n                                  mlmi_bull, mlmi_bear, mlmi_confidence,\n                                  fvg_bull, fvg_bear, fvg_size,\n                                  window=30):\n    \"\"\"Detect NW-RQK → MLMI → FVG synergy pattern with robust state management\"\"\"\n    n = len(nwrqk_bull)\n    synergy_bull = np.zeros(n, dtype=np.bool_)\n    synergy_bear = np.zeros(n, dtype=np.bool_)\n    synergy_strength = np.zeros(n)\n    \n    # Parameter validation\n    if window < 10:\n        window = 30\n    \n    # State tracking arrays\n    nwrqk_active_bull = np.zeros(n, dtype=np.bool_)\n    nwrqk_active_bear = np.zeros(n, dtype=np.bool_)\n    mlmi_confirmed_bull = np.zeros(n, dtype=np.bool_)\n    mlmi_confirmed_bear = np.zeros(n, dtype=np.bool_)\n    \n    # State timing for decay\n    nwrqk_bull_time = np.full(n, -1, dtype=np.int64)\n    nwrqk_bear_time = np.full(n, -1, dtype=np.int64)\n    mlmi_bull_time = np.full(n, -1, dtype=np.int64) \n    mlmi_bear_time = np.full(n, -1, dtype=np.int64)\n    \n    # Track last synergy to prevent duplicate signals\n    last_bull_synergy = -window\n    last_bear_synergy = -window\n    \n    for i in prange(1, n):\n        # Carry forward states\n        if i > 0:\n            nwrqk_active_bull[i] = nwrqk_active_bull[i-1]\n            nwrqk_active_bear[i] = nwrqk_active_bear[i-1]\n            mlmi_confirmed_bull[i] = mlmi_confirmed_bull[i-1]\n            mlmi_confirmed_bear[i] = mlmi_confirmed_bear[i-1]\n            nwrqk_bull_time[i] = nwrqk_bull_time[i-1]\n            nwrqk_bear_time[i] = nwrqk_bear_time[i-1]\n            mlmi_bull_time[i] = mlmi_bull_time[i-1]\n            mlmi_bear_time[i] = mlmi_bear_time[i-1]\n        \n        # Step 1: NW-RQK signal activation with strength validation\n        if nwrqk_bull[i] and nwrqk_strength[i] > 0.5 and not np.isnan(nwrqk_strength[i]):\n            nwrqk_active_bull[i] = True\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bull_time[i] = i\n            nwrqk_bear_time[i] = -1\n            mlmi_bear_time[i] = -1\n        elif nwrqk_bear[i] and nwrqk_strength[i] > 0.5 and not np.isnan(nwrqk_strength[i]):\n            nwrqk_active_bear[i] = True\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bear_time[i] = i\n            nwrqk_bull_time[i] = -1\n            mlmi_bull_time[i] = -1\n        \n        # Step 2: MLMI confirmation with confidence validation\n        if nwrqk_active_bull[i] and mlmi_bull[i] and mlmi_confidence[i] > 0.3 and not np.isnan(mlmi_confidence[i]):\n            mlmi_confirmed_bull[i] = True\n            mlmi_bull_time[i] = i\n        elif nwrqk_active_bear[i] and mlmi_bear[i] and mlmi_confidence[i] > 0.3 and not np.isnan(mlmi_confidence[i]):\n            mlmi_confirmed_bear[i] = True\n            mlmi_bear_time[i] = i\n        \n        # Step 3: FVG validation for entry with minimum spacing\n        if mlmi_confirmed_bull[i] and fvg_bull[i] and (i - last_bull_synergy) >= 5:\n            synergy_bull[i] = True\n            last_bull_synergy = i\n            \n            # Calculate synergy strength with validation\n            strength_components = np.zeros(3)\n            \n            # Find recent NW-RQK strength\n            if nwrqk_bull_time[i] >= 0:\n                time_since_nwrqk = i - nwrqk_bull_time[i]\n                if time_since_nwrqk < window:\n                    for j in range(max(0, nwrqk_bull_time[i]), min(i + 1, nwrqk_bull_time[i] + window)):\n                        if nwrqk_bull[j] and not np.isnan(nwrqk_strength[j]):\n                            strength_components[0] = max(strength_components[0], nwrqk_strength[j])\n            \n            # MLMI confidence\n            if not np.isnan(mlmi_confidence[i]):\n                strength_components[1] = mlmi_confidence[i]\n            \n            # FVG size\n            if not np.isnan(fvg_size[i]):\n                strength_components[2] = min(abs(fvg_size[i]) * 100, 1.0)\n            \n            # Calculate weighted strength with validation\n            valid_components = 0\n            total_strength = 0.0\n            for comp in strength_components:\n                if comp > 0 and not np.isnan(comp):\n                    total_strength += comp\n                    valid_components += 1\n            \n            if valid_components > 0:\n                synergy_strength[i] = total_strength / valid_components\n            else:\n                synergy_strength[i] = 0.5  # Default strength\n            \n            # Reset states after signal\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bull_time[i] = -1\n            mlmi_bull_time[i] = -1\n            \n        elif mlmi_confirmed_bear[i] and fvg_bear[i] and (i - last_bear_synergy) >= 5:\n            synergy_bear[i] = True\n            last_bear_synergy = i\n            \n            # Calculate synergy strength with validation\n            strength_components = np.zeros(3)\n            \n            # Find recent NW-RQK strength\n            if nwrqk_bear_time[i] >= 0:\n                time_since_nwrqk = i - nwrqk_bear_time[i]\n                if time_since_nwrqk < window:\n                    for j in range(max(0, nwrqk_bear_time[i]), min(i + 1, nwrqk_bear_time[i] + window)):\n                        if nwrqk_bear[j] and not np.isnan(nwrqk_strength[j]):\n                            strength_components[0] = max(strength_components[0], nwrqk_strength[j])\n            \n            # MLMI confidence\n            if not np.isnan(mlmi_confidence[i]):\n                strength_components[1] = mlmi_confidence[i]\n            \n            # FVG size\n            if not np.isnan(fvg_size[i]):\n                strength_components[2] = min(abs(fvg_size[i]) * 100, 1.0)\n            \n            # Calculate weighted strength with validation\n            valid_components = 0\n            total_strength = 0.0\n            for comp in strength_components:\n                if comp > 0 and not np.isnan(comp):\n                    total_strength += comp\n                    valid_components += 1\n            \n            if valid_components > 0:\n                synergy_strength[i] = total_strength / valid_components\n            else:\n                synergy_strength[i] = 0.5  # Default strength\n            \n            # Reset states after signal\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bear_time[i] = -1\n            mlmi_bear_time[i] = -1\n        \n        # State decay - reset if signals are too old\n        if nwrqk_bull_time[i] >= 0 and i - nwrqk_bull_time[i] > window:\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bull_time[i] = -1\n            mlmi_bull_time[i] = -1\n        \n        if nwrqk_bear_time[i] >= 0 and i - nwrqk_bear_time[i] > window:\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bear_time[i] = -1\n            mlmi_bear_time[i] = -1\n    \n    return synergy_bull, synergy_bear, synergy_strength\n\n# Add validation wrapper for synergy detection\ndef detect_synergy_with_validation(nwrqk_data, mlmi_data, fvg_data, config=None):\n    \"\"\"Detect synergies with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input lengths match\n        n = len(nwrqk_data[0])\n        if len(mlmi_data[0]) != n or len(fvg_data[0]) != n:\n            raise ValueError(\"Input data lengths do not match\")\n        \n        # Detect synergies\n        logger.info(\"Detecting NW-RQK → MLMI → FVG synergies...\")\n        synergy_bull, synergy_bear, synergy_strength = detect_nwrqk_mlmi_fvg_synergy(\n            nwrqk_data[0], nwrqk_data[1], nwrqk_data[2],  # bull, bear, strength\n            mlmi_data[0], mlmi_data[1], mlmi_data[2],     # bull, bear, confidence\n            fvg_data[0], fvg_data[1], fvg_data[2],        # bull, bear, size\n            window=config.SYNERGY_WINDOW\n        )\n        \n        # Validate output\n        total_synergies = synergy_bull.sum() + synergy_bear.sum()\n        if total_synergies == 0:\n            logger.warning(\"No synergies detected - consider adjusting parameters\")\n        \n        # Check synergy rate\n        synergy_rate = total_synergies / n\n        if synergy_rate > 0.1:  # More than 10% bars have synergies\n            logger.warning(f\"High synergy rate: {synergy_rate:.1%} - may indicate overfitting\")\n        \n        # Log statistics\n        logger.info(f\"Synergy detection complete - Bull: {synergy_bull.sum()}, Bear: {synergy_bear.sum()}\")\n        avg_strength = synergy_strength[synergy_strength > 0].mean() if (synergy_strength > 0).any() else 0\n        logger.info(f\"Average synergy strength: {avg_strength:.3f}\")\n        \n        return synergy_bull, synergy_bear, synergy_strength\n        \n    except Exception as e:\n        logger.error(f\"Error in synergy detection: {str(e)}\")\n        # Return safe defaults\n        return np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Strategy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_nwrqk_mlmi_fvg_strategy(df_30m, df_5m):\n    \"\"\"Execute the complete NW-RQK → MLMI → FVG strategy with robust error handling\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY\")\n    logger.info(\"=\"*60)\n    \n    start_time = time.time()\n    \n    try:\n        # Performance tracking\n        performance_metrics = {\n            'start_time': datetime.now(),\n            'errors': [],\n            'warnings': []\n        }\n        \n        # 1. Calculate NW-RQK signals with validation\n        logger.info(\"\\n1. Calculating NW-RQK signals...\")\n        nwrqk_calc_start = time.time()\n        \n        prices = df_30m['close'].values\n        nwrqk_values, nwrqk_bull, nwrqk_bear, nwrqk_strength = calculate_nwrqk_with_validation(\n            prices, StrategyConfig\n        )\n        \n        performance_metrics['nwrqk_time'] = time.time() - nwrqk_calc_start\n        logger.info(f\"   - NW-RQK calculation time: {performance_metrics['nwrqk_time']:.2f}s\")\n        logger.info(f\"   - Bull signals: {nwrqk_bull.sum()}\")\n        logger.info(f\"   - Bear signals: {nwrqk_bear.sum()}\")\n        \n        # 2. Calculate MLMI signals with validation\n        logger.info(\"\\n2. Calculating MLMI signals...\")\n        mlmi_calc_start = time.time()\n        \n        mlmi_bull, mlmi_bear, mlmi_confidence = calculate_mlmi_with_validation(\n            prices, StrategyConfig\n        )\n        \n        performance_metrics['mlmi_time'] = time.time() - mlmi_calc_start\n        logger.info(f\"   - MLMI calculation time: {performance_metrics['mlmi_time']:.2f}s\")\n        logger.info(f\"   - Bull signals: {mlmi_bull.sum()}\")\n        logger.info(f\"   - Bear signals: {mlmi_bear.sum()}\")\n        \n        # 3. Calculate FVG on 5-minute data with validation\n        logger.info(\"\\n3. Calculating FVG signals on 5m data...\")\n        fvg_calc_start = time.time()\n        \n        # Check if FVG columns exist to avoid recalculation\n        if 'fvg_bull' not in df_5m.columns:\n            fvg_bull_5m, fvg_bear_5m, fvg_size_5m = calculate_fvg_with_validation(\n                df_5m, StrategyConfig\n            )\n            df_5m['fvg_bull'] = fvg_bull_5m\n            df_5m['fvg_bear'] = fvg_bear_5m\n            df_5m['fvg_size'] = fvg_size_5m\n        \n        performance_metrics['fvg_time'] = time.time() - fvg_calc_start\n        logger.info(f\"   - FVG calculation time: {performance_metrics['fvg_time']:.2f}s\")\n        logger.info(f\"   - Bull FVGs: {df_5m['fvg_bull'].sum()}\")\n        logger.info(f\"   - Bear FVGs: {df_5m['fvg_bear'].sum()}\")\n        \n        # 4. Map 5m FVG to 30m timeframe with validation\n        logger.info(\"\\n4. Mapping FVG signals to 30m timeframe...\")\n        \n        try:\n            # Resample FVG signals\n            fvg_resampled = df_5m[['fvg_bull', 'fvg_bear', 'fvg_size']].resample('30min').agg({\n                'fvg_bull': 'max',\n                'fvg_bear': 'max',\n                'fvg_size': 'mean'\n            })\n            \n            # Align with 30m data\n            fvg_aligned = fvg_resampled.reindex(df_30m.index, method='ffill')\n            fvg_aligned = fvg_aligned.fillna(False)\n            \n            # Validate alignment\n            if len(fvg_aligned) != len(df_30m):\n                raise ValueError(\"FVG alignment failed - length mismatch\")\n                \n        except Exception as e:\n            logger.error(f\"Error in FVG resampling: {str(e)}\")\n            # Create empty FVG signals as fallback\n            fvg_aligned = pd.DataFrame(index=df_30m.index)\n            fvg_aligned['fvg_bull'] = False\n            fvg_aligned['fvg_bear'] = False\n            fvg_aligned['fvg_size'] = 0\n            performance_metrics['errors'].append(f\"FVG resampling error: {str(e)}\")\n        \n        # 5. Detect synergies with validation\n        logger.info(\"\\n5. Detecting NW-RQK → MLMI → FVG synergies...\")\n        synergy_calc_start = time.time()\n        \n        synergy_bull, synergy_bear, synergy_strength = detect_synergy_with_validation(\n            (nwrqk_bull, nwrqk_bear, nwrqk_strength),\n            (mlmi_bull, mlmi_bear, mlmi_confidence),\n            (fvg_aligned['fvg_bull'].values.astype(np.bool_),\n             fvg_aligned['fvg_bear'].values.astype(np.bool_),\n             fvg_aligned['fvg_size'].fillna(0).values),\n            StrategyConfig\n        )\n        \n        performance_metrics['synergy_time'] = time.time() - synergy_calc_start\n        logger.info(f\"   - Synergy detection time: {performance_metrics['synergy_time']:.2f}s\")\n        logger.info(f\"   - Bull synergies: {synergy_bull.sum()}\")\n        logger.info(f\"   - Bear synergies: {synergy_bear.sum()}\")\n        logger.info(f\"   - Total signals: {synergy_bull.sum() + synergy_bear.sum()}\")\n        \n        # 6. Create signals DataFrame with risk management\n        signals = pd.DataFrame(index=df_30m.index)\n        signals['synergy_bull'] = synergy_bull\n        signals['synergy_bear'] = synergy_bear\n        signals['synergy_strength'] = synergy_strength\n        signals['price'] = df_30m['close']\n        \n        # Add volatility for risk management\n        signals['volatility'] = df_30m['volatility']\n        \n        # Generate position signals with risk filters\n        signals['signal'] = 0\n        \n        # Apply risk filters\n        max_volatility = signals['volatility'].quantile(0.95)\n        min_strength = 0.3\n        \n        # Bull signals with risk filters\n        valid_bull = (signals['synergy_bull'] & \n                     (signals['volatility'] < max_volatility) & \n                     (signals['synergy_strength'] > min_strength))\n        signals.loc[valid_bull, 'signal'] = 1\n        \n        # Bear signals with risk filters  \n        valid_bear = (signals['synergy_bear'] & \n                     (signals['volatility'] < max_volatility) & \n                     (signals['synergy_strength'] > min_strength))\n        signals.loc[valid_bear, 'signal'] = -1\n        \n        # Add signal quality metrics\n        signals['signal_quality'] = signals['synergy_strength'] * (1 - signals['volatility'] / max_volatility)\n        \n        # Performance summary\n        performance_metrics['total_time'] = time.time() - start_time\n        performance_metrics['signals_generated'] = (signals['signal'] != 0).sum()\n        performance_metrics['signals_filtered'] = (synergy_bull.sum() + synergy_bear.sum()) - performance_metrics['signals_generated']\n        \n        logger.info(f\"\\nTotal execution time: {performance_metrics['total_time']:.2f} seconds\")\n        logger.info(f\"Signals after risk filtering: {performance_metrics['signals_generated']}\")\n        logger.info(f\"Signals filtered by risk management: {performance_metrics['signals_filtered']}\")\n        \n        # Store performance metrics in signals\n        signals.attrs['performance_metrics'] = performance_metrics\n        \n        return signals\n        \n    except Exception as e:\n        logger.error(f\"Critical error in strategy execution: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        \n        # Return empty signals on error\n        signals = pd.DataFrame(index=df_30m.index)\n        signals['signal'] = 0\n        signals['price'] = df_30m['close']\n        signals.attrs['error'] = str(e)\n        return signals\n\n# Import traceback for error handling\nimport traceback\n\n# Run the strategy with error handling\ntry:\n    signals = run_nwrqk_mlmi_fvg_strategy(df_30m, df_5m)\n    logger.info(\"Strategy execution completed successfully\")\nexcept Exception as e:\n    logger.error(f\"Failed to run strategy: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VectorBT Backtesting with Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_vectorbt_backtest(signals, initial_capital=100000, position_size=0.1, \n                         sl_pct=0.02, tp_pct=0.03, fees=0.001):\n    \"\"\"Run VectorBT backtest with dynamic position sizing and risk management\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"VECTORBT BACKTEST WITH RISK MANAGEMENT\")\n    logger.info(\"=\"*60)\n    \n    backtest_start = time.time()\n    \n    try:\n        # Validate signals\n        if signals is None or len(signals) == 0:\n            raise ValueError(\"Invalid signals data\")\n        \n        if 'signal' not in signals.columns or 'price' not in signals.columns:\n            raise ValueError(\"Signals must contain 'signal' and 'price' columns\")\n        \n        # Prepare data\n        price = signals['price']\n        entries = signals['signal'] == 1\n        exits = signals['signal'] == -1\n        \n        # Check if we have any signals\n        if not entries.any() and not exits.any():\n            logger.warning(\"No trading signals generated - check strategy parameters\")\n            return None, None\n        \n        # Dynamic position sizing based on signal strength and volatility\n        if 'synergy_strength' in signals.columns and 'volatility' in signals.columns:\n            # Scale position size by signal strength (0.5 to 1.5x base size)\n            strength_factor = 0.5 + 0.5 * np.minimum(signals['synergy_strength'], 1.0)\n            \n            # Reduce position size in high volatility (0.5 to 1.0x)\n            vol_percentile = signals['volatility'].rolling(252).apply(\n                lambda x: stats.rankdata(x)[-1] / len(x) if len(x) > 0 else 0.5\n            ).fillna(0.5)\n            vol_factor = 1.0 - 0.5 * vol_percentile\n            \n            # Combined position sizing\n            position_sizes = position_size * strength_factor * vol_factor\n            position_sizes = position_sizes.fillna(position_size)\n            \n            # Apply position size limits\n            position_sizes = np.clip(position_sizes, \n                                   position_size * 0.5,  # Min 50% of base\n                                   position_size * 1.5)  # Max 150% of base\n        else:\n            position_sizes = position_size\n        \n        # Risk management parameters\n        sl_pct = StrategyConfig.STOP_LOSS_PCT\n        tp_pct = StrategyConfig.TAKE_PROFIT_PCT\n        \n        # Run backtest with VectorBT\n        portfolio = vbt.Portfolio.from_signals(\n            price,\n            entries=entries,\n            exits=exits,\n            size=position_sizes,\n            size_type='percent',\n            init_cash=initial_capital,\n            fees=fees,\n            slippage=StrategyConfig.SLIPPAGE,\n            freq='30min',\n            sl_stop=sl_pct,\n            tp_stop=tp_pct,\n            stop_exit_price='close',  # Use close price for stops\n            upon_stop_exit='close_position',  # Close full position on stop\n            raise_reject=False,  # Don't raise on rejected orders\n            log=True  # Enable logging for debugging\n        )\n        \n        # Calculate metrics with error handling\n        try:\n            stats = portfolio.stats()\n            \n            # Additional risk metrics\n            returns = portfolio.returns()\n            \n            # Maximum consecutive losses\n            trade_returns = portfolio.trades.records_readable['Return [%]'].values\n            losing_trades = trade_returns < 0\n            max_consecutive_losses = 0\n            current_losses = 0\n            for loss in losing_trades:\n                if loss:\n                    current_losses += 1\n                    max_consecutive_losses = max(max_consecutive_losses, current_losses)\n                else:\n                    current_losses = 0\n            \n            # Risk-adjusted metrics\n            downside_returns = returns[returns < 0]\n            if len(downside_returns) > 0:\n                downside_std = downside_returns.std()\n                sortino_ratio = (returns.mean() / downside_std) * np.sqrt(252 * 48) if downside_std > 0 else 0\n            else:\n                sortino_ratio = np.inf\n            \n            # Add custom metrics to stats\n            stats['Max Consecutive Losses'] = max_consecutive_losses\n            stats['Sortino Ratio (Custom)'] = sortino_ratio\n            stats['Average Position Size'] = position_sizes.mean() if isinstance(position_sizes, pd.Series) else position_size\n            \n        except Exception as e:\n            logger.error(f\"Error calculating portfolio statistics: {str(e)}\")\n            stats = {'Error': str(e)}\n        \n        logger.info(f\"\\nBacktest execution time: {time.time() - backtest_start:.2f} seconds\")\n        \n        # Log key performance metrics\n        if 'Total Return [%]' in stats:\n            logger.info(\"\\nKey Performance Metrics:\")\n            logger.info(f\"Total Return: {stats.get('Total Return [%]', 'N/A'):.2f}%\")\n            logger.info(f\"Sharpe Ratio: {stats.get('Sharpe Ratio', 'N/A'):.2f}\")\n            logger.info(f\"Max Drawdown: {stats.get('Max Drawdown [%]', 'N/A'):.2f}%\")\n            logger.info(f\"Win Rate: {stats.get('Win Rate [%]', 'N/A'):.2f}%\")\n            logger.info(f\"Total Trades: {stats.get('Total Trades', 'N/A')}\")\n            \n            # Calculate annual metrics\n            if price.index[-1] and price.index[0]:\n                n_years = (price.index[-1] - price.index[0]).days / 365.25\n                if n_years > 0 and stats.get('Total Return [%]'):\n                    annual_return = (1 + stats['Total Return [%]'] / 100) ** (1 / n_years) - 1\n                    trades_per_year = stats.get('Total Trades', 0) / n_years\n                    \n                    logger.info(f\"\\nAnnualized Return: {annual_return * 100:.2f}%\")\n                    logger.info(f\"Trades per Year: {trades_per_year:.0f}\")\n            \n            # Risk warnings\n            if stats.get('Max Drawdown [%]', 0) > StrategyConfig.MAX_DRAWDOWN_LIMIT * 100:\n                logger.warning(f\"⚠️  Maximum drawdown exceeds limit: {stats['Max Drawdown [%]']:.2f}% > {StrategyConfig.MAX_DRAWDOWN_LIMIT * 100:.0f}%\")\n            \n            if stats.get('Win Rate [%]', 0) < 40:\n                logger.warning(f\"⚠️  Low win rate: {stats['Win Rate [%]']:.2f}%\")\n        \n        return portfolio, stats\n        \n    except Exception as e:\n        logger.error(f\"Critical error in backtesting: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        return None, {'Error': str(e)}\n\n# Run backtest with error handling\ntry:\n    portfolio, stats = run_vectorbt_backtest(signals)\n    if portfolio is not None:\n        logger.info(\"Backtest completed successfully\")\n    else:\n        logger.error(\"Backtest failed - check logs for details\")\nexcept Exception as e:\n    logger.error(f\"Failed to run backtest: {str(e)}\")\n    portfolio, stats = None, {'Error': str(e)}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_dashboard(signals, portfolio):\n",
    "    \"\"\"Create comprehensive performance dashboard\"\"\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Portfolio Value', 'Monthly Returns',\n",
    "            'Cumulative Returns', 'Drawdown',\n",
    "            'Trade Distribution', 'Signal Strength vs Returns',\n",
    "            'Rolling Sharpe Ratio', 'Win Rate by Month'\n",
    "        ),\n",
    "        row_heights=[0.25, 0.25, 0.25, 0.25],\n",
    "        specs=[\n",
    "            [{\"secondary_y\": False}, {\"type\": \"bar\"}],\n",
    "            [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "            [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"secondary_y\": False}, {\"type\": \"bar\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Portfolio Value\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio.value().index,\n",
    "            y=portfolio.value().values,\n",
    "            name='Portfolio Value',\n",
    "            line=dict(color='cyan', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Monthly Returns\n",
    "    monthly_returns = portfolio.returns().resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "    colors = ['green' if r > 0 else 'red' for r in monthly_returns]\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=monthly_returns.index,\n",
    "            y=monthly_returns.values * 100,\n",
    "            name='Monthly Returns',\n",
    "            marker_color=colors\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Cumulative Returns\n",
    "    cum_returns = (1 + portfolio.returns()).cumprod() - 1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=cum_returns.index,\n",
    "            y=cum_returns.values * 100,\n",
    "            name='Cumulative Returns',\n",
    "            fill='tozeroy',\n",
    "            line=dict(color='lightblue')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Drawdown\n",
    "    drawdown = portfolio.drawdown() * 100\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=drawdown.index,\n",
    "            y=-drawdown.values,\n",
    "            name='Drawdown',\n",
    "            fill='tozeroy',\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Trade Distribution\n",
    "    trade_returns = portfolio.trades.records_readable['Return [%]'].values\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=trade_returns,\n",
    "            nbinsx=50,\n",
    "            name='Trade Returns',\n",
    "            marker_color='purple'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Signal Strength vs Returns\n",
    "    trade_records = portfolio.trades.records_readable\n",
    "    entry_times = pd.to_datetime(trade_records['Entry Timestamp'])\n",
    "    signal_strengths = []\n",
    "    for entry_time in entry_times:\n",
    "        idx = signals.index.get_indexer([entry_time], method='nearest')[0]\n",
    "        if idx < len(signals):\n",
    "            signal_strengths.append(signals.iloc[idx]['synergy_strength'])\n",
    "        else:\n",
    "            signal_strengths.append(0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=signal_strengths,\n",
    "            y=trade_returns,\n",
    "            mode='markers',\n",
    "            name='Strength vs Return',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=trade_returns,\n",
    "                colorscale='RdYlGn',\n",
    "                showscale=True\n",
    "            )\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # 7. Rolling Sharpe Ratio\n",
    "    rolling_sharpe = portfolio.sharpe_ratio(rolling=252)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rolling_sharpe.index,\n",
    "            y=rolling_sharpe.values,\n",
    "            name='Rolling Sharpe',\n",
    "            line=dict(color='orange')\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Win Rate by Month\n",
    "    trades_df = trade_records.copy()\n",
    "    trades_df['Month'] = pd.to_datetime(trades_df['Entry Timestamp']).dt.to_period('M')\n",
    "    monthly_stats = trades_df.groupby('Month').agg({\n",
    "        'Return [%]': ['count', lambda x: (x > 0).sum() / len(x) * 100]\n",
    "    })\n",
    "    monthly_stats.columns = ['Count', 'Win Rate']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=monthly_stats.index.astype(str),\n",
    "            y=monthly_stats['Win Rate'],\n",
    "            name='Win Rate %',\n",
    "            marker_color='lightgreen'\n",
    "        ),\n",
    "        row=4, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"NW-RQK → MLMI → FVG Synergy Performance Dashboard\",\n",
    "        showlegend=False,\n",
    "        height=1600,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Return %\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Signal Strength\", row=3, col=2)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=4, col=1)\n",
    "    fig.update_xaxes(title_text=\"Month\", row=4, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Value ($)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Return %\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Return %\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Drawdown %\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Return %\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Sharpe Ratio\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Win Rate %\", row=4, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = create_performance_dashboard(signals, portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monte Carlo Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(parallel=True, fastmath=True)\ndef monte_carlo_simulation(returns, n_simulations=1000, n_periods=252):\n    \"\"\"Run Monte Carlo simulation for confidence intervals with validation\"\"\"\n    n_returns = len(returns)\n    \n    # Validate inputs\n    if n_returns < 10:\n        raise ValueError(\"Insufficient returns for Monte Carlo simulation\")\n    \n    if n_simulations < 100:\n        n_simulations = 100\n    \n    if n_periods < 10:\n        n_periods = 252\n    \n    final_values = np.zeros(n_simulations)\n    \n    # Filter out extreme returns to avoid unrealistic simulations\n    valid_returns = returns[~np.isnan(returns)]\n    valid_returns = valid_returns[np.abs(valid_returns) < 0.5]  # Cap at 50% moves\n    \n    if len(valid_returns) < 10:\n        raise ValueError(\"Insufficient valid returns after filtering\")\n    \n    for sim in prange(n_simulations):\n        # Bootstrap sample returns\n        sim_returns = np.zeros(n_periods)\n        for i in range(n_periods):\n            idx = np.random.randint(0, len(valid_returns))\n            sim_returns[i] = valid_returns[idx]\n        \n        # Calculate final value with compound returns\n        cumulative_return = 1.0\n        for ret in sim_returns:\n            cumulative_return *= (1 + ret)\n        \n        final_values[sim] = cumulative_return\n    \n    return final_values\n\ndef run_monte_carlo_analysis(portfolio):\n    \"\"\"Run Monte Carlo analysis for strategy validation with comprehensive checks\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"MONTE CARLO VALIDATION\")\n    logger.info(\"=\"*60)\n    \n    mc_start = time.time()\n    \n    try:\n        # Validate portfolio\n        if portfolio is None:\n            logger.error(\"No portfolio provided for Monte Carlo analysis\")\n            return None, None\n        \n        # Get trade returns with validation\n        try:\n            trades_df = portfolio.trades.records_readable\n            if len(trades_df) == 0:\n                logger.error(\"No trades found in portfolio\")\n                return None, None\n            \n            trade_returns = trades_df['Return [%]'].values / 100\n            \n            # Filter out invalid returns\n            valid_mask = ~np.isnan(trade_returns) & (np.abs(trade_returns) < 5.0)  # Cap at 500% moves\n            trade_returns = trade_returns[valid_mask]\n            \n            if len(trade_returns) < 10:\n                logger.error(f\"Insufficient valid trades for Monte Carlo: {len(trade_returns)}\")\n                return None, None\n            \n        except Exception as e:\n            logger.error(f\"Error extracting trade returns: {str(e)}\")\n            return None, None\n        \n        # Run simulation with error handling\n        try:\n            final_values = monte_carlo_simulation(trade_returns, n_simulations=10000)\n        except Exception as e:\n            logger.error(f\"Error in Monte Carlo simulation: {str(e)}\")\n            # Try with fewer simulations\n            try:\n                logger.warning(\"Retrying with 1000 simulations...\")\n                final_values = monte_carlo_simulation(trade_returns, n_simulations=1000)\n            except:\n                return None, None\n        \n        # Calculate statistics\n        mc_returns = (final_values - 1) * 100\n        percentiles = np.percentile(mc_returns, [5, 25, 50, 75, 95])\n        \n        logger.info(f\"\\nMonte Carlo simulation completed in {time.time() - mc_start:.2f} seconds\")\n        logger.info(f\"Based on {len(trade_returns)} historical trades\")\n        logger.info(\"\\nConfidence Intervals for Annual Returns:\")\n        logger.info(f\"5th percentile:  {percentiles[0]:.2f}%\")\n        logger.info(f\"25th percentile: {percentiles[1]:.2f}%\")\n        logger.info(f\"Median:          {percentiles[2]:.2f}%\")\n        logger.info(f\"75th percentile: {percentiles[3]:.2f}%\")\n        logger.info(f\"95th percentile: {percentiles[4]:.2f}%\")\n        \n        # Risk metrics\n        prob_profit = (mc_returns > 0).mean() * 100\n        prob_loss_10pct = (mc_returns < -10).mean() * 100\n        prob_gain_20pct = (mc_returns > 20).mean() * 100\n        expected_return = mc_returns.mean()\n        return_std = mc_returns.std()\n        \n        logger.info(f\"\\nRisk Metrics:\")\n        logger.info(f\"Probability of Profit: {prob_profit:.1f}%\")\n        logger.info(f\"Probability of >20% Gain: {prob_gain_20pct:.1f}%\")\n        logger.info(f\"Probability of >10% Loss: {prob_loss_10pct:.1f}%\")\n        logger.info(f\"Expected Return: {expected_return:.2f}%\")\n        logger.info(f\"Return Std Dev: {return_std:.2f}%\")\n        \n        # Create visualization with error handling\n        try:\n            fig = go.Figure()\n            \n            # Add histogram\n            fig.add_trace(go.Histogram(\n                x=mc_returns,\n                nbinsx=100,\n                name='Simulated Returns',\n                marker_color='lightblue',\n                opacity=0.7,\n                showlegend=False\n            ))\n            \n            # Add percentile lines\n            colors = ['red', 'orange', 'green', 'orange', 'red']\n            for i, (p, label) in enumerate(zip(percentiles, ['5%', '25%', '50%', '75%', '95%'])):\n                fig.add_vline(\n                    x=p, \n                    line_dash=\"dash\", \n                    line_color=colors[i],\n                    annotation_text=f\"{label}: {p:.1f}%\",\n                    annotation_position=\"top\"\n                )\n            \n            # Add zero line\n            fig.add_vline(x=0, line_dash=\"solid\", line_color=\"black\", line_width=2)\n            \n            fig.update_layout(\n                title=f\"Monte Carlo Simulation Results ({len(final_values):,} simulations)\",\n                xaxis_title=\"Annual Return %\",\n                yaxis_title=\"Frequency\",\n                template='plotly_dark',\n                height=600,\n                showlegend=False\n            )\n            \n            fig.show()\n            \n        except Exception as e:\n            logger.error(f\"Error creating Monte Carlo visualization: {str(e)}\")\n        \n        return mc_returns, percentiles\n        \n    except Exception as e:\n        logger.error(f\"Critical error in Monte Carlo analysis: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        return None, None\n\n# Run Monte Carlo validation with error handling\nif portfolio is not None:\n    try:\n        mc_returns, percentiles = run_monte_carlo_analysis(portfolio)\n        if mc_returns is not None:\n            logger.info(\"Monte Carlo analysis completed successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to run Monte Carlo analysis: {str(e)}\")\n        mc_returns, percentiles = None, None\nelse:\n    logger.warning(\"Skipping Monte Carlo analysis - no valid portfolio\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics and Trade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(signals, portfolio, stats):\n",
    "    \"\"\"Generate comprehensive performance report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Time period analysis\n",
    "    start_date = signals.index[0]\n",
    "    end_date = signals.index[-1]\n",
    "    n_years = (end_date - start_date).days / 365.25\n",
    "    \n",
    "    print(f\"\\nBacktest Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Duration: {n_years:.1f} years\")\n",
    "    \n",
    "    # Trade analysis\n",
    "    trades = portfolio.trades.records_readable\n",
    "    total_trades = len(trades)\n",
    "    winning_trades = len(trades[trades['Return [%]'] > 0])\n",
    "    losing_trades = len(trades[trades['Return [%]'] < 0])\n",
    "    \n",
    "    print(f\"\\nTrade Statistics:\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Trades per Year: {total_trades / n_years:.0f}\")\n",
    "    print(f\"Winning Trades: {winning_trades}\")\n",
    "    print(f\"Losing Trades: {losing_trades}\")\n",
    "    print(f\"Win Rate: {(winning_trades / total_trades * 100) if total_trades > 0 else 0:.2f}%\")\n",
    "    \n",
    "    # Return analysis\n",
    "    avg_win = trades[trades['Return [%]'] > 0]['Return [%]'].mean() if winning_trades > 0 else 0\n",
    "    avg_loss = trades[trades['Return [%]'] < 0]['Return [%]'].mean() if losing_trades > 0 else 0\n",
    "    profit_factor = abs(avg_win * winning_trades / (avg_loss * losing_trades)) if losing_trades > 0 else np.inf\n",
    "    \n",
    "    print(f\"\\nReturn Metrics:\")\n",
    "    print(f\"Average Win: {avg_win:.2f}%\")\n",
    "    print(f\"Average Loss: {avg_loss:.2f}%\")\n",
    "    print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "    print(f\"Expectancy: {trades['Return [%]'].mean():.2f}%\")\n",
    "    \n",
    "    # Risk metrics\n",
    "    print(f\"\\nRisk Metrics:\")\n",
    "    print(f\"Maximum Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "    print(f\"Average Drawdown: {portfolio.drawdown().mean() * 100:.2f}%\")\n",
    "    print(f\"Calmar Ratio: {stats['Calmar Ratio']:.2f}\")\n",
    "    print(f\"Sortino Ratio: {stats['Sortino Ratio']:.2f}\")\n",
    "    \n",
    "    # Signal quality analysis\n",
    "    bull_signals = signals[signals['synergy_bull']]\n",
    "    bear_signals = signals[signals['synergy_bear']]\n",
    "    \n",
    "    print(f\"\\nSignal Analysis:\")\n",
    "    print(f\"Total Bull Signals: {len(bull_signals)}\")\n",
    "    print(f\"Total Bear Signals: {len(bear_signals)}\")\n",
    "    print(f\"Average Signal Strength: {signals['synergy_strength'][signals['synergy_strength'] > 0].mean():.3f}\")\n",
    "    \n",
    "    # Monthly performance\n",
    "    monthly_returns = portfolio.returns().resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "    positive_months = (monthly_returns > 0).sum()\n",
    "    total_months = len(monthly_returns)\n",
    "    \n",
    "    print(f\"\\nMonthly Performance:\")\n",
    "    print(f\"Positive Months: {positive_months}/{total_months} ({positive_months/total_months*100:.1f}%)\")\n",
    "    print(f\"Best Month: {monthly_returns.max() * 100:.2f}%\")\n",
    "    print(f\"Worst Month: {monthly_returns.min() * 100:.2f}%\")\n",
    "    print(f\"Average Monthly Return: {monthly_returns.mean() * 100:.2f}%\")\n",
    "    \n",
    "    return trades\n",
    "\n",
    "# Generate report\n",
    "trades_df = generate_comprehensive_report(signals, portfolio, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results with comprehensive error handling\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"SAVING RESULTS\")\nlogger.info(\"=\"*60)\n\n# Create results directory if it doesn't exist\nresults_dir = '/home/QuantNova/AlgoSpace-8/results'\ntry:\n    os.makedirs(results_dir, exist_ok=True)\n    logger.info(f\"Results directory verified: {results_dir}\")\nexcept Exception as e:\n    logger.error(f\"Failed to create results directory: {str(e)}\")\n    results_dir = '.'  # Fallback to current directory\n\n# Prepare timestamp for unique filenames\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Save signals with error handling\ntry:\n    signals_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_signals_{timestamp}.csv')\n    signals.to_csv(signals_file)\n    logger.info(f\"✓ Signals saved to {signals_file}\")\nexcept Exception as e:\n    logger.error(f\"Failed to save signals: {str(e)}\")\n\n# Save trade records with validation\nif portfolio is not None:\n    try:\n        trades_df = portfolio.trades.records_readable\n        if len(trades_df) > 0:\n            trades_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_trades_{timestamp}.csv')\n            trades_df.to_csv(trades_file)\n            logger.info(f\"✓ Trade records saved to {trades_file}\")\n        else:\n            logger.warning(\"No trades to save\")\n    except Exception as e:\n        logger.error(f\"Failed to save trade records: {str(e)}\")\nelse:\n    logger.warning(\"No portfolio available - skipping trade records\")\n\n# Save performance metrics with comprehensive details\ntry:\n    metrics_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_metrics_{timestamp}.txt')\n    with open(metrics_file, 'w') as f:\n        f.write(\"=\"*60 + \"\\n\")\n        f.write(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY PERFORMANCE REPORT\\n\")\n        f.write(\"=\"*60 + \"\\n\\n\")\n        \n        # Strategy configuration\n        f.write(\"STRATEGY CONFIGURATION:\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        f.write(f\"Run Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"Data Period: {df_30m.index[0]} to {df_30m.index[-1]}\\n\")\n        f.write(f\"Initial Capital: ${StrategyConfig.INITIAL_CAPITAL:,.2f}\\n\")\n        f.write(f\"Position Size: {StrategyConfig.POSITION_SIZE_BASE * 100:.1f}%\\n\")\n        f.write(f\"Stop Loss: {StrategyConfig.STOP_LOSS_PCT * 100:.1f}%\\n\")\n        f.write(f\"Take Profit: {StrategyConfig.TAKE_PROFIT_PCT * 100:.1f}%\\n\")\n        f.write(f\"Trading Fees: {StrategyConfig.TRADING_FEES * 100:.2f}%\\n\")\n        f.write(f\"Slippage: {StrategyConfig.SLIPPAGE * 100:.2f}%\\n\\n\")\n        \n        # Performance metrics\n        if stats is not None and 'Error' not in stats:\n            f.write(\"PERFORMANCE METRICS:\\n\")\n            f.write(\"-\"*30 + \"\\n\")\n            for key, value in stats.items():\n                if isinstance(value, (int, float)):\n                    if 'Return' in key or 'Ratio' in key or '%' in key:\n                        f.write(f\"{key}: {value:.2f}\\n\")\n                    else:\n                        f.write(f\"{key}: {value:.4f}\\n\")\n                else:\n                    f.write(f\"{key}: {value}\\n\")\n        else:\n            f.write(\"Performance metrics unavailable due to errors\\n\")\n        \n        # Signal statistics\n        f.write(\"\\nSIGNAL STATISTICS:\\n\")\n        f.write(\"-\"*30 + \"\\n\")\n        f.write(f\"Total Signals Generated: {(signals['signal'] != 0).sum()}\\n\")\n        f.write(f\"Bull Signals: {(signals['signal'] == 1).sum()}\\n\")\n        f.write(f\"Bear Signals: {(signals['signal'] == -1).sum()}\\n\")\n        \n        if 'synergy_strength' in signals.columns:\n            avg_strength = signals.loc[signals['signal'] != 0, 'synergy_strength'].mean()\n            f.write(f\"Average Signal Strength: {avg_strength:.3f}\\n\")\n        \n        # Risk analysis\n        if hasattr(signals, 'attrs') and 'performance_metrics' in signals.attrs:\n            perf = signals.attrs['performance_metrics']\n            f.write(\"\\nPERFORMANCE ANALYSIS:\\n\")\n            f.write(\"-\"*30 + \"\\n\")\n            f.write(f\"Total Execution Time: {perf.get('total_time', 0):.2f} seconds\\n\")\n            f.write(f\"Signals Filtered by Risk: {perf.get('signals_filtered', 0)}\\n\")\n            \n            if perf.get('errors'):\n                f.write(\"\\nERRORS ENCOUNTERED:\\n\")\n                for error in perf['errors']:\n                    f.write(f\"- {error}\\n\")\n        \n        # Monte Carlo results\n        if percentiles is not None:\n            f.write(\"\\nMONTE CARLO ANALYSIS:\\n\")\n            f.write(\"-\"*30 + \"\\n\")\n            f.write(f\"5th Percentile Return: {percentiles[0]:.2f}%\\n\")\n            f.write(f\"Median Return: {percentiles[2]:.2f}%\\n\")\n            f.write(f\"95th Percentile Return: {percentiles[4]:.2f}%\\n\")\n        \n        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n        f.write(\"END OF REPORT\\n\")\n        f.write(\"=\"*60 + \"\\n\")\n    \n    logger.info(f\"✓ Performance metrics saved to {metrics_file}\")\nexcept Exception as e:\n    logger.error(f\"Failed to save performance metrics: {str(e)}\")\n\n# Save configuration for reproducibility\ntry:\n    config_file = os.path.join(results_dir, f'synergy_3_config_{timestamp}.json')\n    config_dict = {attr: getattr(StrategyConfig, attr) \n                   for attr in dir(StrategyConfig) \n                   if not attr.startswith('_') and not callable(getattr(StrategyConfig, attr))}\n    \n    with open(config_file, 'w') as f:\n        json.dump(config_dict, f, indent=2, default=str)\n    \n    logger.info(f\"✓ Configuration saved to {config_file}\")\nexcept Exception as e:\n    logger.error(f\"Failed to save configuration: {str(e)}\")\n\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY COMPLETE\")\nlogger.info(\"=\"*60)\n\n# Final summary\nlogger.info(\"\\nFINAL SUMMARY:\")\nif portfolio is not None and stats is not None and 'Total Return [%]' in stats:\n    logger.info(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n    logger.info(f\"Sharpe Ratio: {stats.get('Sharpe Ratio', 'N/A'):.2f}\")\n    logger.info(f\"Max Drawdown: {stats.get('Max Drawdown [%]', 'N/A'):.2f}%\")\n    logger.info(f\"Total Trades: {stats.get('Total Trades', 0)}\")\nelse:\n    logger.info(\"Strategy execution completed with errors - check logs for details\")\n\nlogger.info(f\"\\nAll results saved to: {results_dir}\")\nlogger.info(\"Notebook execution completed successfully!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}