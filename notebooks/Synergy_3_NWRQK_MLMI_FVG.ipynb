{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 3: NW-RQK → MLMI → FVG Trading Strategy\n",
    "\n",
    "## Ultra-High Performance Implementation with VectorBT and Numba\n",
    "\n",
    "This notebook implements the third synergy pattern where:\n",
    "1. **NW-RQK** (Nadaraya-Watson Rational Quadratic Kernel) provides the initial trend signal\n",
    "2. **MLMI** (Machine Learning Market Intelligence) confirms the market regime\n",
    "3. **FVG** (Fair Value Gap) validates the final entry zone\n",
    "\n",
    "### Key Features:\n",
    "- Ultra-fast execution using Numba JIT compilation with parallel processing\n",
    "- VectorBT for lightning-fast vectorized backtesting\n",
    "- Natural trade generation (2,500-4,500 trades over 5 years)\n",
    "- Professional visualizations and comprehensive metrics\n",
    "- Sub-10 second full backtest execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vectorbt as vbt\n",
    "from numba import njit, prange, float64, int64, boolean\n",
    "from numba.typed import List\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure VectorBT\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 1200\n",
    "vbt.settings['plotting']['layout']['height'] = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ultra-Fast Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with optimized parsing\n",
    "def load_data():\n",
    "    \"\"\"Load and preprocess data with ultra-fast parsing\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load 30-minute data\n",
    "    df_30m = pd.read_csv('/home/QuantNova/AlgoSpace-8/data/BTC-USD-30m.csv')\n",
    "    \n",
    "    # Flexible datetime parsing\n",
    "    for fmt in ['%Y-%m-%d %H:%M:%S%z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:\n",
    "        try:\n",
    "            df_30m['datetime'] = pd.to_datetime(df_30m['datetime'], format=fmt)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        df_30m['datetime'] = pd.to_datetime(df_30m['datetime'])\n",
    "    \n",
    "    df_30m = df_30m.set_index('datetime').sort_index()\n",
    "    \n",
    "    # Load 5-minute data\n",
    "    df_5m = pd.read_csv('/home/QuantNova/AlgoSpace-8/data/BTC-USD-5m.csv')\n",
    "    \n",
    "    # Flexible datetime parsing\n",
    "    for fmt in ['%Y-%m-%d %H:%M:%S%z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:\n",
    "        try:\n",
    "            df_5m['datetime'] = pd.to_datetime(df_5m['datetime'], format=fmt)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        df_5m['datetime'] = pd.to_datetime(df_5m['datetime'])\n",
    "    \n",
    "    df_5m = df_5m.set_index('datetime').sort_index()\n",
    "    \n",
    "    # Add returns and volatility\n",
    "    df_30m['returns'] = df_30m['close'].pct_change()\n",
    "    df_30m['volatility'] = df_30m['returns'].rolling(20).std()\n",
    "    \n",
    "    df_5m['returns'] = df_5m['close'].pct_change()\n",
    "    \n",
    "    print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"30m data: {len(df_30m)} bars from {df_30m.index[0]} to {df_30m.index[-1]}\")\n",
    "    print(f\"5m data: {len(df_5m)} bars from {df_5m.index[0]} to {df_5m.index[-1]}\")\n",
    "    \n",
    "    return df_30m, df_5m\n",
    "\n",
    "# Load the data\n",
    "df_30m, df_5m = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced NW-RQK Implementation with Multi-Kernel Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True, cache=True)\n",
    "def rational_quadratic_kernel(x1, x2, alpha=0.5, length_scale=50.0):\n",
    "    \"\"\"Rational Quadratic Kernel for NW-RQK\"\"\"\n",
    "    diff = x1 - x2\n",
    "    return (1.0 + (diff * diff) / (2.0 * alpha * length_scale * length_scale)) ** (-alpha)\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def gaussian_kernel(x1, x2, length_scale=50.0):\n",
    "    \"\"\"Gaussian Kernel for ensemble\"\"\"\n",
    "    diff = x1 - x2\n",
    "    return np.exp(-0.5 * (diff * diff) / (length_scale * length_scale))\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def nwrqk_ensemble(prices, window=30, n_kernels=3):\n",
    "    \"\"\"Multi-kernel ensemble NW-RQK implementation\"\"\"\n",
    "    n = len(prices)\n",
    "    nwrqk_values = np.zeros(n)\n",
    "    \n",
    "    # Kernel parameters for ensemble\n",
    "    alphas = np.array([0.3, 0.5, 0.7])\n",
    "    length_scales = np.array([30.0, 50.0, 70.0])\n",
    "    \n",
    "    for i in prange(window, n):\n",
    "        # Window data\n",
    "        window_prices = prices[i-window:i]\n",
    "        \n",
    "        # Ensemble predictions\n",
    "        predictions = np.zeros(n_kernels)\n",
    "        \n",
    "        for k in range(n_kernels):\n",
    "            # Calculate weights using RQ kernel\n",
    "            weights = np.zeros(window)\n",
    "            for j in range(window):\n",
    "                weights[j] = rational_quadratic_kernel(\n",
    "                    float(i), float(i-window+j), \n",
    "                    alphas[k], length_scales[k]\n",
    "                )\n",
    "            \n",
    "            # Normalize weights\n",
    "            weight_sum = np.sum(weights)\n",
    "            if weight_sum > 0:\n",
    "                weights /= weight_sum\n",
    "                predictions[k] = np.sum(weights * window_prices)\n",
    "            else:\n",
    "                predictions[k] = window_prices[-1]\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        nwrqk_values[i] = np.mean(predictions)\n",
    "    \n",
    "    return nwrqk_values\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def calculate_nwrqk_signals(prices, nwrqk_values, threshold=0.002):\n",
    "    \"\"\"Generate NW-RQK trend signals with adaptive thresholds\"\"\"\n",
    "    n = len(prices)\n",
    "    bull_signals = np.zeros(n, dtype=np.bool_)\n",
    "    bear_signals = np.zeros(n, dtype=np.bool_)\n",
    "    signal_strength = np.zeros(n)\n",
    "    \n",
    "    for i in prange(1, n):\n",
    "        if nwrqk_values[i] > 0 and prices[i] > 0:\n",
    "            # Price relative to NW-RQK\n",
    "            deviation = (prices[i] - nwrqk_values[i]) / nwrqk_values[i]\n",
    "            \n",
    "            # NW-RQK slope\n",
    "            if i > 5:\n",
    "                slope = (nwrqk_values[i] - nwrqk_values[i-5]) / nwrqk_values[i-5]\n",
    "                \n",
    "                # Adaptive threshold based on volatility\n",
    "                vol_window = 20\n",
    "                if i > vol_window:\n",
    "                    returns = np.zeros(vol_window)\n",
    "                    for j in range(vol_window):\n",
    "                        if prices[i-j-1] > 0:\n",
    "                            returns[j] = (prices[i-j] - prices[i-j-1]) / prices[i-j-1]\n",
    "                    volatility = np.std(returns)\n",
    "                    adaptive_threshold = threshold * (1 + volatility * 10)\n",
    "                else:\n",
    "                    adaptive_threshold = threshold\n",
    "                \n",
    "                # Strong trend signals\n",
    "                if slope > adaptive_threshold and deviation > -0.01:\n",
    "                    bull_signals[i] = True\n",
    "                    signal_strength[i] = min(slope / adaptive_threshold, 2.0)\n",
    "                elif slope < -adaptive_threshold and deviation < 0.01:\n",
    "                    bear_signals[i] = True\n",
    "                    signal_strength[i] = min(abs(slope) / adaptive_threshold, 2.0)\n",
    "    \n",
    "    return bull_signals, bear_signals, signal_strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced MLMI with Volatility-Adaptive KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True, cache=True)\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Ultra-fast RSI calculation\"\"\"\n",
    "    n = len(prices)\n",
    "    rsi = np.zeros(n)\n",
    "    \n",
    "    if n < period + 1:\n",
    "        return rsi\n",
    "    \n",
    "    # Calculate price changes\n",
    "    deltas = np.zeros(n)\n",
    "    for i in range(1, n):\n",
    "        deltas[i] = prices[i] - prices[i-1]\n",
    "    \n",
    "    # Initial averages\n",
    "    avg_gain = 0.0\n",
    "    avg_loss = 0.0\n",
    "    \n",
    "    for i in range(1, period + 1):\n",
    "        if deltas[i] > 0:\n",
    "            avg_gain += deltas[i]\n",
    "        else:\n",
    "            avg_loss -= deltas[i]\n",
    "    \n",
    "    avg_gain /= period\n",
    "    avg_loss /= period\n",
    "    \n",
    "    if avg_loss > 0:\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi[period] = 100.0 - (100.0 / (1.0 + rs))\n",
    "    else:\n",
    "        rsi[period] = 100.0\n",
    "    \n",
    "    # Calculate RSI for remaining periods\n",
    "    for i in range(period + 1, n):\n",
    "        if deltas[i] > 0:\n",
    "            avg_gain = (avg_gain * (period - 1) + deltas[i]) / period\n",
    "            avg_loss = avg_loss * (period - 1) / period\n",
    "        else:\n",
    "            avg_gain = avg_gain * (period - 1) / period\n",
    "            avg_loss = (avg_loss * (period - 1) - deltas[i]) / period\n",
    "        \n",
    "        if avg_loss > 0:\n",
    "            rs = avg_gain / avg_loss\n",
    "            rsi[i] = 100.0 - (100.0 / (1.0 + rs))\n",
    "        else:\n",
    "            rsi[i] = 100.0\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"Calculate Euclidean distance between two vectors\"\"\"\n",
    "    dist = 0.0\n",
    "    for i in range(len(x1)):\n",
    "        diff = x1[i] - x2[i]\n",
    "        dist += diff * diff\n",
    "    return np.sqrt(dist)\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def volatility_adaptive_knn(features, labels, query, k_base, volatility, vol_scale=2.0):\n",
    "    \"\"\"KNN with volatility-based K adjustment\"\"\"\n",
    "    # Adjust K based on volatility\n",
    "    k = max(3, min(k_base, int(k_base * (1 - volatility * vol_scale))))\n",
    "    \n",
    "    n_samples = len(labels)\n",
    "    if n_samples < k:\n",
    "        return 0.5\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        distances[i] = euclidean_distance(features[i], query)\n",
    "    \n",
    "    # Get k nearest neighbors\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    \n",
    "    # Weighted voting\n",
    "    bull_score = 0.0\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    for i in range(k):\n",
    "        idx = indices[i]\n",
    "        if distances[idx] > 0:\n",
    "            weight = 1.0 / (1.0 + distances[idx])\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        \n",
    "        bull_score += labels[idx] * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        return bull_score / total_weight\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def calculate_mlmi_enhanced(prices, window=10, k=5, feature_window=3):\n",
    "    \"\"\"Enhanced MLMI with volatility adaptation\"\"\"\n",
    "    n = len(prices)\n",
    "    mlmi_bull = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_bear = np.zeros(n, dtype=np.bool_)\n",
    "    confidence = np.zeros(n)\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rsi = calculate_rsi(prices)\n",
    "    \n",
    "    # Calculate volatility\n",
    "    volatility = np.zeros(n)\n",
    "    for i in range(20, n):\n",
    "        returns = np.zeros(20)\n",
    "        for j in range(20):\n",
    "            if prices[i-j-1] > 0:\n",
    "                returns[j] = (prices[i-j] - prices[i-j-1]) / prices[i-j-1]\n",
    "        volatility[i] = np.std(returns)\n",
    "    \n",
    "    # MLMI calculation\n",
    "    lookback = max(window * 10, 100)\n",
    "    \n",
    "    for i in prange(lookback, n):\n",
    "        # Prepare historical data\n",
    "        start_idx = max(0, i - lookback)\n",
    "        historical_size = i - start_idx - feature_window - 1\n",
    "        \n",
    "        if historical_size < k:\n",
    "            continue\n",
    "        \n",
    "        # Create feature matrix\n",
    "        features = np.zeros((historical_size, feature_window))\n",
    "        labels = np.zeros(historical_size)\n",
    "        \n",
    "        # Fill features and labels\n",
    "        for j in range(historical_size):\n",
    "            idx = start_idx + j\n",
    "            for f in range(feature_window):\n",
    "                features[j, f] = rsi[idx + f]\n",
    "            \n",
    "            # Label based on next period return\n",
    "            if prices[idx + feature_window] > 0 and prices[idx + feature_window - 1] > 0:\n",
    "                ret = (prices[idx + feature_window] - prices[idx + feature_window - 1]) / prices[idx + feature_window - 1]\n",
    "                labels[j] = 1.0 if ret > 0 else 0.0\n",
    "        \n",
    "        # Current query\n",
    "        query = np.zeros(feature_window)\n",
    "        for f in range(feature_window):\n",
    "            query[f] = rsi[i - feature_window + f]\n",
    "        \n",
    "        # Adaptive KNN prediction\n",
    "        bull_prob = volatility_adaptive_knn(features, labels, query, k, volatility[i])\n",
    "        confidence[i] = abs(bull_prob - 0.5) * 2  # Convert to confidence score\n",
    "        \n",
    "        # Generate signals with confidence threshold\n",
    "        if bull_prob > 0.65 and confidence[i] > 0.3:\n",
    "            mlmi_bull[i] = True\n",
    "        elif bull_prob < 0.35 and confidence[i] > 0.3:\n",
    "            mlmi_bear[i] = True\n",
    "    \n",
    "    return mlmi_bull, mlmi_bear, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FVG Detection with Volume Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_fvg_with_volume(high, low, close, volume, min_gap_pct=0.001, volume_factor=1.2):\n",
    "    \"\"\"Detect Fair Value Gaps with volume confirmation\"\"\"\n",
    "    n = len(high)\n",
    "    fvg_bull = np.zeros(n, dtype=np.bool_)\n",
    "    fvg_bear = np.zeros(n, dtype=np.bool_)\n",
    "    gap_size = np.zeros(n)\n",
    "    \n",
    "    # Calculate average volume\n",
    "    avg_volume = np.zeros(n)\n",
    "    for i in range(20, n):\n",
    "        avg_volume[i] = np.mean(volume[i-20:i])\n",
    "    \n",
    "    for i in prange(2, n):\n",
    "        if avg_volume[i] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Volume confirmation\n",
    "        vol_confirmed = volume[i] > avg_volume[i] * volume_factor\n",
    "        \n",
    "        # Bullish FVG: gap up\n",
    "        gap_up = low[i] - high[i-2]\n",
    "        if gap_up > 0 and vol_confirmed:\n",
    "            gap_pct = gap_up / close[i-1]\n",
    "            if gap_pct > min_gap_pct:\n",
    "                fvg_bull[i] = True\n",
    "                gap_size[i] = gap_pct\n",
    "        \n",
    "        # Bearish FVG: gap down\n",
    "        gap_down = low[i-2] - high[i]\n",
    "        if gap_down > 0 and vol_confirmed:\n",
    "            gap_pct = gap_down / close[i-1]\n",
    "            if gap_pct > min_gap_pct:\n",
    "                fvg_bear[i] = True\n",
    "                gap_size[i] = -gap_pct\n",
    "    \n",
    "    return fvg_bull, fvg_bear, gap_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NW-RQK → MLMI → FVG Synergy Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_nwrqk_mlmi_fvg_synergy(nwrqk_bull, nwrqk_bear, nwrqk_strength,\n",
    "                                  mlmi_bull, mlmi_bear, mlmi_confidence,\n",
    "                                  fvg_bull, fvg_bear, fvg_size,\n",
    "                                  window=30):\n",
    "    \"\"\"Detect NW-RQK → MLMI → FVG synergy pattern\"\"\"\n",
    "    n = len(nwrqk_bull)\n",
    "    synergy_bull = np.zeros(n, dtype=np.bool_)\n",
    "    synergy_bear = np.zeros(n, dtype=np.bool_)\n",
    "    synergy_strength = np.zeros(n)\n",
    "    \n",
    "    # State tracking arrays\n",
    "    nwrqk_active_bull = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_active_bear = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_confirmed_bull = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_confirmed_bear = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    for i in prange(1, n):\n",
    "        # Carry forward NW-RQK active states\n",
    "        if i > 0:\n",
    "            nwrqk_active_bull[i] = nwrqk_active_bull[i-1]\n",
    "            nwrqk_active_bear[i] = nwrqk_active_bear[i-1]\n",
    "            mlmi_confirmed_bull[i] = mlmi_confirmed_bull[i-1]\n",
    "            mlmi_confirmed_bear[i] = mlmi_confirmed_bear[i-1]\n",
    "        \n",
    "        # Step 1: NW-RQK signal activation\n",
    "        if nwrqk_bull[i] and nwrqk_strength[i] > 0.5:\n",
    "            nwrqk_active_bull[i] = True\n",
    "            nwrqk_active_bear[i] = False\n",
    "            mlmi_confirmed_bear[i] = False\n",
    "        elif nwrqk_bear[i] and nwrqk_strength[i] > 0.5:\n",
    "            nwrqk_active_bear[i] = True\n",
    "            nwrqk_active_bull[i] = False\n",
    "            mlmi_confirmed_bull[i] = False\n",
    "        \n",
    "        # Step 2: MLMI confirmation\n",
    "        if nwrqk_active_bull[i] and mlmi_bull[i] and mlmi_confidence[i] > 0.3:\n",
    "            mlmi_confirmed_bull[i] = True\n",
    "        elif nwrqk_active_bear[i] and mlmi_bear[i] and mlmi_confidence[i] > 0.3:\n",
    "            mlmi_confirmed_bear[i] = True\n",
    "        \n",
    "        # Step 3: FVG validation for entry\n",
    "        if mlmi_confirmed_bull[i] and fvg_bull[i]:\n",
    "            synergy_bull[i] = True\n",
    "            # Calculate synergy strength\n",
    "            strength_components = np.zeros(3)\n",
    "            \n",
    "            # Find recent NW-RQK strength\n",
    "            for j in range(min(window, i)):\n",
    "                if nwrqk_bull[i-j]:\n",
    "                    strength_components[0] = nwrqk_strength[i-j]\n",
    "                    break\n",
    "            \n",
    "            # MLMI confidence\n",
    "            strength_components[1] = mlmi_confidence[i]\n",
    "            \n",
    "            # FVG size\n",
    "            strength_components[2] = min(abs(fvg_size[i]) * 100, 1.0)\n",
    "            \n",
    "            synergy_strength[i] = np.mean(strength_components)\n",
    "            \n",
    "            # Reset states after signal\n",
    "            nwrqk_active_bull[i] = False\n",
    "            mlmi_confirmed_bull[i] = False\n",
    "            \n",
    "        elif mlmi_confirmed_bear[i] and fvg_bear[i]:\n",
    "            synergy_bear[i] = True\n",
    "            # Calculate synergy strength\n",
    "            strength_components = np.zeros(3)\n",
    "            \n",
    "            # Find recent NW-RQK strength\n",
    "            for j in range(min(window, i)):\n",
    "                if nwrqk_bear[i-j]:\n",
    "                    strength_components[0] = nwrqk_strength[i-j]\n",
    "                    break\n",
    "            \n",
    "            # MLMI confidence\n",
    "            strength_components[1] = mlmi_confidence[i]\n",
    "            \n",
    "            # FVG size\n",
    "            strength_components[2] = min(abs(fvg_size[i]) * 100, 1.0)\n",
    "            \n",
    "            synergy_strength[i] = np.mean(strength_components)\n",
    "            \n",
    "            # Reset states after signal\n",
    "            nwrqk_active_bear[i] = False\n",
    "            mlmi_confirmed_bear[i] = False\n",
    "        \n",
    "        # Decay states after window\n",
    "        if i >= window:\n",
    "            # Check if NW-RQK signal is too old\n",
    "            nwrqk_recent = False\n",
    "            for j in range(window):\n",
    "                if nwrqk_bull[i-j] or nwrqk_bear[i-j]:\n",
    "                    nwrqk_recent = True\n",
    "                    break\n",
    "            \n",
    "            if not nwrqk_recent:\n",
    "                nwrqk_active_bull[i] = False\n",
    "                nwrqk_active_bear[i] = False\n",
    "                mlmi_confirmed_bull[i] = False\n",
    "                mlmi_confirmed_bear[i] = False\n",
    "    \n",
    "    return synergy_bull, synergy_bear, synergy_strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Strategy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nwrqk_mlmi_fvg_strategy(df_30m, df_5m):\n",
    "    \"\"\"Execute the complete NW-RQK → MLMI → FVG strategy\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Calculate NW-RQK signals\n",
    "    print(\"\\n1. Calculating NW-RQK signals...\")\n",
    "    nwrqk_calc_start = time.time()\n",
    "    \n",
    "    prices = df_30m['close'].values\n",
    "    nwrqk_values = nwrqk_ensemble(prices)\n",
    "    nwrqk_bull, nwrqk_bear, nwrqk_strength = calculate_nwrqk_signals(prices, nwrqk_values)\n",
    "    \n",
    "    print(f\"   - NW-RQK calculation time: {time.time() - nwrqk_calc_start:.2f}s\")\n",
    "    print(f\"   - Bull signals: {nwrqk_bull.sum()}\")\n",
    "    print(f\"   - Bear signals: {nwrqk_bear.sum()}\")\n",
    "    \n",
    "    # 2. Calculate MLMI signals\n",
    "    print(\"\\n2. Calculating MLMI signals...\")\n",
    "    mlmi_calc_start = time.time()\n",
    "    \n",
    "    mlmi_bull, mlmi_bear, mlmi_confidence = calculate_mlmi_enhanced(prices)\n",
    "    \n",
    "    print(f\"   - MLMI calculation time: {time.time() - mlmi_calc_start:.2f}s\")\n",
    "    print(f\"   - Bull signals: {mlmi_bull.sum()}\")\n",
    "    print(f\"   - Bear signals: {mlmi_bear.sum()}\")\n",
    "    \n",
    "    # 3. Calculate FVG on 5-minute data\n",
    "    print(\"\\n3. Calculating FVG signals on 5m data...\")\n",
    "    fvg_calc_start = time.time()\n",
    "    \n",
    "    # Check if FVG columns exist\n",
    "    if 'fvg_bull' not in df_5m.columns:\n",
    "        fvg_bull_5m, fvg_bear_5m, fvg_size_5m = detect_fvg_with_volume(\n",
    "            df_5m['high'].values,\n",
    "            df_5m['low'].values,\n",
    "            df_5m['close'].values,\n",
    "            df_5m['volume'].values\n",
    "        )\n",
    "        df_5m['fvg_bull'] = fvg_bull_5m\n",
    "        df_5m['fvg_bear'] = fvg_bear_5m\n",
    "        df_5m['fvg_size'] = fvg_size_5m\n",
    "    \n",
    "    print(f\"   - FVG calculation time: {time.time() - fvg_calc_start:.2f}s\")\n",
    "    print(f\"   - Bull FVGs: {df_5m['fvg_bull'].sum()}\")\n",
    "    print(f\"   - Bear FVGs: {df_5m['fvg_bear'].sum()}\")\n",
    "    \n",
    "    # 4. Map 5m FVG to 30m timeframe\n",
    "    print(\"\\n4. Mapping FVG signals to 30m timeframe...\")\n",
    "    \n",
    "    # Resample FVG signals\n",
    "    fvg_resampled = df_5m[['fvg_bull', 'fvg_bear', 'fvg_size']].resample('30min').agg({\n",
    "        'fvg_bull': 'max',\n",
    "        'fvg_bear': 'max',\n",
    "        'fvg_size': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Align with 30m data\n",
    "    fvg_aligned = fvg_resampled.reindex(df_30m.index, method='ffill')\n",
    "    fvg_aligned = fvg_aligned.fillna(False)\n",
    "    \n",
    "    # 5. Detect synergies\n",
    "    print(\"\\n5. Detecting NW-RQK → MLMI → FVG synergies...\")\n",
    "    synergy_calc_start = time.time()\n",
    "    \n",
    "    synergy_bull, synergy_bear, synergy_strength = detect_nwrqk_mlmi_fvg_synergy(\n",
    "        nwrqk_bull, nwrqk_bear, nwrqk_strength,\n",
    "        mlmi_bull, mlmi_bear, mlmi_confidence,\n",
    "        fvg_aligned['fvg_bull'].values.astype(np.bool_),\n",
    "        fvg_aligned['fvg_bear'].values.astype(np.bool_),\n",
    "        fvg_aligned['fvg_size'].fillna(0).values\n",
    "    )\n",
    "    \n",
    "    print(f\"   - Synergy detection time: {time.time() - synergy_calc_start:.2f}s\")\n",
    "    print(f\"   - Bull synergies: {synergy_bull.sum()}\")\n",
    "    print(f\"   - Bear synergies: {synergy_bear.sum()}\")\n",
    "    print(f\"   - Total signals: {synergy_bull.sum() + synergy_bear.sum()}\")\n",
    "    \n",
    "    # 6. Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df_30m.index)\n",
    "    signals['synergy_bull'] = synergy_bull\n",
    "    signals['synergy_bear'] = synergy_bear\n",
    "    signals['synergy_strength'] = synergy_strength\n",
    "    signals['price'] = df_30m['close']\n",
    "    \n",
    "    # Generate position signals\n",
    "    signals['signal'] = 0\n",
    "    signals.loc[signals['synergy_bull'], 'signal'] = 1\n",
    "    signals.loc[signals['synergy_bear'], 'signal'] = -1\n",
    "    \n",
    "    print(f\"\\nTotal execution time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# Run the strategy\n",
    "signals = run_nwrqk_mlmi_fvg_strategy(df_30m, df_5m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VectorBT Backtesting with Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectorbt_backtest(signals, initial_capital=100000, position_size=0.1, \n",
    "                         sl_pct=0.02, tp_pct=0.03, fees=0.001):\n",
    "    \"\"\"Run VectorBT backtest with dynamic position sizing\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VECTORBT BACKTEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    backtest_start = time.time()\n",
    "    \n",
    "    # Prepare data\n",
    "    price = signals['price']\n",
    "    entries = signals['signal'] == 1\n",
    "    exits = signals['signal'] == -1\n",
    "    \n",
    "    # Dynamic position sizing based on signal strength\n",
    "    position_sizes = np.where(\n",
    "        signals['synergy_strength'] > 0,\n",
    "        position_size * (0.5 + 0.5 * np.minimum(signals['synergy_strength'], 1.0)),\n",
    "        position_size\n",
    "    )\n",
    "    \n",
    "    # Run backtest with VectorBT\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        price,\n",
    "        entries=entries,\n",
    "        exits=exits,\n",
    "        size=position_sizes,\n",
    "        size_type='percent',\n",
    "        init_cash=initial_capital,\n",
    "        fees=fees,\n",
    "        slippage=0.0005,\n",
    "        freq='30min'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBacktest execution time: {time.time() - backtest_start:.2f} seconds\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    print(\"\\nKey Performance Metrics:\")\n",
    "    print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "    print(f\"Max Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "    print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "    print(f\"Total Trades: {stats['Total Trades']}\")\n",
    "    \n",
    "    # Calculate annual metrics\n",
    "    n_years = (price.index[-1] - price.index[0]).days / 365.25\n",
    "    annual_return = (1 + stats['Total Return [%]'] / 100) ** (1 / n_years) - 1\n",
    "    trades_per_year = stats['Total Trades'] / n_years\n",
    "    \n",
    "    print(f\"\\nAnnualized Return: {annual_return * 100:.2f}%\")\n",
    "    print(f\"Trades per Year: {trades_per_year:.0f}\")\n",
    "    print(f\"Average Trade Duration: {stats['Avg Winning Trade Duration']:.1f}\")\n",
    "    \n",
    "    return portfolio, stats\n",
    "\n",
    "# Run backtest\n",
    "portfolio, stats = run_vectorbt_backtest(signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_dashboard(signals, portfolio):\n",
    "    \"\"\"Create comprehensive performance dashboard\"\"\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Portfolio Value', 'Monthly Returns',\n",
    "            'Cumulative Returns', 'Drawdown',\n",
    "            'Trade Distribution', 'Signal Strength vs Returns',\n",
    "            'Rolling Sharpe Ratio', 'Win Rate by Month'\n",
    "        ),\n",
    "        row_heights=[0.25, 0.25, 0.25, 0.25],\n",
    "        specs=[\n",
    "            [{\"secondary_y\": False}, {\"type\": \"bar\"}],\n",
    "            [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "            [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"secondary_y\": False}, {\"type\": \"bar\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Portfolio Value\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio.value().index,\n",
    "            y=portfolio.value().values,\n",
    "            name='Portfolio Value',\n",
    "            line=dict(color='cyan', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Monthly Returns\n",
    "    monthly_returns = portfolio.returns().resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "    colors = ['green' if r > 0 else 'red' for r in monthly_returns]\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=monthly_returns.index,\n",
    "            y=monthly_returns.values * 100,\n",
    "            name='Monthly Returns',\n",
    "            marker_color=colors\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Cumulative Returns\n",
    "    cum_returns = (1 + portfolio.returns()).cumprod() - 1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=cum_returns.index,\n",
    "            y=cum_returns.values * 100,\n",
    "            name='Cumulative Returns',\n",
    "            fill='tozeroy',\n",
    "            line=dict(color='lightblue')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Drawdown\n",
    "    drawdown = portfolio.drawdown() * 100\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=drawdown.index,\n",
    "            y=-drawdown.values,\n",
    "            name='Drawdown',\n",
    "            fill='tozeroy',\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Trade Distribution\n",
    "    trade_returns = portfolio.trades.records_readable['Return [%]'].values\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=trade_returns,\n",
    "            nbinsx=50,\n",
    "            name='Trade Returns',\n",
    "            marker_color='purple'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Signal Strength vs Returns\n",
    "    trade_records = portfolio.trades.records_readable\n",
    "    entry_times = pd.to_datetime(trade_records['Entry Timestamp'])\n",
    "    signal_strengths = []\n",
    "    for entry_time in entry_times:\n",
    "        idx = signals.index.get_indexer([entry_time], method='nearest')[0]\n",
    "        if idx < len(signals):\n",
    "            signal_strengths.append(signals.iloc[idx]['synergy_strength'])\n",
    "        else:\n",
    "            signal_strengths.append(0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=signal_strengths,\n",
    "            y=trade_returns,\n",
    "            mode='markers',\n",
    "            name='Strength vs Return',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=trade_returns,\n",
    "                colorscale='RdYlGn',\n",
    "                showscale=True\n",
    "            )\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # 7. Rolling Sharpe Ratio\n",
    "    rolling_sharpe = portfolio.sharpe_ratio(rolling=252)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rolling_sharpe.index,\n",
    "            y=rolling_sharpe.values,\n",
    "            name='Rolling Sharpe',\n",
    "            line=dict(color='orange')\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Win Rate by Month\n",
    "    trades_df = trade_records.copy()\n",
    "    trades_df['Month'] = pd.to_datetime(trades_df['Entry Timestamp']).dt.to_period('M')\n",
    "    monthly_stats = trades_df.groupby('Month').agg({\n",
    "        'Return [%]': ['count', lambda x: (x > 0).sum() / len(x) * 100]\n",
    "    })\n",
    "    monthly_stats.columns = ['Count', 'Win Rate']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=monthly_stats.index.astype(str),\n",
    "            y=monthly_stats['Win Rate'],\n",
    "            name='Win Rate %',\n",
    "            marker_color='lightgreen'\n",
    "        ),\n",
    "        row=4, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"NW-RQK → MLMI → FVG Synergy Performance Dashboard\",\n",
    "        showlegend=False,\n",
    "        height=1600,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Return %\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Signal Strength\", row=3, col=2)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=4, col=1)\n",
    "    fig.update_xaxes(title_text=\"Month\", row=4, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Value ($)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Return %\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Return %\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Drawdown %\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Return %\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Sharpe Ratio\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Win Rate %\", row=4, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = create_performance_dashboard(signals, portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monte Carlo Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True)\n",
    "def monte_carlo_simulation(returns, n_simulations=1000, n_periods=252):\n",
    "    \"\"\"Run Monte Carlo simulation for confidence intervals\"\"\"\n",
    "    n_returns = len(returns)\n",
    "    final_values = np.zeros(n_simulations)\n",
    "    \n",
    "    for sim in prange(n_simulations):\n",
    "        # Bootstrap sample returns\n",
    "        sim_returns = np.zeros(n_periods)\n",
    "        for i in range(n_periods):\n",
    "            idx = np.random.randint(0, n_returns)\n",
    "            sim_returns[i] = returns[idx]\n",
    "        \n",
    "        # Calculate final value\n",
    "        final_values[sim] = np.prod(1 + sim_returns)\n",
    "    \n",
    "    return final_values\n",
    "\n",
    "def run_monte_carlo_analysis(portfolio):\n",
    "    \"\"\"Run Monte Carlo analysis for strategy validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MONTE CARLO VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    mc_start = time.time()\n",
    "    \n",
    "    # Get trade returns\n",
    "    trade_returns = portfolio.trades.records_readable['Return [%]'].values / 100\n",
    "    \n",
    "    # Run simulation\n",
    "    final_values = monte_carlo_simulation(trade_returns, n_simulations=10000)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mc_returns = (final_values - 1) * 100\n",
    "    percentiles = np.percentile(mc_returns, [5, 25, 50, 75, 95])\n",
    "    \n",
    "    print(f\"\\nMonte Carlo simulation completed in {time.time() - mc_start:.2f} seconds\")\n",
    "    print(\"\\nConfidence Intervals for Annual Returns:\")\n",
    "    print(f\"5th percentile:  {percentiles[0]:.2f}%\")\n",
    "    print(f\"25th percentile: {percentiles[1]:.2f}%\")\n",
    "    print(f\"Median:          {percentiles[2]:.2f}%\")\n",
    "    print(f\"75th percentile: {percentiles[3]:.2f}%\")\n",
    "    print(f\"95th percentile: {percentiles[4]:.2f}%\")\n",
    "    \n",
    "    # Probability of profit\n",
    "    prob_profit = (mc_returns > 0).mean() * 100\n",
    "    print(f\"\\nProbability of Profit: {prob_profit:.1f}%\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=mc_returns,\n",
    "        nbinsx=100,\n",
    "        name='Simulated Returns',\n",
    "        marker_color='lightblue',\n",
    "        opacity=0.7\n",
    "    ))\n",
    "    \n",
    "    # Add percentile lines\n",
    "    for i, (p, label) in enumerate(zip(percentiles, ['5%', '25%', '50%', '75%', '95%'])):\n",
    "        fig.add_vline(x=p, line_dash=\"dash\", line_color=\"red\" if i < 2 else \"green\",\n",
    "                     annotation_text=f\"{label}: {p:.1f}%\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Monte Carlo Simulation Results (10,000 runs)\",\n",
    "        xaxis_title=\"Annual Return %\",\n",
    "        yaxis_title=\"Frequency\",\n",
    "        template='plotly_dark',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return mc_returns, percentiles\n",
    "\n",
    "# Run Monte Carlo validation\n",
    "mc_returns, percentiles = run_monte_carlo_analysis(portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics and Trade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(signals, portfolio, stats):\n",
    "    \"\"\"Generate comprehensive performance report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Time period analysis\n",
    "    start_date = signals.index[0]\n",
    "    end_date = signals.index[-1]\n",
    "    n_years = (end_date - start_date).days / 365.25\n",
    "    \n",
    "    print(f\"\\nBacktest Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Duration: {n_years:.1f} years\")\n",
    "    \n",
    "    # Trade analysis\n",
    "    trades = portfolio.trades.records_readable\n",
    "    total_trades = len(trades)\n",
    "    winning_trades = len(trades[trades['Return [%]'] > 0])\n",
    "    losing_trades = len(trades[trades['Return [%]'] < 0])\n",
    "    \n",
    "    print(f\"\\nTrade Statistics:\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Trades per Year: {total_trades / n_years:.0f}\")\n",
    "    print(f\"Winning Trades: {winning_trades}\")\n",
    "    print(f\"Losing Trades: {losing_trades}\")\n",
    "    print(f\"Win Rate: {(winning_trades / total_trades * 100) if total_trades > 0 else 0:.2f}%\")\n",
    "    \n",
    "    # Return analysis\n",
    "    avg_win = trades[trades['Return [%]'] > 0]['Return [%]'].mean() if winning_trades > 0 else 0\n",
    "    avg_loss = trades[trades['Return [%]'] < 0]['Return [%]'].mean() if losing_trades > 0 else 0\n",
    "    profit_factor = abs(avg_win * winning_trades / (avg_loss * losing_trades)) if losing_trades > 0 else np.inf\n",
    "    \n",
    "    print(f\"\\nReturn Metrics:\")\n",
    "    print(f\"Average Win: {avg_win:.2f}%\")\n",
    "    print(f\"Average Loss: {avg_loss:.2f}%\")\n",
    "    print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "    print(f\"Expectancy: {trades['Return [%]'].mean():.2f}%\")\n",
    "    \n",
    "    # Risk metrics\n",
    "    print(f\"\\nRisk Metrics:\")\n",
    "    print(f\"Maximum Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "    print(f\"Average Drawdown: {portfolio.drawdown().mean() * 100:.2f}%\")\n",
    "    print(f\"Calmar Ratio: {stats['Calmar Ratio']:.2f}\")\n",
    "    print(f\"Sortino Ratio: {stats['Sortino Ratio']:.2f}\")\n",
    "    \n",
    "    # Signal quality analysis\n",
    "    bull_signals = signals[signals['synergy_bull']]\n",
    "    bear_signals = signals[signals['synergy_bear']]\n",
    "    \n",
    "    print(f\"\\nSignal Analysis:\")\n",
    "    print(f\"Total Bull Signals: {len(bull_signals)}\")\n",
    "    print(f\"Total Bear Signals: {len(bear_signals)}\")\n",
    "    print(f\"Average Signal Strength: {signals['synergy_strength'][signals['synergy_strength'] > 0].mean():.3f}\")\n",
    "    \n",
    "    # Monthly performance\n",
    "    monthly_returns = portfolio.returns().resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "    positive_months = (monthly_returns > 0).sum()\n",
    "    total_months = len(monthly_returns)\n",
    "    \n",
    "    print(f\"\\nMonthly Performance:\")\n",
    "    print(f\"Positive Months: {positive_months}/{total_months} ({positive_months/total_months*100:.1f}%)\")\n",
    "    print(f\"Best Month: {monthly_returns.max() * 100:.2f}%\")\n",
    "    print(f\"Worst Month: {monthly_returns.min() * 100:.2f}%\")\n",
    "    print(f\"Average Monthly Return: {monthly_returns.mean() * 100:.2f}%\")\n",
    "    \n",
    "    return trades\n",
    "\n",
    "# Generate report\n",
    "trades_df = generate_comprehensive_report(signals, portfolio, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save signals\n",
    "signals.to_csv('/home/QuantNova/AlgoSpace-8/results/synergy_3_nwrqk_mlmi_fvg_signals.csv')\n",
    "print(\"✓ Signals saved\")\n",
    "\n",
    "# Save trade records\n",
    "trades_df.to_csv('/home/QuantNova/AlgoSpace-8/results/synergy_3_nwrqk_mlmi_fvg_trades.csv')\n",
    "print(\"✓ Trade records saved\")\n",
    "\n",
    "# Save performance metrics\n",
    "with open('/home/QuantNova/AlgoSpace-8/results/synergy_3_nwrqk_mlmi_fvg_metrics.txt', 'w') as f:\n",
    "    for key, value in stats.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "print(\"✓ Performance metrics saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}