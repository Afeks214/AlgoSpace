{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 3: NW-RQK → MLMI → FVG Trading Strategy\n",
    "\n",
    "## Ultra-High Performance Implementation with VectorBT and Numba\n",
    "\n",
    "This notebook implements the third synergy pattern where:\n",
    "1. **NW-RQK** (Nadaraya-Watson Rational Quadratic Kernel) provides the initial trend signal\n",
    "2. **MLMI** (Machine Learning Market Intelligence) confirms the market regime\n",
    "3. **FVG** (Fair Value Gap) validates the final entry zone\n",
    "\n",
    "### Key Features:\n",
    "- Ultra-fast execution using Numba JIT compilation with parallel processing\n",
    "- VectorBT for lightning-fast vectorized backtesting\n",
    "- Natural trade generation (2,500-4,500 trades over 5 years)\n",
    "- Professional visualizations and comprehensive metrics\n",
    "- Sub-10 second full backtest execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries with comprehensive error handling\nimport numpy as np\nimport pandas as pd\nimport vectorbt as vbt\nfrom numba import njit, prange, float64, int64, boolean\nfrom numba.typed import List\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nfrom datetime import datetime, timedelta\nimport time\nfrom scipy import stats\nimport logging\nimport os\nimport sys\nfrom typing import Dict, Any, Tuple, Optional\nimport json\nimport traceback  # Added missing import\nfrom functools import lru_cache  # For caching expensive operations\nimport threading  # For thread-safe operations\nfrom pathlib import Path  # For better file operations\n\n# Version checks\nREQUIRED_VERSIONS = {\n    'pandas': '1.3.0',\n    'numpy': '1.19.0',\n    'numba': '0.53.0',\n    'vectorbt': '0.20.0'\n}\n\ndef check_package_versions():\n    \"\"\"Check if required package versions are met\"\"\"\n    import importlib\n    for package, min_version in REQUIRED_VERSIONS.items():\n        try:\n            module = importlib.import_module(package)\n            version = getattr(module, '__version__', '0.0.0')\n            if version < min_version:\n                warnings.warn(f\"{package} version {version} is below recommended {min_version}\")\n        except ImportError:\n            raise ImportError(f\"Required package {package} not found\")\n\n# Check versions on import\ntry:\n    check_package_versions()\nexcept Exception as e:\n    warnings.warn(f\"Version check failed: {str(e)}\")\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\n# Configure VectorBT with error handling\ntry:\n    vbt.settings.set_theme('dark')\n    vbt.settings['plotting']['layout']['width'] = 1200\n    vbt.settings['plotting']['layout']['height'] = 800\nexcept Exception as e:\n    warnings.warn(f\"VectorBT configuration failed: {str(e)}\")\n\n# Enhanced logging setup with rotation\nfrom logging.handlers import RotatingFileHandler\n\ndef setup_logging(log_file='synergy_3_strategy.log', level=logging.INFO):\n    \"\"\"Setup production-grade logging with rotation\"\"\"\n    logger = logging.getLogger('Synergy3Strategy')\n    logger.setLevel(level)\n    \n    # Clear existing handlers\n    logger.handlers = []\n    \n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(level)\n    \n    # File handler with rotation\n    file_handler = RotatingFileHandler(\n        log_file,\n        maxBytes=10*1024*1024,  # 10MB\n        backupCount=5\n    )\n    file_handler.setLevel(level)\n    \n    # Formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\n\n# Setup logger\nlogger = setup_logging()\n\n# Performance tracking decorator\ndef track_performance(func):\n    \"\"\"Decorator to track function execution time\"\"\"\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        try:\n            result = func(*args, **kwargs)\n            execution_time = time.time() - start_time\n            logger.debug(f\"{func.__name__} executed in {execution_time:.3f} seconds\")\n            return result\n        except Exception as e:\n            execution_time = time.time() - start_time\n            logger.error(f\"{func.__name__} failed after {execution_time:.3f} seconds: {str(e)}\")\n            raise\n    return wrapper\n\n# Thread-safe configuration management\nclass StrategyConfig:\n    \"\"\"Centralized configuration for the strategy with validation and thread safety\"\"\"\n    \n    _lock = threading.Lock()\n    _instance = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n        return cls._instance\n    \n    def __init__(self):\n        if hasattr(self, '_initialized'):\n            return\n        self._initialized = True\n        \n        # Data Configuration - FIXED PATHS\n        self.DATA_PATH_30M = '/home/QuantNova/AlgoSpace-Strategy-1/NQ - 30 min - ETH.csv'\n        self.DATA_PATH_5M = '/home/QuantNova/AlgoSpace-Strategy-1/@NQ - 5 min - ETH.csv'\n        # Updated datetime formats to handle both cases\n        self.DATETIME_FORMATS = ['%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M', '%Y-%m-%d %H:%M:%S%z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']\n        \n        # NW-RQK Configuration\n        self.NWRQK_WINDOW = 30\n        self.NWRQK_N_KERNELS = 3\n        self.NWRQK_ALPHAS = [0.3, 0.5, 0.7]\n        self.NWRQK_LENGTH_SCALES = [30.0, 50.0, 70.0]\n        self.NWRQK_THRESHOLD = 0.002\n        self.NWRQK_VOLATILITY_ADAPTIVE = True\n        \n        # MLMI Configuration\n        self.MLMI_WINDOW = 10\n        self.MLMI_K_NEIGHBORS = 5\n        self.MLMI_FEATURE_WINDOW = 3\n        self.MLMI_LOOKBACK = 100\n        self.MLMI_RSI_PERIOD = 14\n        self.MLMI_VOLATILITY_WINDOW = 20\n        self.MLMI_VOLATILITY_SCALE = 2.0\n        self.MLMI_BULL_THRESHOLD = 0.65\n        self.MLMI_BEAR_THRESHOLD = 0.35\n        self.MLMI_CONFIDENCE_THRESHOLD = 0.3\n        \n        # FVG Configuration\n        self.FVG_MIN_GAP_PCT = 0.001\n        self.FVG_VOLUME_FACTOR = 1.2\n        self.FVG_VOLUME_WINDOW = 20\n        \n        # Synergy Configuration\n        self.SYNERGY_WINDOW = 30\n        self.SYNERGY_NWRQK_STRENGTH_THRESHOLD = 0.5\n        self.SYNERGY_MLMI_CONFIDENCE_THRESHOLD = 0.3\n        self.SYNERGY_STATE_DECAY_WINDOW = 30\n        \n        # Risk Management\n        self.POSITION_SIZE_BASE = 0.1\n        self.STOP_LOSS_PCT = 0.02\n        self.TAKE_PROFIT_PCT = 0.03\n        self.MAX_DRAWDOWN_LIMIT = 0.15\n        self.MAX_DAILY_LOSS = 0.05\n        \n        # Backtesting\n        self.INITIAL_CAPITAL = 100000\n        self.TRADING_FEES = 0.001\n        self.SLIPPAGE = 0.0005\n        \n        # Validation\n        self.MAX_MISSING_DATA_PCT = 0.05\n        self.OUTLIER_STD_THRESHOLD = 10\n        self.MIN_DATA_POINTS = 1000\n        \n        # Performance settings\n        self.CACHE_SIZE = 128  # LRU cache size\n        self.MAX_WORKERS = 4  # Max parallel workers\n        self.MEMORY_LIMIT_MB = 4096  # Memory limit\n    \n    def validate(self):\n        \"\"\"Validate configuration parameters with comprehensive checks\"\"\"\n        errors = []\n        \n        # Data validation\n        if not Path(self.DATA_PATH_30M).exists():\n            errors.append(f\"30m data file not found: {self.DATA_PATH_30M}\")\n        if not Path(self.DATA_PATH_5M).exists():\n            errors.append(f\"5m data file not found: {self.DATA_PATH_5M}\")\n        \n        # NW-RQK validation\n        if self.NWRQK_WINDOW < 10:\n            logger.warning(\"NW-RQK window < 10 may produce unstable results\")\n        if len(self.NWRQK_ALPHAS) != self.NWRQK_N_KERNELS:\n            errors.append(\"Number of alphas must match n_kernels\")\n        if not all(0 < alpha < 1 for alpha in self.NWRQK_ALPHAS):\n            errors.append(\"All alphas must be between 0 and 1\")\n        \n        # MLMI validation\n        if self.MLMI_BULL_THRESHOLD <= self.MLMI_BEAR_THRESHOLD:\n            errors.append(\"Bull threshold must be greater than bear threshold\")\n        if not 0 < self.MLMI_CONFIDENCE_THRESHOLD < 1:\n            errors.append(\"MLMI confidence threshold must be between 0 and 1\")\n        \n        # Risk management validation\n        if not 0 < self.STOP_LOSS_PCT < 1:\n            errors.append(\"Stop loss must be between 0 and 1\")\n        if not 0 < self.TAKE_PROFIT_PCT < 1:\n            errors.append(\"Take profit must be between 0 and 1\")\n        if not 0 < self.POSITION_SIZE_BASE < 1:\n            errors.append(\"Position size must be between 0 and 1\")\n        \n        # Raise error if critical validations fail\n        if errors:\n            error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors)\n            raise ValueError(error_msg)\n        \n        logger.info(\"Configuration validated successfully\")\n        return True\n    \n    def to_dict(self):\n        \"\"\"Convert configuration to dictionary for serialization\"\"\"\n        with self._lock:\n            return {\n                attr: getattr(self, attr)\n                for attr in dir(self)\n                if not attr.startswith('_') and not callable(getattr(self, attr))\n            }\n    \n    def update(self, **kwargs):\n        \"\"\"Update configuration with validation\"\"\"\n        with self._lock:\n            for key, value in kwargs.items():\n                if hasattr(self, key):\n                    setattr(self, key, value)\n                else:\n                    logger.warning(f\"Unknown configuration parameter: {key}\")\n            self.validate()\n\n# Initialize and validate configuration\ntry:\n    StrategyConfig = StrategyConfig()\n    StrategyConfig.validate()\n    logger.info(\"Strategy configuration initialized successfully\")\nexcept Exception as e:\n    logger.error(f\"Configuration initialization failed: {str(e)}\")\n    raise\n\n# Global performance metrics\nclass PerformanceMetrics:\n    \"\"\"Track global performance metrics\"\"\"\n    def __init__(self):\n        self.start_time = time.time()\n        self.metrics = {}\n        self._lock = threading.Lock()\n    \n    def add_metric(self, name, value):\n        with self._lock:\n            self.metrics[name] = value\n    \n    def get_summary(self):\n        with self._lock:\n            total_time = time.time() - self.start_time\n            return {\n                'total_execution_time': total_time,\n                **self.metrics\n            }\n\n# Initialize global metrics\nperformance_metrics = PerformanceMetrics()\n\nlogger.info(\"=\"*60)\nlogger.info(\"SYNERGY 3: NW-RQK → MLMI → FVG STRATEGY\")\nlogger.info(\"Production-grade implementation initialized\")\nlogger.info(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ultra-Fast Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data with optimized parsing and comprehensive error handling\ndef load_data():\n    \"\"\"Load and preprocess data with ultra-fast parsing and robust error handling\"\"\"\n    logger.info(\"Starting data loading process...\")\n    start_time = time.time()\n    \n    try:\n        # Validate file existence\n        if not os.path.exists(StrategyConfig.DATA_PATH_30M):\n            raise FileNotFoundError(f\"30m data file not found: {StrategyConfig.DATA_PATH_30M}\")\n        if not os.path.exists(StrategyConfig.DATA_PATH_5M):\n            raise FileNotFoundError(f\"5m data file not found: {StrategyConfig.DATA_PATH_5M}\")\n        \n        # Load 30-minute data with error handling\n        logger.info(f\"Loading 30m data from {StrategyConfig.DATA_PATH_30M}\")\n        df_30m = load_single_timeframe(StrategyConfig.DATA_PATH_30M, '30m')\n        \n        # Load 5-minute data with error handling\n        logger.info(f\"Loading 5m data from {StrategyConfig.DATA_PATH_5M}\")\n        df_5m = load_single_timeframe(StrategyConfig.DATA_PATH_5M, '5m')\n        \n        # Validate data quality\n        df_30m = validate_and_clean_data(df_30m, '30m')\n        df_5m = validate_and_clean_data(df_5m, '5m')\n        \n        # Add derived features\n        df_30m = add_robust_features(df_30m, '30m')\n        df_5m = add_robust_features(df_5m, '5m')\n        \n        # Align data timeframes\n        df_30m, df_5m = align_timeframes(df_30m, df_5m)\n        \n        # Final data integrity check\n        perform_final_validation(df_30m, df_5m)\n        \n        logger.info(f\"Data loading completed in {time.time() - start_time:.2f} seconds\")\n        logger.info(f\"30m data: {len(df_30m)} bars from {df_30m.index[0]} to {df_30m.index[-1]}\")\n        logger.info(f\"5m data: {len(df_5m)} bars from {df_5m.index[0]} to {df_5m.index[-1]}\")\n        \n        return df_30m, df_5m\n        \n    except Exception as e:\n        logger.error(f\"Critical error in data loading: {str(e)}\")\n        raise\n\ndef load_single_timeframe(file_path: str, timeframe: str) -> pd.DataFrame:\n    \"\"\"Load data for a single timeframe with robust error handling\"\"\"\n    try:\n        # Load CSV with error handling\n        df = pd.read_csv(file_path, na_values=['', 'null', 'NULL', 'NaN'])\n        \n        # Convert column names to lowercase for consistency\n        df.columns = df.columns.str.lower()\n        \n        # Check for required columns (now lowercase)\n        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n        \n        # Handle case where datetime column might be named differently\n        if 'timestamp' not in df.columns and 'datetime' in df.columns:\n            df.rename(columns={'datetime': 'timestamp'}, inplace=True)\n        elif 'timestamp' not in df.columns and 'date' in df.columns:\n            df.rename(columns={'date': 'timestamp'}, inplace=True)\n        \n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns in {timeframe} data: {missing_columns}\")\n        \n        # Parse timestamps with multiple approaches for robustness\n        logger.info(f\"Parsing timestamps for {timeframe} data...\")\n        \n        # First try with dayfirst=True and format='mixed' for maximum flexibility\n        df['timestamp'] = pd.to_datetime(df['timestamp'], dayfirst=True, format='mixed', errors='coerce')\n        \n        # Check initial parsing success\n        initial_valid = df['timestamp'].notna().sum()\n        \n        # If many failed, try specific formats for remaining NaT values\n        if df['timestamp'].isna().any():\n            failed_mask = df['timestamp'].isna()\n            failed_count = failed_mask.sum()\n            logger.info(f\"Retrying {failed_count} failed timestamp parsings...\")\n            \n            # Try common formats on failed timestamps\n            formats_to_try = [\n                '%d/%m/%Y %H:%M:%S',\n                '%d/%m/%Y %H:%M',\n                '%Y-%m-%d %H:%M:%S',\n                '%m/%d/%Y %H:%M:%S',\n                '%m/%d/%Y %H:%M'\n            ]\n            \n            original_timestamps = df.loc[failed_mask, 'timestamp'].copy()\n            for fmt in formats_to_try:\n                if failed_mask.any():\n                    try:\n                        parsed = pd.to_datetime(df.loc[failed_mask, 'timestamp'], format=fmt, errors='coerce')\n                        valid_parsed = parsed.notna()\n                        if valid_parsed.any():\n                            df.loc[failed_mask & valid_parsed, 'timestamp'] = parsed[valid_parsed]\n                            failed_mask = df['timestamp'].isna()\n                            logger.debug(f\"Format '{fmt}' parsed {valid_parsed.sum()} timestamps\")\n                    except:\n                        continue\n        \n        # Final parsing statistics\n        valid_timestamps = df['timestamp'].notna().sum()\n        total_rows = len(df)\n        parse_success_rate = valid_timestamps / total_rows\n        \n        logger.info(f\"Timestamp parsing complete: {parse_success_rate:.1%} success ({valid_timestamps}/{total_rows})\")\n        \n        # Remove rows with invalid datetime\n        invalid_datetime = df['timestamp'].isna()\n        if invalid_datetime.any():\n            logger.warning(f\"Removing {invalid_datetime.sum()} rows with unparseable timestamps\")\n            df = df[~invalid_datetime]\n        \n        # Set index and sort\n        df = df.set_index('timestamp').sort_index()\n        \n        # Remove duplicate timestamps\n        duplicates = df.index.duplicated()\n        if duplicates.any():\n            logger.warning(f\"Removing {duplicates.sum()} duplicate timestamps in {timeframe} data\")\n            df = df[~duplicates]\n        \n        # Ensure numeric data types\n        numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n        for col in numeric_columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n        \n        # Check minimum data requirement\n        if len(df) < StrategyConfig.MIN_DATA_POINTS:\n            logger.warning(f\"Data has {len(df)} rows, which is less than minimum required {StrategyConfig.MIN_DATA_POINTS}\")\n            # Lower the requirement for this specific case\n            if len(df) < 100:\n                raise ValueError(f\"Insufficient data: {len(df)} rows, need at least 100\")\n        \n        logger.info(f\"Successfully loaded {len(df)} rows of {timeframe} data\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error loading {timeframe} data: {str(e)}\")\n        raise\n\ndef validate_and_clean_data(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n    \"\"\"Validate data quality and handle issues robustly\"\"\"\n    logger.info(f\"Validating {timeframe} data...\")\n    \n    try:\n        # Enhanced data integrity checks\n        initial_row_count = len(df)\n        \n        # Check for missing values\n        missing_count = df.isnull().sum()\n        total_missing = missing_count.sum()\n        \n        if total_missing > 0:\n            missing_pct = total_missing / (len(df) * len(df.columns))\n            logger.warning(f\"Found {total_missing} missing values ({missing_pct:.2%}) in {timeframe} data\")\n            \n            if missing_pct > StrategyConfig.MAX_MISSING_DATA_PCT:\n                logger.error(f\"Too many missing values: {missing_pct:.2%}\")\n                # Try to recover by forward/backward filling\n                # Updated: Replace deprecated fillna(method='ffill') with df.ffill()\n                df = df.ffill().bfill()\n                \n                # Check again\n                remaining_missing = df.isnull().sum().sum()\n                if remaining_missing > 0:\n                    logger.warning(f\"Still have {remaining_missing} missing values after filling\")\n                    # Fill remaining with reasonable defaults\n                    df['volume'] = df['volume'].fillna(0)\n                    for col in ['open', 'high', 'low', 'close']:\n                        if col in df.columns:\n                            df[col] = df[col].fillna(df[col].mean())\n        \n        # Validate price relationships\n        invalid_candles = (\n            (df['high'] < df['low']) | \n            (df['high'] < df['open']) | \n            (df['high'] < df['close']) |\n            (df['low'] > df['open']) | \n            (df['low'] > df['close'])\n        )\n        \n        if invalid_candles.any():\n            n_invalid = invalid_candles.sum()\n            logger.warning(f\"Found {n_invalid} invalid candles in {timeframe} data, fixing...\")\n            \n            # Fix invalid candles\n            df.loc[invalid_candles, 'high'] = df.loc[invalid_candles, ['open', 'close', 'high']].max(axis=1)\n            df.loc[invalid_candles, 'low'] = df.loc[invalid_candles, ['open', 'close', 'low']].min(axis=1)\n        \n        # Check for outliers\n        for col in ['open', 'high', 'low', 'close']:\n            if col not in df.columns:\n                continue\n                \n            # Calculate rolling statistics\n            rolling_mean = df[col].rolling(window=100, min_periods=10).mean()\n            rolling_std = df[col].rolling(window=100, min_periods=10).std()\n            \n            # Identify outliers\n            z_scores = np.abs((df[col] - rolling_mean) / rolling_std)\n            outliers = z_scores > StrategyConfig.OUTLIER_STD_THRESHOLD\n            \n            if outliers.any():\n                n_outliers = outliers.sum()\n                logger.warning(f\"Found {n_outliers} outliers in {col} for {timeframe} data\")\n                \n                # Cap outliers at threshold\n                df.loc[outliers, col] = rolling_mean[outliers] + np.sign(\n                    df.loc[outliers, col] - rolling_mean[outliers]\n                ) * StrategyConfig.OUTLIER_STD_THRESHOLD * rolling_std[outliers]\n        \n        # Ensure no negative prices\n        negative_prices = (df[['open', 'high', 'low', 'close']] < 0).any(axis=1)\n        if negative_prices.any():\n            logger.error(f\"Found {negative_prices.sum()} rows with negative prices, removing...\")\n            df = df[~negative_prices]\n        \n        # Ensure no zero prices\n        zero_prices = (df[['open', 'high', 'low', 'close']] == 0).any(axis=1)\n        if zero_prices.any():\n            logger.warning(f\"Found {zero_prices.sum()} rows with zero prices, interpolating...\")\n            for col in ['open', 'high', 'low', 'close']:\n                if col in df.columns:\n                    df[col] = df[col].replace(0, np.nan).interpolate(method='linear')\n        \n        # Check for time gaps\n        time_diff = df.index.to_series().diff()\n        expected_freq = pd.Timedelta(timeframe)\n        large_gaps = time_diff > expected_freq * 2\n        \n        if large_gaps.any():\n            logger.warning(f\"Found {large_gaps.sum()} time gaps larger than expected in {timeframe} data\")\n            # Log details of largest gaps\n            largest_gaps = time_diff[large_gaps].nlargest(5)\n            for idx, gap in largest_gaps.items():\n                logger.debug(f\"  Gap at {idx}: {gap}\")\n        \n        # Additional integrity checks\n        # Check for stuck prices (no movement for extended periods)\n        price_unchanged = (df['close'].diff() == 0)\n        consecutive_unchanged = price_unchanged.rolling(window=10).sum()\n        stuck_periods = consecutive_unchanged >= 10\n        \n        if stuck_periods.any():\n            logger.warning(f\"Found {stuck_periods.sum()} periods with stuck prices (10+ bars unchanged)\")\n        \n        # Check volume integrity\n        zero_volume = df['volume'] == 0\n        if zero_volume.sum() > len(df) * 0.5:\n            logger.warning(f\"More than 50% of bars have zero volume - data quality may be compromised\")\n        \n        # Final data quality metrics\n        final_row_count = len(df)\n        rows_removed = initial_row_count - final_row_count\n        \n        if rows_removed > 0:\n            logger.info(f\"Data validation removed {rows_removed} rows ({rows_removed/initial_row_count:.1%})\")\n        \n        logger.info(f\"Validation completed for {timeframe} data\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error in data validation for {timeframe}: {str(e)}\")\n        raise\n\ndef add_robust_features(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n    \"\"\"Add derived features with error handling and validation\"\"\"\n    try:\n        # Calculate returns with handling for division by zero\n        df['returns'] = df['close'].pct_change().fillna(0)\n        \n        # Cap extreme returns\n        extreme_returns = np.abs(df['returns']) > 0.5  # 50% moves\n        if extreme_returns.any():\n            logger.warning(f\"Capping {extreme_returns.sum()} extreme returns in {timeframe} data\")\n            df.loc[extreme_returns, 'returns'] = np.sign(df.loc[extreme_returns, 'returns']) * 0.5\n        \n        # Calculate log returns safely\n        df['log_returns'] = np.log1p(df['returns'])  # log1p is more stable for small values\n        \n        # Calculate volatility with minimum periods\n        df['volatility'] = df['returns'].rolling(window=20, min_periods=5).std().fillna(0)\n        \n        # Volume metrics with safety checks\n        df['volume_sma'] = df['volume'].rolling(window=20, min_periods=5).mean().fillna(df['volume'])\n        df['volume_ratio'] = np.where(\n            df['volume_sma'] > 0,\n            df['volume'] / df['volume_sma'],\n            1.0\n        )\n        \n        # Price ranges with safety checks\n        df['high_low_range'] = np.where(\n            df['close'] > 0,\n            (df['high'] - df['low']) / df['close'],\n            0\n        )\n        df['close_open_range'] = np.where(\n            df['open'] > 0,\n            (df['close'] - df['open']) / df['open'],\n            0\n        )\n        \n        # VWAP calculation with cumulative approach\n        typical_price = (df['high'] + df['low'] + df['close']) / 3\n        df['vwap'] = (typical_price * df['volume']).cumsum() / df['volume'].cumsum()\n        df['vwap'] = df['vwap'].fillna(typical_price)  # Handle initial NaN values\n        \n        # Add trend indicators\n        df['sma_20'] = df['close'].rolling(window=20, min_periods=5).mean()\n        df['sma_50'] = df['close'].rolling(window=50, min_periods=10).mean()\n        df['price_position'] = np.where(\n            df['sma_20'] > 0,\n            (df['close'] - df['sma_20']) / df['sma_20'],\n            0\n        )\n        \n        # ATR calculation for risk management\n        high_low = df['high'] - df['low']\n        high_close = np.abs(df['high'] - df['close'].shift())\n        low_close = np.abs(df['low'] - df['close'].shift())\n        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n        df['atr'] = true_range.rolling(window=14, min_periods=5).mean().fillna(true_range)\n        \n        # Validate all features\n        feature_cols = ['returns', 'log_returns', 'volatility', 'volume_sma', \n                       'volume_ratio', 'high_low_range', 'close_open_range', \n                       'vwap', 'sma_20', 'sma_50', 'price_position', 'atr']\n        \n        for col in feature_cols:\n            if col in df.columns:\n                # Check for infinite values\n                inf_count = np.isinf(df[col]).sum()\n                if inf_count > 0:\n                    logger.warning(f\"Replacing {inf_count} infinite values in {col}\")\n                    df[col] = df[col].replace([np.inf, -np.inf], 0)\n                \n                # Check for NaN values\n                nan_count = df[col].isna().sum()\n                if nan_count > 0:\n                    logger.warning(f\"Found {nan_count} NaN values in {col}, filling with 0\")\n                    df[col] = df[col].fillna(0)\n        \n        logger.info(f\"Added features to {timeframe} data\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error adding features to {timeframe} data: {str(e)}\")\n        raise\n\ndef align_timeframes(df_30m: pd.DataFrame, df_5m: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Align data timeframes with validation\"\"\"\n    try:\n        # Find common time range\n        start_time = max(df_30m.index[0], df_5m.index[0])\n        end_time = min(df_30m.index[-1], df_5m.index[-1])\n        \n        logger.info(f\"Aligning data from {start_time} to {end_time}\")\n        \n        # Filter to common range\n        df_30m = df_30m[(df_30m.index >= start_time) & (df_30m.index <= end_time)]\n        df_5m = df_5m[(df_5m.index >= start_time) & (df_5m.index <= end_time)]\n        \n        # Validate alignment\n        expected_ratio = 6  # 30min / 5min\n        actual_ratio = len(df_5m) / len(df_30m) if len(df_30m) > 0 else 0\n        \n        if abs(actual_ratio - expected_ratio) > 1:\n            logger.warning(f\"Unexpected data ratio: {actual_ratio:.2f} (expected ~{expected_ratio})\")\n        \n        # Check time alignment\n        # Every 30m bar should have a corresponding 5m bar\n        missing_alignments = 0\n        for ts in df_30m.index:\n            if ts not in df_5m.index:\n                missing_alignments += 1\n        \n        if missing_alignments > 0:\n            logger.warning(f\"Found {missing_alignments} 30m timestamps without corresponding 5m data\")\n        \n        logger.info(f\"Final data sizes - 30m: {len(df_30m)}, 5m: {len(df_5m)}\")\n        \n        return df_30m, df_5m\n        \n    except Exception as e:\n        logger.error(f\"Error aligning timeframes: {str(e)}\")\n        raise\n\ndef perform_final_validation(df_30m: pd.DataFrame, df_5m: pd.DataFrame):\n    \"\"\"Perform final comprehensive validation of loaded data\"\"\"\n    try:\n        logger.info(\"Performing final data validation...\")\n        \n        # Check for minimum required data\n        min_days = 30\n        data_range = (df_30m.index[-1] - df_30m.index[0]).days\n        \n        if data_range < min_days:\n            logger.warning(f\"Data spans only {data_range} days, less than recommended {min_days} days\")\n        \n        # Validate data consistency between timeframes\n        # Check if 30m highs/lows are within 5m range\n        validation_samples = min(100, len(df_30m))\n        inconsistencies = 0\n        \n        for i in range(validation_samples):\n            ts_30m = df_30m.index[i]\n            # Find corresponding 5m bars\n            mask_5m = (df_5m.index >= ts_30m) & (df_5m.index < ts_30m + pd.Timedelta('30min'))\n            \n            if mask_5m.any():\n                high_5m = df_5m.loc[mask_5m, 'high'].max()\n                low_5m = df_5m.loc[mask_5m, 'low'].min()\n                \n                # Check consistency (with small tolerance for rounding)\n                tolerance = 0.01  # 1 cent tolerance\n                if abs(df_30m.loc[ts_30m, 'high'] - high_5m) > tolerance:\n                    inconsistencies += 1\n                if abs(df_30m.loc[ts_30m, 'low'] - low_5m) > tolerance:\n                    inconsistencies += 1\n        \n        if inconsistencies > 0:\n            logger.warning(f\"Found {inconsistencies} price inconsistencies between 30m and 5m data\")\n        \n        # Generate data quality report\n        quality_report = {\n            '30m_data': {\n                'total_bars': len(df_30m),\n                'date_range': f\"{df_30m.index[0]} to {df_30m.index[-1]}\",\n                'missing_values': df_30m.isnull().sum().sum(),\n                'zero_volume_bars': (df_30m['volume'] == 0).sum(),\n                'price_range': f\"${df_30m['low'].min():.2f} - ${df_30m['high'].max():.2f}\"\n            },\n            '5m_data': {\n                'total_bars': len(df_5m),\n                'date_range': f\"{df_5m.index[0]} to {df_5m.index[-1]}\",\n                'missing_values': df_5m.isnull().sum().sum(),\n                'zero_volume_bars': (df_5m['volume'] == 0).sum(),\n                'price_range': f\"${df_5m['low'].min():.2f} - ${df_5m['high'].max():.2f}\"\n            }\n        }\n        \n        logger.info(\"Data Quality Report:\")\n        for timeframe, metrics in quality_report.items():\n            logger.info(f\"\\n{timeframe}:\")\n            for metric, value in metrics.items():\n                logger.info(f\"  {metric}: {value}\")\n        \n        logger.info(\"Final data validation completed successfully\")\n        \n    except Exception as e:\n        logger.error(f\"Error in final validation: {str(e)}\")\n        # Non-critical error, don't raise\n\n# Load the data with error handling\ntry:\n    df_30m, df_5m = load_data()\n    logger.info(\"Data loading successful!\")\nexcept Exception as e:\n    logger.error(f\"Failed to load data: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced NW-RQK Implementation with Multi-Kernel Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(fastmath=True, cache=True)\ndef rational_quadratic_kernel(x1, x2, alpha=0.5, length_scale=50.0):\n    \"\"\"Rational Quadratic Kernel for NW-RQK\"\"\"\n    # Validate inputs\n    if alpha <= 0 or alpha >= 1:\n        alpha = 0.5  # Default safe value\n    if length_scale <= 0:\n        length_scale = 50.0  # Default safe value\n    \n    diff = x1 - x2\n    return (1.0 + (diff * diff) / (2.0 * alpha * length_scale * length_scale)) ** (-alpha)\n\n@njit(fastmath=True, cache=True)\ndef gaussian_kernel(x1, x2, length_scale=50.0):\n    \"\"\"Gaussian Kernel for ensemble\"\"\"\n    if length_scale <= 0:\n        length_scale = 50.0\n    \n    diff = x1 - x2\n    return np.exp(-0.5 * (diff * diff) / (length_scale * length_scale))\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef nwrqk_ensemble(prices, window=30, n_kernels=3):\n    \"\"\"Multi-kernel ensemble NW-RQK implementation with validation\"\"\"\n    n = len(prices)\n    nwrqk_values = np.zeros(n)\n    \n    # Input validation\n    if window < 10:\n        window = 10\n    if n_kernels < 1:\n        n_kernels = 3\n    \n    # Kernel parameters for ensemble\n    alphas = np.array([0.3, 0.5, 0.7])\n    length_scales = np.array([30.0, 50.0, 70.0])\n    \n    # Initialize with actual prices for the first window\n    for i in range(min(window, n)):\n        nwrqk_values[i] = prices[i] if i < len(prices) else prices[-1]\n    \n    for i in prange(window, n):\n        # Window data\n        window_prices = prices[i-window:i]\n        \n        # Check for valid window data\n        if np.all(np.isnan(window_prices)) or np.all(window_prices == 0):\n            nwrqk_values[i] = nwrqk_values[i-1] if i > 0 else prices[i]\n            continue\n        \n        # Ensemble predictions\n        predictions = np.zeros(n_kernels)\n        valid_predictions = 0\n        \n        for k in range(n_kernels):\n            # Calculate weights using RQ kernel\n            weights = np.zeros(window)\n            for j in range(window):\n                weights[j] = rational_quadratic_kernel(\n                    float(i), float(i-window+j), \n                    alphas[k % len(alphas)], \n                    length_scales[k % len(length_scales)]\n                )\n            \n            # Normalize weights\n            weight_sum = np.sum(weights)\n            if weight_sum > 1e-10:  # Avoid division by very small numbers\n                weights /= weight_sum\n                predictions[k] = np.sum(weights * window_prices)\n                valid_predictions += 1\n            else:\n                predictions[k] = np.nan\n        \n        # Weighted ensemble - only use valid predictions\n        if valid_predictions > 0:\n            valid_mask = ~np.isnan(predictions)\n            nwrqk_values[i] = np.mean(predictions[valid_mask])\n        else:\n            # Fallback to simple moving average\n            nwrqk_values[i] = np.mean(window_prices)\n    \n    return nwrqk_values\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef calculate_nwrqk_signals(prices, nwrqk_values, threshold=0.002):\n    \"\"\"Generate NW-RQK trend signals with adaptive thresholds and validation\"\"\"\n    n = len(prices)\n    bull_signals = np.zeros(n, dtype=np.bool8)\n    bear_signals = np.zeros(n, dtype=np.bool8)\n    signal_strength = np.zeros(n)\n    \n    # Validate threshold\n    if threshold <= 0:\n        threshold = 0.002\n    \n    for i in prange(1, n):\n        # Skip if invalid data\n        if nwrqk_values[i] <= 0 or prices[i] <= 0 or np.isnan(nwrqk_values[i]) or np.isnan(prices[i]):\n            continue\n        \n        # Price relative to NW-RQK\n        deviation = (prices[i] - nwrqk_values[i]) / nwrqk_values[i]\n        \n        # NW-RQK slope calculation with safety checks\n        if i > 5 and nwrqk_values[i-5] > 0:\n            slope = (nwrqk_values[i] - nwrqk_values[i-5]) / nwrqk_values[i-5]\n            \n            # Adaptive threshold based on volatility\n            vol_window = 20\n            adaptive_threshold = threshold\n            \n            if i > vol_window:\n                returns = np.zeros(vol_window)\n                valid_returns = 0\n                \n                for j in range(vol_window):\n                    if i-j-1 >= 0 and prices[i-j-1] > 0 and prices[i-j] > 0:\n                        returns[valid_returns] = (prices[i-j] - prices[i-j-1]) / prices[i-j-1]\n                        valid_returns += 1\n                \n                if valid_returns > 5:  # Need minimum returns for volatility\n                    volatility = np.std(returns[:valid_returns])\n                    # Cap volatility adjustment to prevent extreme thresholds\n                    volatility = min(volatility, 0.1)  # Cap at 10% volatility\n                    adaptive_threshold = threshold * (1 + volatility * 10)\n            \n            # Strong trend signals with deviation bounds\n            if slope > adaptive_threshold and -0.05 < deviation < 0.05:  # Within 5% of NW-RQK\n                bull_signals[i] = True\n                signal_strength[i] = min(slope / adaptive_threshold, 2.0)\n            elif slope < -adaptive_threshold and -0.05 < deviation < 0.05:\n                bear_signals[i] = True\n                signal_strength[i] = min(abs(slope) / adaptive_threshold, 2.0)\n    \n    return bull_signals, bear_signals, signal_strength\n\n# Add validation wrapper for NW-RQK calculation\ndef calculate_nwrqk_with_validation(prices, config=None):\n    \"\"\"Calculate NW-RQK with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input\n        if len(prices) < config.NWRQK_WINDOW * 2:\n            raise ValueError(f\"Insufficient data for NW-RQK: need at least {config.NWRQK_WINDOW * 2} points\")\n        \n        # Check for valid prices\n        valid_prices = prices[~np.isnan(prices) & (prices > 0)]\n        if len(valid_prices) < len(prices) * 0.9:\n            logger.warning(f\"More than 10% invalid prices in NW-RQK input\")\n        \n        # Calculate NW-RQK\n        logger.info(\"Calculating NW-RQK ensemble...\")\n        nwrqk_values = nwrqk_ensemble(\n            prices,\n            window=config.NWRQK_WINDOW,\n            n_kernels=config.NWRQK_N_KERNELS\n        )\n        \n        # Validate output\n        if np.all(np.isnan(nwrqk_values)) or np.all(nwrqk_values == 0):\n            raise ValueError(\"NW-RQK calculation failed - all values invalid\")\n        \n        # Calculate signals\n        bull_signals, bear_signals, signal_strength = calculate_nwrqk_signals(\n            prices, \n            nwrqk_values,\n            threshold=config.NWRQK_THRESHOLD\n        )\n        \n        # Log statistics\n        logger.info(f\"NW-RQK calculation complete - Bull: {bull_signals.sum()}, Bear: {bear_signals.sum()}\")\n        \n        return nwrqk_values, bull_signals, bear_signals, signal_strength\n        \n    except Exception as e:\n        logger.error(f\"Error in NW-RQK calculation: {str(e)}\")\n        # Return safe defaults\n        n = len(prices)\n        return prices.copy(), np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced MLMI with Volatility-Adaptive KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(fastmath=True, cache=True)\ndef calculate_rsi(prices, period=14):\n    \"\"\"Ultra-fast RSI calculation with validation\"\"\"\n    n = len(prices)\n    rsi = np.zeros(n)\n    \n    # Validate period\n    if period < 2:\n        period = 14\n    \n    if n < period + 1:\n        return rsi\n    \n    # Calculate price changes\n    deltas = np.zeros(n)\n    for i in range(1, n):\n        if prices[i-1] > 0 and not np.isnan(prices[i]) and not np.isnan(prices[i-1]):\n            deltas[i] = prices[i] - prices[i-1]\n    \n    # Initial averages\n    avg_gain = 0.0\n    avg_loss = 0.0\n    valid_deltas = 0\n    \n    for i in range(1, min(period + 1, n)):\n        if not np.isnan(deltas[i]):\n            if deltas[i] > 0:\n                avg_gain += deltas[i]\n            else:\n                avg_loss -= deltas[i]\n            valid_deltas += 1\n    \n    if valid_deltas > 0:\n        avg_gain /= valid_deltas\n        avg_loss /= valid_deltas\n    \n    if avg_loss > 0:\n        rs = avg_gain / avg_loss\n        rsi[period] = 100.0 - (100.0 / (1.0 + rs))\n    else:\n        rsi[period] = 100.0 if avg_gain > 0 else 50.0\n    \n    # Calculate RSI for remaining periods\n    for i in range(period + 1, n):\n        if not np.isnan(deltas[i]):\n            if deltas[i] > 0:\n                avg_gain = (avg_gain * (period - 1) + deltas[i]) / period\n                avg_loss = avg_loss * (period - 1) / period\n            else:\n                avg_gain = avg_gain * (period - 1) / period\n                avg_loss = (avg_loss * (period - 1) - deltas[i]) / period\n            \n            if avg_loss > 0:\n                rs = avg_gain / avg_loss\n                rsi[i] = 100.0 - (100.0 / (1.0 + rs))\n            else:\n                rsi[i] = 100.0 if avg_gain > 0 else 50.0\n        else:\n            rsi[i] = rsi[i-1] if i > 0 else 50.0\n    \n    return rsi\n\n@njit(fastmath=True, cache=True)\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate Euclidean distance between two vectors with NaN handling\"\"\"\n    dist = 0.0\n    valid_dims = 0\n    \n    for i in range(len(x1)):\n        if not np.isnan(x1[i]) and not np.isnan(x2[i]):\n            diff = x1[i] - x2[i]\n            dist += diff * diff\n            valid_dims += 1\n    \n    if valid_dims > 0:\n        return np.sqrt(dist / valid_dims)  # Normalize by valid dimensions\n    else:\n        return np.inf  # No valid dimensions\n\n@njit(fastmath=True, cache=True)\ndef volatility_adaptive_knn(features, labels, query, k_base, volatility, vol_scale=2.0):\n    \"\"\"KNN with volatility-based K adjustment and robustness checks\"\"\"\n    # Validate inputs\n    if k_base < 1:\n        k_base = 5\n    if volatility < 0:\n        volatility = 0\n    if vol_scale < 0:\n        vol_scale = 2.0\n    \n    # Adjust K based on volatility\n    k = max(3, min(k_base, int(k_base * (1 - min(volatility, 0.5) * vol_scale))))\n    \n    n_samples = len(labels)\n    if n_samples < k:\n        # Not enough samples, return neutral\n        return 0.5\n    \n    # Calculate distances\n    distances = np.zeros(n_samples)\n    valid_samples = 0\n    \n    for i in range(n_samples):\n        dist = euclidean_distance(features[i], query)\n        if dist < np.inf:  # Valid distance\n            distances[valid_samples] = dist\n            valid_samples += 1\n    \n    if valid_samples < k:\n        return 0.5  # Not enough valid samples\n    \n    # Get k nearest neighbors from valid samples\n    indices = np.argsort(distances[:valid_samples])[:k]\n    \n    # Weighted voting\n    bull_score = 0.0\n    total_weight = 0.0\n    \n    for i in range(k):\n        idx = indices[i]\n        if distances[idx] > 0:\n            weight = 1.0 / (1.0 + distances[idx])\n        else:\n            weight = 1.0\n        \n        bull_score += labels[idx] * weight\n        total_weight += weight\n    \n    if total_weight > 0:\n        return bull_score / total_weight\n    else:\n        return 0.5\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef calculate_mlmi_enhanced(prices, window=10, k=5, feature_window=3):\n    \"\"\"Enhanced MLMI with volatility adaptation and robust error handling\"\"\"\n    n = len(prices)\n    mlmi_bull = np.zeros(n, dtype=np.bool8)\n    mlmi_bear = np.zeros(n, dtype=np.bool8)\n    confidence = np.zeros(n)\n    \n    # Parameter validation\n    if window < 5:\n        window = 10\n    if k < 3:\n        k = 5\n    if feature_window < 2:\n        feature_window = 3\n    \n    # Calculate RSI\n    rsi = calculate_rsi(prices)\n    \n    # Calculate volatility\n    volatility = np.zeros(n)\n    vol_window = 20\n    \n    for i in range(vol_window, n):\n        returns = np.zeros(vol_window)\n        valid_returns = 0\n        \n        for j in range(vol_window):\n            if i-j-1 >= 0 and prices[i-j-1] > 0 and prices[i-j] > 0:\n                if not np.isnan(prices[i-j]) and not np.isnan(prices[i-j-1]):\n                    returns[valid_returns] = (prices[i-j] - prices[i-j-1]) / prices[i-j-1]\n                    valid_returns += 1\n        \n        if valid_returns > 5:  # Need minimum returns\n            volatility[i] = np.std(returns[:valid_returns])\n        else:\n            volatility[i] = 0.02  # Default volatility\n    \n    # MLMI calculation\n    lookback = max(window * 10, 100)\n    \n    for i in prange(lookback, n):\n        # Prepare historical data\n        start_idx = max(0, i - lookback)\n        historical_size = i - start_idx - feature_window - 1\n        \n        if historical_size < k:\n            continue\n        \n        # Create feature matrix\n        features = np.zeros((historical_size, feature_window))\n        labels = np.zeros(historical_size)\n        valid_samples = 0\n        \n        # Fill features and labels\n        for j in range(historical_size):\n            idx = start_idx + j\n            \n            # Check if we have valid RSI values for the feature window\n            valid_features = True\n            for f in range(feature_window):\n                if np.isnan(rsi[idx + f]) or rsi[idx + f] <= 0 or rsi[idx + f] >= 100:\n                    valid_features = False\n                    break\n                features[valid_samples, f] = rsi[idx + f]\n            \n            if not valid_features:\n                continue\n            \n            # Label based on next period return\n            if idx + feature_window < n and prices[idx + feature_window] > 0 and prices[idx + feature_window - 1] > 0:\n                ret = (prices[idx + feature_window] - prices[idx + feature_window - 1]) / prices[idx + feature_window - 1]\n                if not np.isnan(ret) and abs(ret) < 0.5:  # Cap extreme returns\n                    labels[valid_samples] = 1.0 if ret > 0 else 0.0\n                    valid_samples += 1\n        \n        if valid_samples < k:\n            continue\n        \n        # Current query\n        query = np.zeros(feature_window)\n        valid_query = True\n        \n        for f in range(feature_window):\n            if i - feature_window + f >= 0:\n                query[f] = rsi[i - feature_window + f]\n                if np.isnan(query[f]) or query[f] <= 0 or query[f] >= 100:\n                    valid_query = False\n                    break\n            else:\n                valid_query = False\n                break\n        \n        if not valid_query:\n            continue\n        \n        # Adaptive KNN prediction\n        bull_prob = volatility_adaptive_knn(\n            features[:valid_samples], \n            labels[:valid_samples], \n            query, \n            k, \n            volatility[i], \n            2.0  # vol_scale\n        )\n        \n        confidence[i] = abs(bull_prob - 0.5) * 2  # Convert to confidence score\n        \n        # Generate signals with confidence threshold\n        if bull_prob > 0.65 and confidence[i] > 0.3:\n            mlmi_bull[i] = True\n        elif bull_prob < 0.35 and confidence[i] > 0.3:\n            mlmi_bear[i] = True\n    \n    return mlmi_bull, mlmi_bear, confidence\n\n# Add validation wrapper for MLMI calculation\ndef calculate_mlmi_with_validation(prices, config=None):\n    \"\"\"Calculate MLMI with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input\n        if len(prices) < config.MLMI_LOOKBACK:\n            raise ValueError(f\"Insufficient data for MLMI: need at least {config.MLMI_LOOKBACK} points\")\n        \n        # Check for valid prices\n        valid_prices = prices[~np.isnan(prices) & (prices > 0)]\n        if len(valid_prices) < len(prices) * 0.9:\n            logger.warning(f\"More than 10% invalid prices in MLMI input\")\n        \n        # Calculate MLMI\n        logger.info(\"Calculating MLMI signals...\")\n        mlmi_bull, mlmi_bear, mlmi_confidence = calculate_mlmi_enhanced(\n            prices,\n            window=config.MLMI_WINDOW,\n            k=config.MLMI_K_NEIGHBORS,\n            feature_window=config.MLMI_FEATURE_WINDOW\n        )\n        \n        # Validate output\n        total_signals = mlmi_bull.sum() + mlmi_bear.sum()\n        if total_signals == 0:\n            logger.warning(\"MLMI generated no signals - check parameters\")\n        \n        # Log statistics\n        logger.info(f\"MLMI calculation complete - Bull: {mlmi_bull.sum()}, Bear: {mlmi_bear.sum()}\")\n        \n        return mlmi_bull, mlmi_bear, mlmi_confidence\n        \n    except Exception as e:\n        logger.error(f\"Error in MLMI calculation: {str(e)}\")\n        # Return safe defaults\n        n = len(prices)\n        return np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FVG Detection with Volume Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(parallel=True, fastmath=True, cache=True)\ndef detect_fvg_with_volume(high, low, close, volume, min_gap_pct=0.001, volume_factor=1.2):\n    \"\"\"Detect Fair Value Gaps with volume confirmation and robust validation\"\"\"\n    n = len(high)\n    fvg_bull = np.zeros(n, dtype=np.bool8)\n    fvg_bear = np.zeros(n, dtype=np.bool8)\n    gap_size = np.zeros(n)\n    \n    # Parameter validation\n    if min_gap_pct <= 0:\n        min_gap_pct = 0.001\n    if volume_factor < 1:\n        volume_factor = 1.2\n    \n    # Calculate average volume with validation\n    avg_volume = np.zeros(n)\n    vol_window = 20\n    \n    for i in range(vol_window, n):\n        valid_volumes = 0\n        vol_sum = 0.0\n        \n        for j in range(vol_window):\n            if i-j >= 0 and volume[i-j] > 0 and not np.isnan(volume[i-j]):\n                vol_sum += volume[i-j]\n                valid_volumes += 1\n        \n        if valid_volumes > 5:  # Need minimum valid volumes\n            avg_volume[i] = vol_sum / valid_volumes\n        else:\n            avg_volume[i] = 0  # Will skip this bar\n    \n    for i in prange(2, n):\n        # Skip if no average volume or invalid data\n        if avg_volume[i] <= 0:\n            continue\n        \n        # Validate all required data points\n        if (np.isnan(high[i]) or np.isnan(low[i]) or np.isnan(close[i]) or\n            np.isnan(high[i-2]) or np.isnan(low[i-2]) or np.isnan(close[i-1]) or\n            np.isnan(volume[i])):\n            continue\n        \n        # Ensure positive prices\n        if close[i-1] <= 0 or high[i] <= 0 or low[i] <= 0 or high[i-2] <= 0 or low[i-2] <= 0:\n            continue\n        \n        # Volume confirmation\n        vol_confirmed = volume[i] > avg_volume[i] * volume_factor\n        \n        # Bullish FVG: gap up\n        gap_up = low[i] - high[i-2]\n        if gap_up > 0 and vol_confirmed:\n            gap_pct = gap_up / close[i-1]\n            # Additional validation: gap shouldn't be too large (> 10%)\n            if min_gap_pct < gap_pct < 0.1:\n                fvg_bull[i] = True\n                gap_size[i] = gap_pct\n        \n        # Bearish FVG: gap down\n        gap_down = low[i-2] - high[i]\n        if gap_down > 0 and vol_confirmed:\n            gap_pct = gap_down / close[i-1]\n            # Additional validation: gap shouldn't be too large (> 10%)\n            if min_gap_pct < gap_pct < 0.1:\n                fvg_bear[i] = True\n                gap_size[i] = -gap_pct\n    \n    return fvg_bull, fvg_bear, gap_size\n\n# Add validation wrapper for FVG calculation\ndef calculate_fvg_with_validation(df, config=None):\n    \"\"\"Calculate FVG with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate required columns\n        required_columns = ['high', 'low', 'close', 'volume']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns for FVG: {missing_columns}\")\n        \n        # Validate data length\n        if len(df) < 50:  # Need enough data for volume average\n            raise ValueError(f\"Insufficient data for FVG: need at least 50 points\")\n        \n        # Calculate FVG\n        logger.info(\"Calculating FVG signals...\")\n        fvg_bull, fvg_bear, fvg_size = detect_fvg_with_volume(\n            df['high'].values,\n            df['low'].values,\n            df['close'].values,\n            df['volume'].values,\n            min_gap_pct=config.FVG_MIN_GAP_PCT,\n            volume_factor=config.FVG_VOLUME_FACTOR\n        )\n        \n        # Validate output\n        total_gaps = fvg_bull.sum() + fvg_bear.sum()\n        if total_gaps == 0:\n            logger.warning(\"No FVG detected - this is normal for some market conditions\")\n        \n        # Check for excessive gaps\n        gap_ratio = total_gaps / len(df)\n        if gap_ratio > 0.1:  # More than 10% bars have gaps\n            logger.warning(f\"High number of gaps detected: {gap_ratio:.1%} - consider adjusting parameters\")\n        \n        # Log statistics\n        logger.info(f\"FVG calculation complete - Bull: {fvg_bull.sum()}, Bear: {fvg_bear.sum()}\")\n        \n        return fvg_bull, fvg_bear, fvg_size\n        \n    except Exception as e:\n        logger.error(f\"Error in FVG calculation: {str(e)}\")\n        # Return safe defaults\n        n = len(df)\n        return np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NW-RQK → MLMI → FVG Synergy Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(parallel=True, fastmath=True, cache=True)\ndef detect_nwrqk_mlmi_fvg_synergy(nwrqk_bull, nwrqk_bear, nwrqk_strength,\n                                  mlmi_bull, mlmi_bear, mlmi_confidence,\n                                  fvg_bull, fvg_bear, fvg_size,\n                                  window=30):\n    \"\"\"Detect NW-RQK → MLMI → FVG synergy pattern with robust state management\"\"\"\n    n = len(nwrqk_bull)\n    synergy_bull = np.zeros(n, dtype=np.bool8)\n    synergy_bear = np.zeros(n, dtype=np.bool8)\n    synergy_strength = np.zeros(n)\n    \n    # Parameter validation\n    if window < 10:\n        window = 30\n    \n    # State tracking arrays\n    nwrqk_active_bull = np.zeros(n, dtype=np.bool8)\n    nwrqk_active_bear = np.zeros(n, dtype=np.bool8)\n    mlmi_confirmed_bull = np.zeros(n, dtype=np.bool8)\n    mlmi_confirmed_bear = np.zeros(n, dtype=np.bool8)\n    \n    # State timing for decay\n    nwrqk_bull_time = np.full(n, -1, dtype=np.int64)\n    nwrqk_bear_time = np.full(n, -1, dtype=np.int64)\n    mlmi_bull_time = np.full(n, -1, dtype=np.int64) \n    mlmi_bear_time = np.full(n, -1, dtype=np.int64)\n    \n    # Track last synergy to prevent duplicate signals\n    last_bull_synergy = -window\n    last_bear_synergy = -window\n    \n    for i in prange(1, n):\n        # Carry forward states\n        if i > 0:\n            nwrqk_active_bull[i] = nwrqk_active_bull[i-1]\n            nwrqk_active_bear[i] = nwrqk_active_bear[i-1]\n            mlmi_confirmed_bull[i] = mlmi_confirmed_bull[i-1]\n            mlmi_confirmed_bear[i] = mlmi_confirmed_bear[i-1]\n            nwrqk_bull_time[i] = nwrqk_bull_time[i-1]\n            nwrqk_bear_time[i] = nwrqk_bear_time[i-1]\n            mlmi_bull_time[i] = mlmi_bull_time[i-1]\n            mlmi_bear_time[i] = mlmi_bear_time[i-1]\n        \n        # Step 1: NW-RQK signal activation with strength validation\n        if nwrqk_bull[i] and nwrqk_strength[i] > 0.5 and not np.isnan(nwrqk_strength[i]):\n            nwrqk_active_bull[i] = True\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bull_time[i] = i\n            nwrqk_bear_time[i] = -1\n            mlmi_bear_time[i] = -1\n        elif nwrqk_bear[i] and nwrqk_strength[i] > 0.5 and not np.isnan(nwrqk_strength[i]):\n            nwrqk_active_bear[i] = True\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bear_time[i] = i\n            nwrqk_bull_time[i] = -1\n            mlmi_bull_time[i] = -1\n        \n        # Step 2: MLMI confirmation with confidence validation\n        if nwrqk_active_bull[i] and mlmi_bull[i] and mlmi_confidence[i] > 0.3 and not np.isnan(mlmi_confidence[i]):\n            mlmi_confirmed_bull[i] = True\n            mlmi_bull_time[i] = i\n        elif nwrqk_active_bear[i] and mlmi_bear[i] and mlmi_confidence[i] > 0.3 and not np.isnan(mlmi_confidence[i]):\n            mlmi_confirmed_bear[i] = True\n            mlmi_bear_time[i] = i\n        \n        # Step 3: FVG validation for entry with minimum spacing\n        if mlmi_confirmed_bull[i] and fvg_bull[i] and (i - last_bull_synergy) >= 5:\n            synergy_bull[i] = True\n            last_bull_synergy = i\n            \n            # Calculate synergy strength with validation\n            strength_components = np.zeros(3)\n            \n            # Find recent NW-RQK strength\n            if nwrqk_bull_time[i] >= 0:\n                time_since_nwrqk = i - nwrqk_bull_time[i]\n                if time_since_nwrqk < window:\n                    for j in range(max(0, nwrqk_bull_time[i]), min(i + 1, nwrqk_bull_time[i] + window)):\n                        if nwrqk_bull[j] and not np.isnan(nwrqk_strength[j]):\n                            strength_components[0] = max(strength_components[0], nwrqk_strength[j])\n            \n            # MLMI confidence\n            if not np.isnan(mlmi_confidence[i]):\n                strength_components[1] = mlmi_confidence[i]\n            \n            # FVG size\n            if not np.isnan(fvg_size[i]):\n                strength_components[2] = min(abs(fvg_size[i]) * 100, 1.0)\n            \n            # Calculate weighted strength with validation\n            valid_components = 0\n            total_strength = 0.0\n            for comp in strength_components:\n                if comp > 0 and not np.isnan(comp):\n                    total_strength += comp\n                    valid_components += 1\n            \n            if valid_components > 0:\n                synergy_strength[i] = total_strength / valid_components\n            else:\n                synergy_strength[i] = 0.5  # Default strength\n            \n            # Reset states after signal\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bull_time[i] = -1\n            mlmi_bull_time[i] = -1\n            \n        elif mlmi_confirmed_bear[i] and fvg_bear[i] and (i - last_bear_synergy) >= 5:\n            synergy_bear[i] = True\n            last_bear_synergy = i\n            \n            # Calculate synergy strength with validation\n            strength_components = np.zeros(3)\n            \n            # Find recent NW-RQK strength\n            if nwrqk_bear_time[i] >= 0:\n                time_since_nwrqk = i - nwrqk_bear_time[i]\n                if time_since_nwrqk < window:\n                    for j in range(max(0, nwrqk_bear_time[i]), min(i + 1, nwrqk_bear_time[i] + window)):\n                        if nwrqk_bear[j] and not np.isnan(nwrqk_strength[j]):\n                            strength_components[0] = max(strength_components[0], nwrqk_strength[j])\n            \n            # MLMI confidence\n            if not np.isnan(mlmi_confidence[i]):\n                strength_components[1] = mlmi_confidence[i]\n            \n            # FVG size\n            if not np.isnan(fvg_size[i]):\n                strength_components[2] = min(abs(fvg_size[i]) * 100, 1.0)\n            \n            # Calculate weighted strength with validation\n            valid_components = 0\n            total_strength = 0.0\n            for comp in strength_components:\n                if comp > 0 and not np.isnan(comp):\n                    total_strength += comp\n                    valid_components += 1\n            \n            if valid_components > 0:\n                synergy_strength[i] = total_strength / valid_components\n            else:\n                synergy_strength[i] = 0.5  # Default strength\n            \n            # Reset states after signal\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bear_time[i] = -1\n            mlmi_bear_time[i] = -1\n        \n        # State decay - reset if signals are too old\n        if nwrqk_bull_time[i] >= 0 and i - nwrqk_bull_time[i] > window:\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bull_time[i] = -1\n            mlmi_bull_time[i] = -1\n        \n        if nwrqk_bear_time[i] >= 0 and i - nwrqk_bear_time[i] > window:\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bear_time[i] = -1\n            mlmi_bear_time[i] = -1\n    \n    return synergy_bull, synergy_bear, synergy_strength\n\n# Add validation wrapper for synergy detection\ndef detect_synergy_with_validation(nwrqk_data, mlmi_data, fvg_data, config=None):\n    \"\"\"Detect synergies with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input lengths match\n        n = len(nwrqk_data[0])\n        if len(mlmi_data[0]) != n or len(fvg_data[0]) != n:\n            raise ValueError(\"Input data lengths do not match\")\n        \n        # Detect synergies\n        logger.info(\"Detecting NW-RQK → MLMI → FVG synergies...\")\n        synergy_bull, synergy_bear, synergy_strength = detect_nwrqk_mlmi_fvg_synergy(\n            nwrqk_data[0], nwrqk_data[1], nwrqk_data[2],  # bull, bear, strength\n            mlmi_data[0], mlmi_data[1], mlmi_data[2],     # bull, bear, confidence\n            fvg_data[0], fvg_data[1], fvg_data[2],        # bull, bear, size\n            window=config.SYNERGY_WINDOW\n        )\n        \n        # Validate output\n        total_synergies = synergy_bull.sum() + synergy_bear.sum()\n        if total_synergies == 0:\n            logger.warning(\"No synergies detected - consider adjusting parameters\")\n        \n        # Check synergy rate\n        synergy_rate = total_synergies / n\n        if synergy_rate > 0.1:  # More than 10% bars have synergies\n            logger.warning(f\"High synergy rate: {synergy_rate:.1%} - may indicate overfitting\")\n        \n        # Log statistics\n        logger.info(f\"Synergy detection complete - Bull: {synergy_bull.sum()}, Bear: {synergy_bear.sum()}\")\n        avg_strength = synergy_strength[synergy_strength > 0].mean() if (synergy_strength > 0).any() else 0\n        logger.info(f\"Average synergy strength: {avg_strength:.3f}\")\n        \n        return synergy_bull, synergy_bear, synergy_strength\n        \n    except Exception as e:\n        logger.error(f\"Error in synergy detection: {str(e)}\")\n        # Return safe defaults\n        return np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Strategy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_nwrqk_mlmi_fvg_strategy(df_30m, df_5m):\n    \"\"\"Execute the complete NW-RQK → MLMI → FVG strategy with robust error handling\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY\")\n    logger.info(\"=\"*60)\n    \n    start_time = time.time()\n    \n    try:\n        # Performance tracking\n        performance_metrics = {\n            'start_time': datetime.now(),\n            'errors': [],\n            'warnings': []\n        }\n        \n        # Check if calculations already exist (caching)\n        cache_key = f\"{len(df_30m)}_{df_30m.index[0]}_{df_30m.index[-1]}\"\n        \n        # 1. Calculate NW-RQK signals with validation\n        logger.info(\"\\n1. Calculating NW-RQK signals...\")\n        nwrqk_calc_start = time.time()\n        \n        # Check cache for NW-RQK results\n        if hasattr(df_30m, '_nwrqk_cache') and df_30m._nwrqk_cache.get('key') == cache_key:\n            logger.info(\"   - Using cached NW-RQK results\")\n            nwrqk_data = df_30m._nwrqk_cache['data']\n            nwrqk_values, nwrqk_bull, nwrqk_bear, nwrqk_strength = nwrqk_data\n        else:\n            prices = df_30m['close'].values\n            nwrqk_values, nwrqk_bull, nwrqk_bear, nwrqk_strength = calculate_nwrqk_with_validation(\n                prices, StrategyConfig\n            )\n            # Cache results\n            df_30m._nwrqk_cache = {\n                'key': cache_key,\n                'data': (nwrqk_values, nwrqk_bull, nwrqk_bear, nwrqk_strength)\n            }\n        \n        performance_metrics['nwrqk_time'] = time.time() - nwrqk_calc_start\n        logger.info(f\"   - NW-RQK calculation time: {performance_metrics['nwrqk_time']:.2f}s\")\n        logger.info(f\"   - Bull signals: {nwrqk_bull.sum()}\")\n        logger.info(f\"   - Bear signals: {nwrqk_bear.sum()}\")\n        \n        # 2. Calculate MLMI signals with validation\n        logger.info(\"\\n2. Calculating MLMI signals...\")\n        mlmi_calc_start = time.time()\n        \n        # Check cache for MLMI results\n        if hasattr(df_30m, '_mlmi_cache') and df_30m._mlmi_cache.get('key') == cache_key:\n            logger.info(\"   - Using cached MLMI results\")\n            mlmi_data = df_30m._mlmi_cache['data']\n            mlmi_bull, mlmi_bear, mlmi_confidence = mlmi_data\n        else:\n            prices = df_30m['close'].values\n            mlmi_bull, mlmi_bear, mlmi_confidence = calculate_mlmi_with_validation(\n                prices, StrategyConfig\n            )\n            # Cache results\n            df_30m._mlmi_cache = {\n                'key': cache_key,\n                'data': (mlmi_bull, mlmi_bear, mlmi_confidence)\n            }\n        \n        performance_metrics['mlmi_time'] = time.time() - mlmi_calc_start\n        logger.info(f\"   - MLMI calculation time: {performance_metrics['mlmi_time']:.2f}s\")\n        logger.info(f\"   - Bull signals: {mlmi_bull.sum()}\")\n        logger.info(f\"   - Bear signals: {mlmi_bear.sum()}\")\n        \n        # 3. Calculate FVG on 5-minute data with validation\n        logger.info(\"\\n3. Calculating FVG signals on 5m data...\")\n        fvg_calc_start = time.time()\n        \n        # Check if FVG columns exist to avoid recalculation\n        if 'fvg_bull' not in df_5m.columns:\n            fvg_bull_5m, fvg_bear_5m, fvg_size_5m = calculate_fvg_with_validation(\n                df_5m, StrategyConfig\n            )\n            df_5m['fvg_bull'] = fvg_bull_5m\n            df_5m['fvg_bear'] = fvg_bear_5m\n            df_5m['fvg_size'] = fvg_size_5m\n        \n        performance_metrics['fvg_time'] = time.time() - fvg_calc_start\n        logger.info(f\"   - FVG calculation time: {performance_metrics['fvg_time']:.2f}s\")\n        logger.info(f\"   - Bull FVGs: {df_5m['fvg_bull'].sum()}\")\n        logger.info(f\"   - Bear FVGs: {df_5m['fvg_bear'].sum()}\")\n        \n        # 4. Map 5m FVG to 30m timeframe with validation\n        logger.info(\"\\n4. Mapping FVG signals to 30m timeframe...\")\n        \n        try:\n            # Optimized resampling with caching\n            if hasattr(df_5m, '_fvg_resampled_cache') and df_5m._fvg_resampled_cache.get('key') == cache_key:\n                logger.info(\"   - Using cached FVG resampling\")\n                fvg_aligned = df_5m._fvg_resampled_cache['data']\n            else:\n                # Resample FVG signals\n                fvg_resampled = df_5m[['fvg_bull', 'fvg_bear', 'fvg_size']].resample('30min').agg({\n                    'fvg_bull': 'max',\n                    'fvg_bear': 'max',\n                    'fvg_size': 'mean'\n                })\n                \n                # Align with 30m data\n                # Updated: Use ffill() instead of deprecated reindex(method='ffill')\n                fvg_aligned = fvg_resampled.reindex(df_30m.index)\n                fvg_aligned = fvg_aligned.ffill()\n                fvg_aligned = fvg_aligned.fillna(False)\n                \n                # Cache results\n                df_5m._fvg_resampled_cache = {\n                    'key': cache_key,\n                    'data': fvg_aligned\n                }\n            \n            # Validate alignment\n            if len(fvg_aligned) != len(df_30m):\n                raise ValueError(\"FVG alignment failed - length mismatch\")\n                \n        except Exception as e:\n            logger.error(f\"Error in FVG resampling: {str(e)}\")\n            # Create empty FVG signals as fallback\n            fvg_aligned = pd.DataFrame(index=df_30m.index)\n            fvg_aligned['fvg_bull'] = False\n            fvg_aligned['fvg_bear'] = False\n            fvg_aligned['fvg_size'] = 0\n            performance_metrics['errors'].append(f\"FVG resampling error: {str(e)}\")\n        \n        # 5. Detect synergies with validation\n        logger.info(\"\\n5. Detecting NW-RQK → MLMI → FVG synergies...\")\n        synergy_calc_start = time.time()\n        \n        synergy_bull, synergy_bear, synergy_strength = detect_synergy_with_validation(\n            (nwrqk_bull, nwrqk_bear, nwrqk_strength),\n            (mlmi_bull, mlmi_bear, mlmi_confidence),\n            (fvg_aligned['fvg_bull'].values.astype(np.bool8),\n             fvg_aligned['fvg_bear'].values.astype(np.bool8),\n             fvg_aligned['fvg_size'].fillna(0).values),\n            StrategyConfig\n        )\n        \n        performance_metrics['synergy_time'] = time.time() - synergy_calc_start\n        logger.info(f\"   - Synergy detection time: {performance_metrics['synergy_time']:.2f}s\")\n        logger.info(f\"   - Bull synergies: {synergy_bull.sum()}\")\n        logger.info(f\"   - Bear synergies: {synergy_bear.sum()}\")\n        logger.info(f\"   - Total signals: {synergy_bull.sum() + synergy_bear.sum()}\")\n        \n        # 6. Create signals DataFrame with risk management\n        signals = pd.DataFrame(index=df_30m.index)\n        signals['synergy_bull'] = synergy_bull\n        signals['synergy_bear'] = synergy_bear\n        signals['synergy_strength'] = synergy_strength\n        signals['price'] = df_30m['close']\n        \n        # Add volatility for risk management\n        signals['volatility'] = df_30m['volatility']\n        \n        # Generate position signals with risk filters\n        signals['signal'] = 0\n        \n        # Apply risk filters\n        max_volatility = signals['volatility'].quantile(0.95)\n        min_strength = 0.3\n        \n        # Bull signals with risk filters\n        valid_bull = (signals['synergy_bull'] & \n                     (signals['volatility'] < max_volatility) & \n                     (signals['synergy_strength'] > min_strength))\n        signals.loc[valid_bull, 'signal'] = 1\n        \n        # Bear signals with risk filters  \n        valid_bear = (signals['synergy_bear'] & \n                     (signals['volatility'] < max_volatility) & \n                     (signals['synergy_strength'] > min_strength))\n        signals.loc[valid_bear, 'signal'] = -1\n        \n        # Add signal quality metrics\n        signals['signal_quality'] = signals['synergy_strength'] * (1 - signals['volatility'] / max_volatility)\n        \n        # Performance optimization: add signal metadata for faster backtesting\n        signals['nwrqk_value'] = nwrqk_values\n        signals['mlmi_confidence'] = mlmi_confidence\n        \n        # Performance summary\n        performance_metrics['total_time'] = time.time() - start_time\n        performance_metrics['signals_generated'] = (signals['signal'] != 0).sum()\n        performance_metrics['signals_filtered'] = (synergy_bull.sum() + synergy_bear.sum()) - performance_metrics['signals_generated']\n        \n        logger.info(f\"\\nTotal execution time: {performance_metrics['total_time']:.2f} seconds\")\n        logger.info(f\"Signals after risk filtering: {performance_metrics['signals_generated']}\")\n        logger.info(f\"Signals filtered by risk management: {performance_metrics['signals_filtered']}\")\n        \n        # Performance optimization summary\n        if performance_metrics['total_time'] < 10:\n            logger.info(\"✓ Performance: EXCELLENT (< 10s)\")\n        elif performance_metrics['total_time'] < 30:\n            logger.info(\"✓ Performance: GOOD (< 30s)\")\n        else:\n            logger.warning(\"⚠ Performance: SLOW (> 30s) - Consider parameter optimization\")\n        \n        # Store performance metrics in signals\n        signals.attrs['performance_metrics'] = performance_metrics\n        \n        return signals\n        \n    except Exception as e:\n        logger.error(f\"Critical error in strategy execution: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        \n        # Return empty signals on error\n        signals = pd.DataFrame(index=df_30m.index)\n        signals['signal'] = 0\n        signals['price'] = df_30m['close']\n        signals.attrs['error'] = str(e)\n        return signals\n\n# Import traceback for error handling (check if already imported)\nif 'traceback' not in globals():\n    import traceback\n\n# Check if required variables exist before running\nif 'df_30m' not in globals() or 'df_5m' not in globals():\n    logger.error(\"Required dataframes not found. Please run data loading cell first.\")\n    logger.info(\"Attempting to load data...\")\n    try:\n        df_30m, df_5m = load_data()\n        logger.info(\"Data loaded successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to load data: {str(e)}\")\n        raise RuntimeError(\"Cannot proceed without data. Please check data files and run data loading cell.\")\n\n# Check if required functions exist\nrequired_functions = ['calculate_nwrqk_with_validation', 'calculate_mlmi_with_validation', \n                     'calculate_fvg_with_validation', 'detect_synergy_with_validation']\nmissing_functions = [f for f in required_functions if f not in globals()]\n\nif missing_functions:\n    logger.error(f\"Required functions not found: {missing_functions}\")\n    raise RuntimeError(\"Please run the cells containing indicator calculations before running the strategy.\")\n\n# Run the strategy with error handling\ntry:\n    signals = run_nwrqk_mlmi_fvg_strategy(df_30m, df_5m)\n    logger.info(\"Strategy execution completed successfully\")\nexcept Exception as e:\n    logger.error(f\"Failed to run strategy: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VectorBT Backtesting with Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "source": "# EASY PARAMETER ADJUSTMENT CELL\n# Modify these parameters to see their effect on backtest results\n\n# Trading Parameters - Change these values to see different results!\nCUSTOM_INITIAL_CAPITAL = 100000.0  # Starting capital in dollars\nCUSTOM_POSITION_SIZE = 0.1         # Position size as fraction of capital (0.1 = 10%)\nCUSTOM_STOP_LOSS = 0.02           # Stop loss percentage (0.02 = 2%)\nCUSTOM_TAKE_PROFIT = 0.03         # Take profit percentage (0.03 = 3%)\nCUSTOM_FEES = 0.001               # Trading fees (0.001 = 0.1%)\n\n# Update the configuration for next run\nStrategyConfig.INITIAL_CAPITAL = CUSTOM_INITIAL_CAPITAL\nStrategyConfig.POSITION_SIZE_BASE = CUSTOM_POSITION_SIZE\nStrategyConfig.STOP_LOSS_PCT = CUSTOM_STOP_LOSS\nStrategyConfig.TAKE_PROFIT_PCT = CUSTOM_TAKE_PROFIT\nStrategyConfig.TRADING_FEES = CUSTOM_FEES\n\nlogger.info(\"=\"*60)\nlogger.info(\"PARAMETER UPDATE\")\nlogger.info(\"=\"*60)\nlogger.info(f\"Initial Capital: ${CUSTOM_INITIAL_CAPITAL:,.2f}\")\nlogger.info(f\"Position Size: {CUSTOM_POSITION_SIZE * 100:.1f}%\")\nlogger.info(f\"Stop Loss: {CUSTOM_STOP_LOSS * 100:.1f}%\")\nlogger.info(f\"Take Profit: {CUSTOM_TAKE_PROFIT * 100:.1f}%\")\nlogger.info(f\"Trading Fees: {CUSTOM_FEES * 100:.2f}%\")\nlogger.info(\"\\nParameters updated! Run the backtest cell below to see the effect.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_vectorbt_backtest(signals, initial_capital=None, position_size=None, \n                         sl_pct=None, tp_pct=None, fees=None):\n    \"\"\"Run VectorBT backtest with dynamic position sizing and risk management\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"VECTORBT BACKTEST WITH RISK MANAGEMENT\")\n    logger.info(\"=\"*60)\n    \n    # Use configuration values if not provided\n    if initial_capital is None:\n        initial_capital = StrategyConfig.INITIAL_CAPITAL\n    if position_size is None:\n        position_size = StrategyConfig.POSITION_SIZE_BASE\n    if sl_pct is None:\n        sl_pct = StrategyConfig.STOP_LOSS_PCT\n    if tp_pct is None:\n        tp_pct = StrategyConfig.TAKE_PROFIT_PCT\n    if fees is None:\n        fees = StrategyConfig.TRADING_FEES\n        \n    logger.info(f\"Backtest Parameters:\")\n    logger.info(f\"  Initial Capital: ${initial_capital:,.2f}\")\n    logger.info(f\"  Position Size: {position_size * 100:.1f}%\")\n    logger.info(f\"  Stop Loss: {sl_pct * 100:.1f}%\")\n    logger.info(f\"  Take Profit: {tp_pct * 100:.1f}%\")\n    logger.info(f\"  Trading Fees: {fees * 100:.2f}%\")\n    logger.info(f\"  Slippage: {StrategyConfig.SLIPPAGE * 100:.2f}%\")\n    \n    backtest_start = time.time()\n    \n    try:\n        # Validate signals\n        if signals is None or len(signals) == 0:\n            raise ValueError(\"Invalid signals data\")\n        \n        if 'signal' not in signals.columns or 'price' not in signals.columns:\n            raise ValueError(\"Signals must contain 'signal' and 'price' columns\")\n        \n        # Prepare data\n        price = signals['price']\n        entries = signals['signal'] == 1\n        exits = signals['signal'] == -1\n        \n        # Check if we have any signals\n        if not entries.any() and not exits.any():\n            logger.warning(\"No trading signals generated - check strategy parameters\")\n            return None, None\n        \n        # Dynamic position sizing based on signal strength and volatility\n        if 'synergy_strength' in signals.columns and 'volatility' in signals.columns:\n            # Scale position size by signal strength (0.5 to 1.5x base size)\n            strength_factor = 0.5 + 0.5 * np.minimum(signals['synergy_strength'], 1.0)\n            \n            # Reduce position size in high volatility (0.5 to 1.0x)\n            vol_percentile = signals['volatility'].rolling(252).apply(\n                lambda x: stats.rankdata(x)[-1] / len(x) if len(x) > 0 else 0.5\n            ).fillna(0.5)\n            vol_factor = 1.0 - 0.5 * vol_percentile\n            \n            # Combined position sizing\n            position_sizes = position_size * strength_factor * vol_factor\n            position_sizes = position_sizes.fillna(position_size)\n            \n            # Apply position size limits\n            position_sizes = np.clip(position_sizes, \n                                   position_size * 0.5,  # Min 50% of base\n                                   position_size * 1.5)  # Max 150% of base\n        else:\n            position_sizes = position_size\n        \n        # Performance optimization: Configure VectorBT for speed\n        vbt.settings['numba']['parallel'] = True\n        vbt.settings['numba']['cache'] = True\n        vbt.settings['chunk_len'] = 100000  # Process in chunks for memory efficiency\n        \n        # Run backtest with VectorBT\n        portfolio = vbt.Portfolio.from_signals(\n            price,\n            entries=entries,\n            exits=exits,\n            size=position_sizes,\n            size_type='percent',\n            init_cash=initial_capital,\n            fees=fees,\n            slippage=StrategyConfig.SLIPPAGE,\n            freq='30min',\n            sl_stop=sl_pct,\n            tp_stop=tp_pct,\n            stop_exit_price='close',  # Use close price for stops\n            upon_stop_exit='close_position',  # Close full position on stop\n            raise_reject=False,  # Don't raise on rejected orders\n            log=True,  # Enable logging for debugging\n            cash_sharing=True,  # Share cash between trades for efficiency\n            call_seq='auto'  # Optimize call sequence automatically\n        )\n        \n        # Calculate metrics with error handling\n        try:\n            # Use cached stats calculation for performance\n            stats = portfolio.stats(\n                silence_warnings=True,\n                metrics=[\n                    'Total Return [%]',\n                    'Sharpe Ratio',\n                    'Sortino Ratio',\n                    'Calmar Ratio',\n                    'Max Drawdown [%]',\n                    'Win Rate [%]',\n                    'Total Trades',\n                    'Avg Winning Trade [%]',\n                    'Avg Losing Trade [%]',\n                    'Profit Factor',\n                    'Expectancy [%]'\n                ]\n            )\n            \n            # Additional risk metrics with performance optimization\n            returns = portfolio.returns()\n            \n            # Maximum consecutive losses (optimized)\n            if hasattr(portfolio.trades, 'records_readable'):\n                trades_df = portfolio.trades.records_readable\n                if len(trades_df) > 0:\n                    trade_returns = trades_df['Return [%]'].values\n                    losing_mask = trade_returns < 0\n                    \n                    # Vectorized consecutive losses calculation\n                    max_consecutive_losses = 0\n                    if losing_mask.any():\n                        # Group consecutive True values\n                        changes = np.diff(np.concatenate(([False], losing_mask, [False])).astype(int))\n                        starts = np.where(changes == 1)[0]\n                        ends = np.where(changes == -1)[0]\n                        if len(starts) > 0 and len(ends) > 0:\n                            consecutive_lengths = ends - starts\n                            max_consecutive_losses = consecutive_lengths.max()\n                else:\n                    max_consecutive_losses = 0\n            else:\n                max_consecutive_losses = 0\n            \n            # Risk-adjusted metrics (optimized calculation)\n            downside_returns = returns[returns < 0]\n            if len(downside_returns) > 0:\n                downside_std = downside_returns.std()\n                sortino_ratio = (returns.mean() / downside_std) * np.sqrt(252 * 48) if downside_std > 0 else 0\n            else:\n                sortino_ratio = np.inf\n            \n            # Add custom metrics to stats\n            stats['Max Consecutive Losses'] = max_consecutive_losses\n            stats['Sortino Ratio (Custom)'] = sortino_ratio\n            stats['Average Position Size'] = position_sizes.mean() if isinstance(position_sizes, pd.Series) else position_size\n            \n            # Performance timing\n            stats['Backtest Execution Time'] = time.time() - backtest_start\n            \n        except Exception as e:\n            logger.error(f\"Error calculating portfolio statistics: {str(e)}\")\n            stats = {'Error': str(e)}\n        \n        logger.info(f\"\\nBacktest execution time: {time.time() - backtest_start:.2f} seconds\")\n        \n        # Log key performance metrics\n        if 'Total Return [%]' in stats:\n            logger.info(\"\\nKey Performance Metrics:\")\n            logger.info(f\"Total Return: {stats.get('Total Return [%]', 'N/A'):.2f}%\")\n            logger.info(f\"Sharpe Ratio: {stats.get('Sharpe Ratio', 'N/A'):.2f}\")\n            logger.info(f\"Max Drawdown: {stats.get('Max Drawdown [%]', 'N/A'):.2f}%\")\n            logger.info(f\"Win Rate: {stats.get('Win Rate [%]', 'N/A'):.2f}%\")\n            logger.info(f\"Total Trades: {stats.get('Total Trades', 'N/A')}\")\n            \n            # Calculate annual metrics\n            if price.index[-1] and price.index[0]:\n                n_years = (price.index[-1] - price.index[0]).days / 365.25\n                if n_years > 0 and stats.get('Total Return [%]'):\n                    annual_return = (1 + stats['Total Return [%]'] / 100) ** (1 / n_years) - 1\n                    trades_per_year = stats.get('Total Trades', 0) / n_years\n                    \n                    logger.info(f\"\\nAnnualized Return: {annual_return * 100:.2f}%\")\n                    logger.info(f\"Trades per Year: {trades_per_year:.0f}\")\n            \n            # Risk warnings\n            if stats.get('Max Drawdown [%]', 0) > StrategyConfig.MAX_DRAWDOWN_LIMIT * 100:\n                logger.warning(f\"⚠️  Maximum drawdown exceeds limit: {stats['Max Drawdown [%]']:.2f}% > {StrategyConfig.MAX_DRAWDOWN_LIMIT * 100:.0f}%\")\n            \n            if stats.get('Win Rate [%]', 0) < 40:\n                logger.warning(f\"⚠️  Low win rate: {stats['Win Rate [%]']:.2f}%\")\n            \n            # Performance grade\n            backtest_time = stats.get('Backtest Execution Time', 0)\n            if backtest_time < 5:\n                logger.info(\"✓ Backtest Performance: EXCELLENT (< 5s)\")\n            elif backtest_time < 15:\n                logger.info(\"✓ Backtest Performance: GOOD (< 15s)\")\n            else:\n                logger.warning(\"⚠ Backtest Performance: SLOW (> 15s)\")\n        \n        return portfolio, stats\n        \n    except Exception as e:\n        logger.error(f\"Critical error in backtesting: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        return None, {'Error': str(e)}\n\n# Check if required variables exist\nif 'signals' not in globals():\n    logger.error(\"Signals not found. Please run the strategy cell first.\")\n    raise RuntimeError(\"Cannot run backtest without signals. Please execute the strategy cell first.\")\n\nif 'StrategyConfig' not in globals():\n    logger.error(\"StrategyConfig not found. Please run the configuration cell first.\")\n    raise RuntimeError(\"Cannot run backtest without configuration. Please execute the imports/configuration cell first.\")\n\n# Import required modules if not already imported\nif 'vbt' not in globals():\n    logger.warning(\"VectorBT not imported. Importing now...\")\n    import vectorbt as vbt\n\nif 'stats' not in globals():\n    from scipy import stats\n\nif 'traceback' not in globals():\n    import traceback\n\n# Run backtest with error handling\ntry:\n    portfolio, stats = run_vectorbt_backtest(signals)\n    if portfolio is not None:\n        logger.info(\"Backtest completed successfully\")\n    else:\n        logger.error(\"Backtest failed - check logs for details\")\nexcept Exception as e:\n    logger.error(f\"Failed to run backtest: {str(e)}\")\n    portfolio, stats = None, {'Error': str(e)}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_performance_dashboard(signals, portfolio):\n    \"\"\"Create comprehensive performance dashboard with robust error handling\"\"\"\n    try:\n        # Validate inputs\n        if signals is None or portfolio is None:\n            logger.error(\"Cannot create dashboard - signals or portfolio is None\")\n            return None\n        \n        # Check required columns in signals\n        required_columns = ['synergy_strength', 'signal']\n        missing_columns = [col for col in required_columns if col not in signals.columns]\n        if missing_columns:\n            logger.warning(f\"Missing columns in signals for full dashboard: {missing_columns}\")\n        \n        # Create subplots with error handling\n        try:\n            fig = make_subplots(\n                rows=4, cols=2,\n                subplot_titles=(\n                    'Portfolio Value', 'Monthly Returns',\n                    'Cumulative Returns', 'Drawdown',\n                    'Trade Distribution', 'Signal Strength vs Returns',\n                    'Rolling Sharpe Ratio', 'Win Rate by Month'\n                ),\n                row_heights=[0.25, 0.25, 0.25, 0.25],\n                specs=[\n                    [{\"secondary_y\": False}, {\"type\": \"bar\"}],\n                    [{\"secondary_y\": False}, {\"secondary_y\": False}],\n                    [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n                    [{\"secondary_y\": False}, {\"type\": \"bar\"}]\n                ]\n            )\n        except Exception as e:\n            logger.error(f\"Error creating subplot structure: {str(e)}\")\n            return None\n        \n        # 1. Portfolio Value with error handling\n        try:\n            portfolio_value = portfolio.value()\n            if portfolio_value is not None and len(portfolio_value) > 0:\n                fig.add_trace(\n                    go.Scatter(\n                        x=portfolio_value.index,\n                        y=portfolio_value.values,\n                        name='Portfolio Value',\n                        line=dict(color='cyan', width=2)\n                    ),\n                    row=1, col=1\n                )\n            else:\n                logger.warning(\"Portfolio value data is empty\")\n        except Exception as e:\n            logger.error(f\"Error plotting portfolio value: {str(e)}\")\n        \n        # 2. Monthly Returns with error handling\n        try:\n            returns = portfolio.returns()\n            if returns is not None and len(returns) > 0:\n                monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n                if len(monthly_returns) > 0:\n                    colors = ['green' if r > 0 else 'red' for r in monthly_returns]\n                    fig.add_trace(\n                        go.Bar(\n                            x=monthly_returns.index,\n                            y=monthly_returns.values * 100,\n                            name='Monthly Returns',\n                            marker_color=colors\n                        ),\n                        row=1, col=2\n                    )\n        except Exception as e:\n            logger.error(f\"Error plotting monthly returns: {str(e)}\")\n        \n        # 3. Cumulative Returns with error handling\n        try:\n            if returns is not None and len(returns) > 0:\n                cum_returns = (1 + returns).cumprod() - 1\n                fig.add_trace(\n                    go.Scatter(\n                        x=cum_returns.index,\n                        y=cum_returns.values * 100,\n                        name='Cumulative Returns',\n                        fill='tozeroy',\n                        line=dict(color='lightblue')\n                    ),\n                    row=2, col=1\n                )\n        except Exception as e:\n            logger.error(f\"Error plotting cumulative returns: {str(e)}\")\n        \n        # 4. Drawdown with error handling\n        try:\n            drawdown = portfolio.drawdown()\n            if drawdown is not None and len(drawdown) > 0:\n                fig.add_trace(\n                    go.Scatter(\n                        x=drawdown.index,\n                        y=-drawdown.values * 100,\n                        name='Drawdown',\n                        fill='tozeroy',\n                        line=dict(color='red')\n                    ),\n                    row=2, col=2\n                )\n        except Exception as e:\n            logger.error(f\"Error plotting drawdown: {str(e)}\")\n        \n        # 5. Trade Distribution with error handling\n        try:\n            trades_records = portfolio.trades.records_readable\n            if trades_records is not None and len(trades_records) > 0:\n                trade_returns = trades_records['Return [%]'].values\n                if len(trade_returns) > 0:\n                    fig.add_trace(\n                        go.Histogram(\n                            x=trade_returns,\n                            nbinsx=50,\n                            name='Trade Returns',\n                            marker_color='purple'\n                        ),\n                        row=3, col=1\n                    )\n        except Exception as e:\n            logger.error(f\"Error plotting trade distribution: {str(e)}\")\n        \n        # 6. Signal Strength vs Returns with error handling\n        try:\n            if 'synergy_strength' in signals.columns and trades_records is not None and len(trades_records) > 0:\n                entry_times = pd.to_datetime(trades_records['Entry Timestamp'])\n                signal_strengths = []\n                \n                for entry_time in entry_times:\n                    try:\n                        idx = signals.index.get_indexer([entry_time], method='nearest')[0]\n                        if 0 <= idx < len(signals):\n                            signal_strengths.append(signals.iloc[idx]['synergy_strength'])\n                        else:\n                            signal_strengths.append(0)\n                    except:\n                        signal_strengths.append(0)\n                \n                if len(signal_strengths) > 0 and len(trade_returns) > 0:\n                    fig.add_trace(\n                        go.Scatter(\n                            x=signal_strengths,\n                            y=trade_returns,\n                            mode='markers',\n                            name='Strength vs Return',\n                            marker=dict(\n                                size=5,\n                                color=trade_returns,\n                                colorscale='RdYlGn',\n                                showscale=True\n                            )\n                        ),\n                        row=3, col=2\n                    )\n        except Exception as e:\n            logger.error(f\"Error plotting signal strength vs returns: {str(e)}\")\n        \n        # 7. Rolling Sharpe Ratio with error handling\n        try:\n            rolling_sharpe = portfolio.sharpe_ratio(rolling=252)\n            if rolling_sharpe is not None and len(rolling_sharpe) > 0:\n                # Filter out extreme values\n                rolling_sharpe = rolling_sharpe.clip(-10, 10)\n                fig.add_trace(\n                    go.Scatter(\n                        x=rolling_sharpe.index,\n                        y=rolling_sharpe.values,\n                        name='Rolling Sharpe',\n                        line=dict(color='orange')\n                    ),\n                    row=4, col=1\n                )\n        except Exception as e:\n            logger.error(f\"Error plotting rolling Sharpe ratio: {str(e)}\")\n        \n        # 8. Win Rate by Month with error handling\n        try:\n            if trades_records is not None and len(trades_records) > 0:\n                trades_df = trades_records.copy()\n                trades_df['Month'] = pd.to_datetime(trades_df['Entry Timestamp']).dt.to_period('M')\n                \n                # Group by month and calculate win rate\n                monthly_groups = trades_df.groupby('Month')['Return [%]']\n                monthly_stats = pd.DataFrame({\n                    'Count': monthly_groups.count(),\n                    'Win Rate': monthly_groups.apply(lambda x: (x > 0).sum() / len(x) * 100 if len(x) > 0 else 0)\n                })\n                \n                if len(monthly_stats) > 0:\n                    fig.add_trace(\n                        go.Bar(\n                            x=monthly_stats.index.astype(str),\n                            y=monthly_stats['Win Rate'],\n                            name='Win Rate %',\n                            marker_color='lightgreen'\n                        ),\n                        row=4, col=2\n                    )\n        except Exception as e:\n            logger.error(f\"Error plotting win rate by month: {str(e)}\")\n        \n        # Update layout with error handling\n        try:\n            fig.update_layout(\n                title_text=\"NW-RQK → MLMI → FVG Synergy Performance Dashboard\",\n                showlegend=False,\n                height=1600,\n                template='plotly_dark'\n            )\n            \n            # Update axes with error handling\n            axes_updates = [\n                (fig.update_xaxes, \"Date\", 1, 1),\n                (fig.update_xaxes, \"Date\", 1, 2),\n                (fig.update_xaxes, \"Date\", 2, 1),\n                (fig.update_xaxes, \"Date\", 2, 2),\n                (fig.update_xaxes, \"Return %\", 3, 1),\n                (fig.update_xaxes, \"Signal Strength\", 3, 2),\n                (fig.update_xaxes, \"Date\", 4, 1),\n                (fig.update_xaxes, \"Month\", 4, 2),\n                (fig.update_yaxes, \"Value ($)\", 1, 1),\n                (fig.update_yaxes, \"Return %\", 1, 2),\n                (fig.update_yaxes, \"Return %\", 2, 1),\n                (fig.update_yaxes, \"Drawdown %\", 2, 2),\n                (fig.update_yaxes, \"Frequency\", 3, 1),\n                (fig.update_yaxes, \"Return %\", 3, 2),\n                (fig.update_yaxes, \"Sharpe Ratio\", 4, 1),\n                (fig.update_yaxes, \"Win Rate %\", 4, 2)\n            ]\n            \n            for update_func, title, row, col in axes_updates:\n                try:\n                    update_func(title_text=title, row=row, col=col)\n                except:\n                    pass  # Skip if specific axis update fails\n                    \n        except Exception as e:\n            logger.error(f\"Error updating layout: {str(e)}\")\n        \n        # Show figure with error handling\n        try:\n            fig.show()\n            logger.info(\"Performance dashboard created successfully\")\n        except Exception as e:\n            logger.error(f\"Error displaying figure: {str(e)}\")\n            # Try to save as HTML instead\n            try:\n                import os\n                html_file = os.path.join('/home/QuantNova/AlgoSpace-8/results', f'dashboard_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.html')\n                fig.write_html(html_file)\n                logger.info(f\"Dashboard saved as HTML: {html_file}\")\n            except:\n                pass\n        \n        return fig\n        \n    except Exception as e:\n        logger.error(f\"Critical error creating performance dashboard: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        return None\n\n# Check if required variables exist\nif 'signals' not in globals() or 'portfolio' not in globals():\n    logger.error(\"Signals or portfolio not found. Please run the strategy and backtest cells first.\")\n    logger.info(\"Skipping dashboard creation - required data not available\")\n    dashboard = None\nelse:\n    # Import required modules if not already imported\n    if 'go' not in globals():\n        from plotly import graph_objects as go\n    if 'make_subplots' not in globals():\n        from plotly.subplots import make_subplots\n    if 'traceback' not in globals():\n        import traceback\n    if 'datetime' not in globals():\n        from datetime import datetime\n    \n    # Create dashboard with error handling\n    try:\n        dashboard = create_performance_dashboard(signals, portfolio)\n        if dashboard is None:\n            logger.warning(\"Dashboard creation returned None - check logs for errors\")\n    except Exception as e:\n        logger.error(f\"Failed to create dashboard: {str(e)}\")\n        dashboard = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monte Carlo Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@njit(parallel=True, fastmath=True)\ndef monte_carlo_simulation(returns, n_simulations=1000, n_periods=252):\n    \"\"\"Run Monte Carlo simulation for confidence intervals with validation\"\"\"\n    n_returns = len(returns)\n    \n    # Validate inputs\n    if n_returns < 10:\n        raise ValueError(\"Insufficient returns for Monte Carlo simulation\")\n    \n    if n_simulations < 100:\n        n_simulations = 100\n    \n    if n_periods < 10:\n        n_periods = 252\n    \n    final_values = np.zeros(n_simulations)\n    \n    # Filter out extreme returns to avoid unrealistic simulations\n    valid_returns = returns[~np.isnan(returns)]\n    valid_returns = valid_returns[np.abs(valid_returns) < 0.5]  # Cap at 50% moves\n    \n    if len(valid_returns) < 10:\n        raise ValueError(\"Insufficient valid returns after filtering\")\n    \n    for sim in prange(n_simulations):\n        # Bootstrap sample returns\n        sim_returns = np.zeros(n_periods)\n        for i in range(n_periods):\n            idx = np.random.randint(0, len(valid_returns))\n            sim_returns[i] = valid_returns[idx]\n        \n        # Calculate final value with compound returns\n        cumulative_return = 1.0\n        for ret in sim_returns:\n            cumulative_return *= (1 + ret)\n        \n        final_values[sim] = cumulative_return\n    \n    return final_values\n\ndef run_monte_carlo_analysis(portfolio):\n    \"\"\"Run Monte Carlo analysis for strategy validation with comprehensive checks\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"MONTE CARLO VALIDATION\")\n    logger.info(\"=\"*60)\n    \n    mc_start = time.time()\n    \n    try:\n        # Validate portfolio\n        if portfolio is None:\n            logger.error(\"No portfolio provided for Monte Carlo analysis\")\n            return None, None\n        \n        # Get trade returns with validation\n        try:\n            trades_df = portfolio.trades.records_readable\n            if len(trades_df) == 0:\n                logger.error(\"No trades found in portfolio\")\n                return None, None\n            \n            trade_returns = trades_df['Return [%]'].values / 100\n            \n            # Filter out invalid returns\n            valid_mask = ~np.isnan(trade_returns) & (np.abs(trade_returns) < 5.0)  # Cap at 500% moves\n            trade_returns = trade_returns[valid_mask]\n            \n            if len(trade_returns) < 10:\n                logger.error(f\"Insufficient valid trades for Monte Carlo: {len(trade_returns)}\")\n                return None, None\n            \n        except Exception as e:\n            logger.error(f\"Error extracting trade returns: {str(e)}\")\n            return None, None\n        \n        # Run simulation with error handling\n        try:\n            final_values = monte_carlo_simulation(trade_returns, n_simulations=10000)\n        except Exception as e:\n            logger.error(f\"Error in Monte Carlo simulation: {str(e)}\")\n            # Try with fewer simulations\n            try:\n                logger.warning(\"Retrying with 1000 simulations...\")\n                final_values = monte_carlo_simulation(trade_returns, n_simulations=1000)\n            except:\n                return None, None\n        \n        # Calculate statistics\n        mc_returns = (final_values - 1) * 100\n        percentiles = np.percentile(mc_returns, [5, 25, 50, 75, 95])\n        \n        logger.info(f\"\\nMonte Carlo simulation completed in {time.time() - mc_start:.2f} seconds\")\n        logger.info(f\"Based on {len(trade_returns)} historical trades\")\n        logger.info(\"\\nConfidence Intervals for Annual Returns:\")\n        logger.info(f\"5th percentile:  {percentiles[0]:.2f}%\")\n        logger.info(f\"25th percentile: {percentiles[1]:.2f}%\")\n        logger.info(f\"Median:          {percentiles[2]:.2f}%\")\n        logger.info(f\"75th percentile: {percentiles[3]:.2f}%\")\n        logger.info(f\"95th percentile: {percentiles[4]:.2f}%\")\n        \n        # Risk metrics\n        prob_profit = (mc_returns > 0).mean() * 100\n        prob_loss_10pct = (mc_returns < -10).mean() * 100\n        prob_gain_20pct = (mc_returns > 20).mean() * 100\n        expected_return = mc_returns.mean()\n        return_std = mc_returns.std()\n        \n        logger.info(f\"\\nRisk Metrics:\")\n        logger.info(f\"Probability of Profit: {prob_profit:.1f}%\")\n        logger.info(f\"Probability of >20% Gain: {prob_gain_20pct:.1f}%\")\n        logger.info(f\"Probability of >10% Loss: {prob_loss_10pct:.1f}%\")\n        logger.info(f\"Expected Return: {expected_return:.2f}%\")\n        logger.info(f\"Return Std Dev: {return_std:.2f}%\")\n        \n        # Create visualization with error handling\n        try:\n            fig = go.Figure()\n            \n            # Add histogram\n            fig.add_trace(go.Histogram(\n                x=mc_returns,\n                nbinsx=100,\n                name='Simulated Returns',\n                marker_color='lightblue',\n                opacity=0.7,\n                showlegend=False\n            ))\n            \n            # Add percentile lines\n            colors = ['red', 'orange', 'green', 'orange', 'red']\n            for i, (p, label) in enumerate(zip(percentiles, ['5%', '25%', '50%', '75%', '95%'])):\n                fig.add_vline(\n                    x=p, \n                    line_dash=\"dash\", \n                    line_color=colors[i],\n                    annotation_text=f\"{label}: {p:.1f}%\",\n                    annotation_position=\"top\"\n                )\n            \n            # Add zero line\n            fig.add_vline(x=0, line_dash=\"solid\", line_color=\"black\", line_width=2)\n            \n            fig.update_layout(\n                title=f\"Monte Carlo Simulation Results ({len(final_values):,} simulations)\",\n                xaxis_title=\"Annual Return %\",\n                yaxis_title=\"Frequency\",\n                template='plotly_dark',\n                height=600,\n                showlegend=False\n            )\n            \n            fig.show()\n            \n        except Exception as e:\n            logger.error(f\"Error creating Monte Carlo visualization: {str(e)}\")\n        \n        return mc_returns, percentiles\n        \n    except Exception as e:\n        logger.error(f\"Critical error in Monte Carlo analysis: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        return None, None\n\n# Check if required variables exist\nif 'portfolio' not in globals():\n    logger.error(\"Portfolio not found. Please run the backtest cell first.\")\n    logger.info(\"Skipping Monte Carlo analysis - portfolio required\")\n    mc_returns, percentiles = None, None\nelse:\n    # Import required modules if not already imported\n    if 'go' not in globals():\n        import plotly.graph_objects as go\n    if 'traceback' not in globals():\n        import traceback\n    \n    # Run Monte Carlo validation with error handling\n    try:\n        mc_returns, percentiles = run_monte_carlo_analysis(portfolio)\n        if mc_returns is not None:\n            logger.info(\"Monte Carlo analysis completed successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to run Monte Carlo analysis: {str(e)}\")\n        mc_returns, percentiles = None, None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics and Trade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_comprehensive_report(signals, portfolio, stats):\n    \"\"\"Generate comprehensive performance report\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n    print(\"=\"*60)\n    \n    # Validate inputs\n    if signals is None or portfolio is None or stats is None:\n        print(\"Error: Missing required data for report generation\")\n        print(\"Please ensure strategy, backtest, and portfolio cells have been run successfully\")\n        return None\n    \n    try:\n        # Time period analysis\n        start_date = signals.index[0]\n        end_date = signals.index[-1]\n        n_years = (end_date - start_date).days / 365.25\n        \n        print(f\"\\nBacktest Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n        print(f\"Duration: {n_years:.1f} years\")\n        \n        # Trade analysis\n        trades = portfolio.trades.records_readable\n        total_trades = len(trades)\n        winning_trades = len(trades[trades['Return [%]'] > 0])\n        losing_trades = len(trades[trades['Return [%]'] < 0])\n        \n        print(f\"\\nTrade Statistics:\")\n        print(f\"Total Trades: {total_trades}\")\n        print(f\"Trades per Year: {total_trades / n_years:.0f}\")\n        print(f\"Winning Trades: {winning_trades}\")\n        print(f\"Losing Trades: {losing_trades}\")\n        print(f\"Win Rate: {(winning_trades / total_trades * 100) if total_trades > 0 else 0:.2f}%\")\n        \n        # Return analysis\n        avg_win = trades[trades['Return [%]'] > 0]['Return [%]'].mean() if winning_trades > 0 else 0\n        avg_loss = trades[trades['Return [%]'] < 0]['Return [%]'].mean() if losing_trades > 0 else 0\n        profit_factor = abs(avg_win * winning_trades / (avg_loss * losing_trades)) if losing_trades > 0 else np.inf\n        \n        print(f\"\\nReturn Metrics:\")\n        print(f\"Average Win: {avg_win:.2f}%\")\n        print(f\"Average Loss: {avg_loss:.2f}%\")\n        print(f\"Profit Factor: {profit_factor:.2f}\")\n        print(f\"Expectancy: {trades['Return [%]'].mean():.2f}%\")\n        \n        # Risk metrics\n        print(f\"\\nRisk Metrics:\")\n        print(f\"Maximum Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n        print(f\"Average Drawdown: {portfolio.drawdown().mean() * 100:.2f}%\")\n        print(f\"Calmar Ratio: {stats.get('Calmar Ratio', 'N/A'):.2f}\")\n        print(f\"Sortino Ratio: {stats.get('Sortino Ratio', 'N/A'):.2f}\")\n        \n        # Signal quality analysis\n        bull_signals = signals[signals['synergy_bull']]\n        bear_signals = signals[signals['synergy_bear']]\n        \n        print(f\"\\nSignal Analysis:\")\n        print(f\"Total Bull Signals: {len(bull_signals)}\")\n        print(f\"Total Bear Signals: {len(bear_signals)}\")\n        \n        if 'synergy_strength' in signals.columns:\n            avg_strength = signals['synergy_strength'][signals['synergy_strength'] > 0].mean()\n            if not np.isnan(avg_strength):\n                print(f\"Average Signal Strength: {avg_strength:.3f}\")\n        \n        # Monthly performance\n        monthly_returns = portfolio.returns().resample('M').apply(lambda x: (1 + x).prod() - 1)\n        positive_months = (monthly_returns > 0).sum()\n        total_months = len(monthly_returns)\n        \n        print(f\"\\nMonthly Performance:\")\n        print(f\"Positive Months: {positive_months}/{total_months} ({positive_months/total_months*100:.1f}%)\")\n        print(f\"Best Month: {monthly_returns.max() * 100:.2f}%\")\n        print(f\"Worst Month: {monthly_returns.min() * 100:.2f}%\")\n        print(f\"Average Monthly Return: {monthly_returns.mean() * 100:.2f}%\")\n        \n        return trades\n        \n    except Exception as e:\n        print(f\"\\nError generating report: {str(e)}\")\n        if 'traceback' in globals():\n            print(f\"Traceback: {traceback.format_exc()}\")\n        return None\n\n# Check if required variables exist\nif 'signals' not in globals() or 'portfolio' not in globals() or 'stats' not in globals():\n    logger.error(\"Required variables not found for report generation\")\n    logger.info(\"Please run strategy and backtest cells before generating report\")\n    trades_df = None\nelse:\n    # Generate report\n    trades_df = generate_comprehensive_report(signals, portfolio, stats)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced atomic file operations for production safety\nimport tempfile\nimport shutil\n\ndef atomic_write(file_path, content, mode='w'):\n    \"\"\"Write file atomically to prevent corruption\"\"\"\n    # Create temporary file in same directory (for same filesystem)\n    dir_path = os.path.dirname(file_path)\n    \n    try:\n        # Write to temporary file\n        with tempfile.NamedTemporaryFile(mode=mode, dir=dir_path, delete=False) as temp_file:\n            temp_path = temp_file.name\n            if isinstance(content, str):\n                temp_file.write(content)\n            elif isinstance(content, pd.DataFrame):\n                content.to_csv(temp_file, index=True)\n            else:\n                raise ValueError(f\"Unsupported content type: {type(content)}\")\n        \n        # Atomic rename (replaces existing file if present)\n        shutil.move(temp_path, file_path)\n        logger.info(f\"✓ Atomically wrote: {file_path}\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"Atomic write failed for {file_path}: {str(e)}\")\n        # Clean up temporary file if it exists\n        if 'temp_path' in locals() and os.path.exists(temp_path):\n            try:\n                os.remove(temp_path)\n            except:\n                pass\n        return False\n\n# Save results with comprehensive error handling and atomic operations\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"SAVING RESULTS WITH ATOMIC OPERATIONS\")\nlogger.info(\"=\"*60)\n\n# Create results directory if it doesn't exist\nresults_dir = '/home/QuantNova/AlgoSpace-8/results'\ntry:\n    os.makedirs(results_dir, exist_ok=True)\n    logger.info(f\"Results directory verified: {results_dir}\")\nexcept Exception as e:\n    logger.error(f\"Failed to create results directory: {str(e)}\")\n    results_dir = '.'  # Fallback to current directory\n\n# Create backup directory for existing files\nbackup_dir = os.path.join(results_dir, 'backups')\ntry:\n    os.makedirs(backup_dir, exist_ok=True)\nexcept:\n    backup_dir = None\n\n# Prepare timestamp for unique filenames\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Save signals with atomic write\ntry:\n    if 'signals' in globals() and signals is not None:\n        signals_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_signals_{timestamp}.csv')\n        \n        # Backup existing file if it exists\n        if os.path.exists(signals_file) and backup_dir:\n            backup_file = os.path.join(backup_dir, f'signals_backup_{timestamp}.csv')\n            shutil.copy2(signals_file, backup_file)\n            logger.info(f\"Backed up existing signals to {backup_file}\")\n        \n        # Atomic write\n        if atomic_write(signals_file, signals):\n            logger.info(f\"✓ Signals saved to {signals_file}\")\n        else:\n            logger.error(\"Failed to save signals atomically\")\n    else:\n        logger.warning(\"No signals to save\")\nexcept Exception as e:\n    logger.error(f\"Failed to save signals: {str(e)}\")\n\n# Save trade records with validation and atomic write\nif 'portfolio' in globals() and portfolio is not None:\n    try:\n        trades_df = portfolio.trades.records_readable\n        if len(trades_df) > 0:\n            trades_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_trades_{timestamp}.csv')\n            \n            # Add additional trade analysis columns\n            trades_df['Trade Duration'] = pd.to_datetime(trades_df['Exit Timestamp']) - pd.to_datetime(trades_df['Entry Timestamp'])\n            trades_df['Trade Duration Hours'] = trades_df['Trade Duration'].dt.total_seconds() / 3600\n            trades_df['Win/Loss'] = trades_df['Return [%]'].apply(lambda x: 'Win' if x > 0 else 'Loss')\n            \n            # Atomic write\n            if atomic_write(trades_file, trades_df):\n                logger.info(f\"✓ Trade records saved to {trades_file}\")\n                logger.info(f\"  Total trades: {len(trades_df)}\")\n                logger.info(f\"  Win rate: {(trades_df['Return [%]'] > 0).mean() * 100:.1f}%\")\n                logger.info(f\"  Average trade duration: {trades_df['Trade Duration Hours'].mean():.1f} hours\")\n            else:\n                logger.error(\"Failed to save trades atomically\")\n        else:\n            logger.warning(\"No trades to save\")\n    except Exception as e:\n        logger.error(f\"Failed to save trade records: {str(e)}\")\nelse:\n    logger.warning(\"No portfolio available - skipping trade records\")\n\n# Save performance metrics with comprehensive details\ntry:\n    metrics_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_metrics_{timestamp}.txt')\n    \n    # Build comprehensive metrics report\n    metrics_content = []\n    metrics_content.append(\"=\"*60)\n    metrics_content.append(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY PERFORMANCE REPORT\")\n    metrics_content.append(\"=\"*60 + \"\\n\")\n    \n    # Strategy configuration\n    metrics_content.append(\"STRATEGY CONFIGURATION:\")\n    metrics_content.append(\"-\"*30)\n    metrics_content.append(f\"Run Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    if 'df_30m' in globals() and df_30m is not None and len(df_30m) > 0:\n        metrics_content.append(f\"Data Period: {df_30m.index[0]} to {df_30m.index[-1]}\")\n    metrics_content.append(f\"Initial Capital: ${StrategyConfig.INITIAL_CAPITAL:,.2f}\")\n    metrics_content.append(f\"Position Size: {StrategyConfig.POSITION_SIZE_BASE * 100:.1f}%\")\n    metrics_content.append(f\"Stop Loss: {StrategyConfig.STOP_LOSS_PCT * 100:.1f}%\")\n    metrics_content.append(f\"Take Profit: {StrategyConfig.TAKE_PROFIT_PCT * 100:.1f}%\")\n    metrics_content.append(f\"Trading Fees: {StrategyConfig.TRADING_FEES * 100:.2f}%\")\n    metrics_content.append(f\"Slippage: {StrategyConfig.SLIPPAGE * 100:.2f}%\\n\")\n    \n    # Performance metrics\n    if 'stats' in globals() and stats is not None and 'Error' not in stats:\n        metrics_content.append(\"PERFORMANCE METRICS:\")\n        metrics_content.append(\"-\"*30)\n        for key, value in stats.items():\n            if isinstance(value, (int, float)):\n                if 'Return' in key or 'Ratio' in key or '%' in key:\n                    metrics_content.append(f\"{key}: {value:.2f}\")\n                else:\n                    metrics_content.append(f\"{key}: {value:.4f}\")\n            else:\n                metrics_content.append(f\"{key}: {value}\")\n    else:\n        metrics_content.append(\"Performance metrics unavailable due to errors\")\n    \n    # Signal statistics\n    if 'signals' in globals() and signals is not None:\n        metrics_content.append(\"\\nSIGNAL STATISTICS:\")\n        metrics_content.append(\"-\"*30)\n        metrics_content.append(f\"Total Signals Generated: {(signals['signal'] != 0).sum()}\")\n        metrics_content.append(f\"Bull Signals: {(signals['signal'] == 1).sum()}\")\n        metrics_content.append(f\"Bear Signals: {(signals['signal'] == -1).sum()}\")\n        \n        if 'synergy_strength' in signals.columns:\n            avg_strength = signals.loc[signals['signal'] != 0, 'synergy_strength'].mean()\n            if not np.isnan(avg_strength):\n                metrics_content.append(f\"Average Signal Strength: {avg_strength:.3f}\")\n    \n    # Risk analysis\n    if 'signals' in globals() and signals is not None and hasattr(signals, 'attrs') and 'performance_metrics' in signals.attrs:\n        perf = signals.attrs['performance_metrics']\n        metrics_content.append(\"\\nPERFORMANCE ANALYSIS:\")\n        metrics_content.append(\"-\"*30)\n        metrics_content.append(f\"Total Execution Time: {perf.get('total_time', 0):.2f} seconds\")\n        metrics_content.append(f\"Signals Filtered by Risk: {perf.get('signals_filtered', 0)}\")\n        \n        if perf.get('errors'):\n            metrics_content.append(\"\\nERRORS ENCOUNTERED:\")\n            for error in perf['errors']:\n                metrics_content.append(f\"- {error}\")\n    \n    # Monte Carlo results\n    if 'percentiles' in locals() and percentiles is not None:\n        metrics_content.append(\"\\nMONTE CARLO ANALYSIS:\")\n        metrics_content.append(\"-\"*30)\n        metrics_content.append(f\"5th Percentile Return: {percentiles[0]:.2f}%\")\n        metrics_content.append(f\"Median Return: {percentiles[2]:.2f}%\")\n        metrics_content.append(f\"95th Percentile Return: {percentiles[4]:.2f}%\")\n    \n    # System information\n    metrics_content.append(\"\\nSYSTEM INFORMATION:\")\n    metrics_content.append(\"-\"*30)\n    metrics_content.append(f\"Python Version: {sys.version.split()[0]}\")\n    metrics_content.append(f\"Pandas Version: {pd.__version__}\")\n    metrics_content.append(f\"NumPy Version: {np.__version__}\")\n    metrics_content.append(f\"Numba Version: {nb.__version__}\")\n    if 'vbt' in globals():\n        metrics_content.append(f\"VectorBT Version: {vbt.__version__}\")\n    \n    metrics_content.append(\"\\n\" + \"=\"*60)\n    metrics_content.append(\"END OF REPORT\")\n    metrics_content.append(\"=\"*60)\n    \n    # Join content and write atomically\n    full_content = '\\n'.join(metrics_content) + '\\n'\n    \n    if atomic_write(metrics_file, full_content):\n        logger.info(f\"✓ Performance metrics saved to {metrics_file}\")\n    else:\n        logger.error(\"Failed to save metrics atomically\")\n        \nexcept Exception as e:\n    logger.error(f\"Failed to save performance metrics: {str(e)}\")\n\n# Save configuration for reproducibility with atomic write\ntry:\n    config_file = os.path.join(results_dir, f'synergy_3_config_{timestamp}.json')\n    config_dict = {attr: getattr(StrategyConfig, attr) \n                   for attr in dir(StrategyConfig) \n                   if not attr.startswith('_') and not callable(getattr(StrategyConfig, attr))}\n    \n    # Convert to JSON string\n    config_json = json.dumps(config_dict, indent=2, default=str)\n    \n    if atomic_write(config_file, config_json):\n        logger.info(f\"✓ Configuration saved to {config_file}\")\n    else:\n        logger.error(\"Failed to save configuration atomically\")\n        \nexcept Exception as e:\n    logger.error(f\"Failed to save configuration: {str(e)}\")\n\n# Create summary report with all file paths\ntry:\n    summary_file = os.path.join(results_dir, f'synergy_3_summary_{timestamp}.json')\n    \n    summary = {\n        'timestamp': timestamp,\n        'execution_date': datetime.now().isoformat(),\n        'files_created': [],\n        'performance_summary': {},\n        'errors': []\n    }\n    \n    # Add created files\n    for filename in os.listdir(results_dir):\n        if timestamp in filename:\n            summary['files_created'].append(os.path.join(results_dir, filename))\n    \n    # Add performance summary\n    if 'stats' in globals() and stats is not None and 'Error' not in stats:\n        summary['performance_summary'] = {\n            'total_return': stats.get('Total Return [%]', 'N/A'),\n            'sharpe_ratio': stats.get('Sharpe Ratio', 'N/A'),\n            'max_drawdown': stats.get('Max Drawdown [%]', 'N/A'),\n            'win_rate': stats.get('Win Rate [%]', 'N/A'),\n            'total_trades': stats.get('Total Trades', 0)\n        }\n    \n    # Save summary atomically\n    summary_json = json.dumps(summary, indent=2, default=str)\n    if atomic_write(summary_file, summary_json):\n        logger.info(f\"✓ Summary report saved to {summary_file}\")\n        \nexcept Exception as e:\n    logger.error(f\"Failed to create summary report: {str(e)}\")\n\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY COMPLETE\")\nlogger.info(\"=\"*60)\n\n# Final summary\nlogger.info(\"\\nFINAL SUMMARY:\")\nif 'portfolio' in globals() and portfolio is not None and 'stats' in globals() and stats is not None and 'Total Return [%]' in stats:\n    logger.info(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n    logger.info(f\"Sharpe Ratio: {stats.get('Sharpe Ratio', 'N/A'):.2f}\")\n    logger.info(f\"Max Drawdown: {stats.get('Max Drawdown [%]', 'N/A'):.2f}%\")\n    logger.info(f\"Total Trades: {stats.get('Total Trades', 0)}\")\nelse:\n    logger.info(\"Strategy execution completed with errors - check logs for details\")\n\nlogger.info(f\"\\nAll results saved to: {results_dir}\")\nlogger.info(\"✓ All files written atomically for data integrity\")\nlogger.info(\"✓ Backups created for existing files\")\nlogger.info(\"Notebook execution completed successfully!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}