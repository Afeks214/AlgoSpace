{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 1: MLMI → FVG → NW-RQK Trading Strategy\n",
    "\n",
    "**Ultra-Fast Backtesting with VectorBT and Numba JIT Compilation**\n",
    "\n",
    "This notebook implements the first synergy pattern where:\n",
    "1. MLMI provides the primary trend signal\n",
    "2. FVG confirms entry zones\n",
    "3. NW-RQK validates the final entry\n",
    "\n",
    "Performance targets:\n",
    "- Full backtest execution: < 5 seconds\n",
    "- Parameter optimization: < 30 seconds for 1000 combinations\n",
    "- Zero Python loops in critical paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from numba import njit, prange, typed, types\n",
    "from numba.typed import Dict\n",
    "import warnings\n",
    "import time\n",
    "from typing import Tuple, Dict as TypeDict, Optional\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Numba for maximum performance\n",
    "import numba\n",
    "numba.config.THREADING_LAYER = 'threadsafe'\n",
    "numba.config.NUMBA_NUM_THREADS = numba.config.NUMBA_DEFAULT_NUM_THREADS\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Synergy 1: MLMI → FVG → NW-RQK Strategy\")\n",
    "print(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\n",
    "print(f\"VectorBT version: {vbt.__version__}\")\n",
    "print(\"Environment ready for ultra-fast backtesting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Optimized Data Loading\n",
    "\n",
    "@njit(cache=True)\n",
    "def parse_timestamp_fast(timestamp_str: str) -> float:\n",
    "    \"\"\"Ultra-fast timestamp parsing - returns Unix timestamp\"\"\"\n",
    "    # This is a simplified version - in production you'd use proper parsing\n",
    "    # For now, we'll use pandas for parsing then convert to numeric\n",
    "    return 0.0  # Placeholder\n",
    "\n",
    "def load_data_optimized(file_path: str, timeframe: str = '5m') -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare data with optimizations\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read CSV with optimized settings\n",
    "    df = pd.read_csv(file_path, \n",
    "                     parse_dates=['Timestamp'],\n",
    "                     infer_datetime_format=True,\n",
    "                     date_parser=lambda x: pd.to_datetime(x, dayfirst=True),\n",
    "                     index_col='Timestamp')\n",
    "    \n",
    "    # Ensure numeric types for fast operations\n",
    "    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float64)\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
    "    \n",
    "    # Sort index for faster operations\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"Loaded {len(df):,} rows in {load_time:.2f} seconds\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data files\n",
    "print(\"Loading data files...\")\n",
    "file_5m = \"/home/QuantNova/AlgoSpace-Strategy-1/@NQ - 5 min - ETH.csv\"\n",
    "file_30m = \"/home/QuantNova/AlgoSpace-Strategy-1/NQ - 30 min - ETH.csv\"\n",
    "\n",
    "df_5m = load_data_optimized(file_5m, '5m')\n",
    "df_30m = load_data_optimized(file_30m, '30m')\n",
    "\n",
    "print(f\"\\n5-minute data: {df_5m.index[0]} to {df_5m.index[-1]}\")\n",
    "print(f\"30-minute data: {df_30m.index[0]} to {df_30m.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Ultra-Fast Indicator Calculations\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def wma_vectorized(values: np.ndarray, period: int) -> np.ndarray:\n",
    "    \"\"\"Vectorized Weighted Moving Average\"\"\"\n",
    "    n = len(values)\n",
    "    result = np.full(n, np.nan, dtype=np.float64)\n",
    "    \n",
    "    if period > n:\n",
    "        return result\n",
    "    \n",
    "    # Pre-calculate weights\n",
    "    weights = np.arange(1, period + 1, dtype=np.float64)\n",
    "    sum_weights = np.sum(weights)\n",
    "    \n",
    "    # Vectorized calculation\n",
    "    for i in range(period - 1, n):\n",
    "        window = values[i - period + 1:i + 1]\n",
    "        result[i] = np.dot(window, weights) / sum_weights\n",
    "    \n",
    "    return result\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def rsi_vectorized(prices: np.ndarray, period: int) -> np.ndarray:\n",
    "    \"\"\"Vectorized RSI calculation\"\"\"\n",
    "    n = len(prices)\n",
    "    rsi = np.full(n, 50.0, dtype=np.float64)\n",
    "    \n",
    "    if period >= n:\n",
    "        return rsi\n",
    "    \n",
    "    # Calculate price differences\n",
    "    deltas = np.diff(prices)\n",
    "    gains = np.maximum(deltas, 0)\n",
    "    losses = -np.minimum(deltas, 0)\n",
    "    \n",
    "    # Initial averages\n",
    "    avg_gain = np.mean(gains[:period])\n",
    "    avg_loss = np.mean(losses[:period])\n",
    "    \n",
    "    # Calculate RSI\n",
    "    if avg_loss > 0:\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi[period] = 100 - (100 / (1 + rs))\n",
    "    else:\n",
    "        rsi[period] = 100\n",
    "    \n",
    "    # Wilder's smoothing\n",
    "    for i in range(period, n - 1):\n",
    "        avg_gain = (avg_gain * (period - 1) + gains[i]) / period\n",
    "        avg_loss = (avg_loss * (period - 1) + losses[i]) / period\n",
    "        \n",
    "        if avg_loss > 0:\n",
    "            rs = avg_gain / avg_loss\n",
    "            rsi[i + 1] = 100 - (100 / (1 + rs))\n",
    "        else:\n",
    "            rsi[i + 1] = 100\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def calculate_fvg_parallel(high: np.ndarray, low: np.ndarray, \n",
    "                          lookback: int = 3, validity: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Parallel FVG detection\"\"\"\n",
    "    n = len(high)\n",
    "    bull_active = np.zeros(n, dtype=np.bool_)\n",
    "    bear_active = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    # Parallel detection\n",
    "    for i in prange(lookback, n):\n",
    "        # Bullish FVG\n",
    "        if low[i] > high[i - lookback]:\n",
    "            end_idx = min(i + validity, n)\n",
    "            for j in range(i, end_idx):\n",
    "                if low[j] >= high[i - lookback]:\n",
    "                    bull_active[j] = True\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        # Bearish FVG\n",
    "        if high[i] < low[i - lookback]:\n",
    "            end_idx = min(i + validity, n)\n",
    "            for j in range(i, end_idx):\n",
    "                if high[j] <= low[i - lookback]:\n",
    "                    bear_active[j] = True\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return bull_active, bear_active\n",
    "\n",
    "print(\"Calculating indicators with parallel processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate MLMI components on 30-minute data\n",
    "close_30m = df_30m['Close'].values\n",
    "ma_fast = wma_vectorized(close_30m, 5)\n",
    "ma_slow = wma_vectorized(close_30m, 20)\n",
    "rsi_fast = rsi_vectorized(close_30m, 5)\n",
    "rsi_slow = rsi_vectorized(close_30m, 20)\n",
    "rsi_fast_smooth = wma_vectorized(rsi_fast, 20)\n",
    "rsi_slow_smooth = wma_vectorized(rsi_slow, 20)\n",
    "\n",
    "# Calculate FVG on 5-minute data\n",
    "high_5m = df_5m['High'].values\n",
    "low_5m = df_5m['Low'].values\n",
    "fvg_bull, fvg_bear = calculate_fvg_parallel(high_5m, low_5m)\n",
    "\n",
    "calc_time = time.time() - start_time\n",
    "print(f\"All indicators calculated in {calc_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: MLMI Calculation with KNN\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def knn_predict_fast(features: np.ndarray, labels: np.ndarray, query: np.ndarray, \n",
    "                    k: int, size: int) -> float:\n",
    "    \"\"\"Ultra-fast KNN prediction\"\"\"\n",
    "    if size == 0 or k == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate squared distances (skip sqrt for speed)\n",
    "    distances = np.zeros(size, dtype=np.float64)\n",
    "    for i in range(size):\n",
    "        dist = 0.0\n",
    "        for j in range(2):\n",
    "            diff = features[i, j] - query[j]\n",
    "            dist += diff * diff\n",
    "        distances[i] = dist\n",
    "    \n",
    "    # Find k nearest neighbors using partial sort\n",
    "    k = min(k, size)\n",
    "    indices = np.argpartition(distances, k)[:k]\n",
    "    \n",
    "    # Vote\n",
    "    vote = 0.0\n",
    "    for i in range(k):\n",
    "        vote += labels[indices[i]]\n",
    "    \n",
    "    return vote\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def calculate_mlmi_signals(ma_fast: np.ndarray, ma_slow: np.ndarray,\n",
    "                          rsi_fast_smooth: np.ndarray, rsi_slow_smooth: np.ndarray,\n",
    "                          close: np.ndarray, k_neighbors: int = 200) -> np.ndarray:\n",
    "    \"\"\"Calculate MLMI with vectorized operations\"\"\"\n",
    "    n = len(close)\n",
    "    mlmi_values = np.zeros(n, dtype=np.float64)\n",
    "    \n",
    "    # Pre-allocate KNN storage\n",
    "    max_size = min(10000, n)\n",
    "    features = np.zeros((max_size, 2), dtype=np.float64)\n",
    "    labels = np.zeros(max_size, dtype=np.float64)\n",
    "    data_size = 0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        # Detect crossovers\n",
    "        bull_cross = ma_fast[i] > ma_slow[i] and ma_fast[i-1] <= ma_slow[i-1]\n",
    "        bear_cross = ma_fast[i] < ma_slow[i] and ma_fast[i-1] >= ma_slow[i-1]\n",
    "        \n",
    "        if (bull_cross or bear_cross) and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            # Store pattern\n",
    "            if data_size >= max_size:\n",
    "                # Shift data\n",
    "                shift = max_size // 4\n",
    "                features[:-shift] = features[shift:]\n",
    "                labels[:-shift] = labels[shift:]\n",
    "                data_size = max_size - shift\n",
    "            \n",
    "            features[data_size, 0] = rsi_slow_smooth[i]\n",
    "            features[data_size, 1] = rsi_fast_smooth[i]\n",
    "            if i < n - 1:\n",
    "                labels[data_size] = 1.0 if close[i+1] > close[i] else -1.0\n",
    "            else:\n",
    "                labels[data_size] = 0.0\n",
    "            data_size += 1\n",
    "        \n",
    "        # Make prediction\n",
    "        if data_size > 0 and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            query = np.array([rsi_slow_smooth[i], rsi_fast_smooth[i]], dtype=np.float64)\n",
    "            mlmi_values[i] = knn_predict_fast(features, labels, query, \n",
    "                                            min(k_neighbors, data_size), data_size)\n",
    "    \n",
    "    return mlmi_values\n",
    "\n",
    "# Calculate MLMI\n",
    "print(\"\\nCalculating MLMI signals...\")\n",
    "start_time = time.time()\n",
    "\n",
    "mlmi_values = calculate_mlmi_signals(ma_fast, ma_slow, rsi_fast_smooth, \n",
    "                                    rsi_slow_smooth, close_30m)\n",
    "\n",
    "# Store in dataframe\n",
    "df_30m['mlmi'] = mlmi_values\n",
    "df_30m['mlmi_bull'] = mlmi_values > 0\n",
    "df_30m['mlmi_bear'] = mlmi_values < 0\n",
    "\n",
    "mlmi_time = time.time() - start_time\n",
    "print(f\"MLMI calculated in {mlmi_time:.3f} seconds\")\n",
    "print(f\"MLMI range: [{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: NW-RQK Calculation\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def rational_quadratic_kernel(x: float, h: float, r: float) -> float:\n",
    "    \"\"\"Rational quadratic kernel function\"\"\"\n",
    "    return (1.0 + (x * x) / (h * h * 2.0 * r)) ** (-r)\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def nadaraya_watson_parallel(prices: np.ndarray, h: float, r: float, \n",
    "                           min_periods: int = 25) -> np.ndarray:\n",
    "    \"\"\"Parallel Nadaraya-Watson regression\"\"\"\n",
    "    n = len(prices)\n",
    "    result = np.full(n, np.nan, dtype=np.float64)\n",
    "    \n",
    "    # Parallel processing\n",
    "    for i in prange(min_periods, n):\n",
    "        weighted_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "        \n",
    "        # Limit window for performance\n",
    "        window_size = min(i + 1, 500)\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            if i - j >= 0:\n",
    "                weight = rational_quadratic_kernel(float(j), h, r)\n",
    "                weighted_sum += prices[i - j] * weight\n",
    "                weight_sum += weight\n",
    "        \n",
    "        if weight_sum > 0:\n",
    "            result[i] = weighted_sum / weight_sum\n",
    "    \n",
    "    return result\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def detect_nwrqk_signals(yhat1: np.ndarray, yhat2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Detect NW-RQK trend changes and crossovers\"\"\"\n",
    "    n = len(yhat1)\n",
    "    bull_signals = np.zeros(n, dtype=np.bool_)\n",
    "    bear_signals = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        if not np.isnan(yhat1[i]) and not np.isnan(yhat1[i-1]) and not np.isnan(yhat1[i-2]):\n",
    "            # Trend changes\n",
    "            was_bear = yhat1[i-2] > yhat1[i-1]\n",
    "            was_bull = yhat1[i-2] < yhat1[i-1]\n",
    "            is_bull = yhat1[i-1] < yhat1[i]\n",
    "            is_bear = yhat1[i-1] > yhat1[i]\n",
    "            \n",
    "            if is_bull and was_bear:\n",
    "                bull_signals[i] = True\n",
    "            elif is_bear and was_bull:\n",
    "                bear_signals[i] = True\n",
    "        \n",
    "        # Crossovers\n",
    "        if i > 0 and not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]):\n",
    "            if not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n",
    "                if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1]:\n",
    "                    bull_signals[i] = True\n",
    "                elif yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1]:\n",
    "                    bear_signals[i] = True\n",
    "    \n",
    "    return bull_signals, bear_signals\n",
    "\n",
    "# Calculate NW-RQK\n",
    "print(\"\\nCalculating NW-RQK with parallel processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Parameters\n",
    "h = 8.0\n",
    "r = 8.0\n",
    "lag = 2\n",
    "\n",
    "# Calculate regression lines\n",
    "yhat1 = nadaraya_watson_parallel(close_30m, h, r)\n",
    "yhat2 = nadaraya_watson_parallel(close_30m, h - lag, r)\n",
    "\n",
    "# Detect signals\n",
    "nwrqk_bull, nwrqk_bear = detect_nwrqk_signals(yhat1, yhat2)\n",
    "\n",
    "# Store in dataframe\n",
    "df_30m['nwrqk_bull'] = nwrqk_bull\n",
    "df_30m['nwrqk_bear'] = nwrqk_bear\n",
    "\n",
    "nwrqk_time = time.time() - start_time\n",
    "print(f\"NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n",
    "print(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Timeframe Alignment\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def align_indicators_fast(values_30m: np.ndarray, n_5m: int, ratio: int = 6) -> np.ndarray:\n",
    "    \"\"\"Ultra-fast timeframe alignment using parallel processing\"\"\"\n",
    "    aligned = np.zeros(n_5m, dtype=values_30m.dtype)\n",
    "    \n",
    "    # Parallel alignment\n",
    "    for i in prange(n_5m):\n",
    "        idx_30m = i // ratio\n",
    "        if idx_30m < len(values_30m):\n",
    "            aligned[i] = values_30m[idx_30m]\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "print(\"\\nAligning timeframes with parallel processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Ensure indices are aligned\n",
    "df_5m_aligned = df_5m.copy()\n",
    "\n",
    "# Align 30-minute indicators to 5-minute timeframe\n",
    "n_5m = len(df_5m_aligned)\n",
    "\n",
    "# MLMI alignment\n",
    "mlmi_aligned = df_30m['mlmi'].reindex(df_5m_aligned.index, method='ffill').fillna(0).values\n",
    "mlmi_bull_aligned = df_30m['mlmi_bull'].reindex(df_5m_aligned.index, method='ffill').fillna(False).values\n",
    "mlmi_bear_aligned = df_30m['mlmi_bear'].reindex(df_5m_aligned.index, method='ffill').fillna(False).values\n",
    "\n",
    "# NW-RQK alignment\n",
    "nwrqk_bull_aligned = df_30m['nwrqk_bull'].reindex(df_5m_aligned.index, method='ffill').fillna(False).values\n",
    "nwrqk_bear_aligned = df_30m['nwrqk_bear'].reindex(df_5m_aligned.index, method='ffill').fillna(False).values\n",
    "\n",
    "# Add to dataframe\n",
    "df_5m_aligned['mlmi'] = mlmi_aligned\n",
    "df_5m_aligned['mlmi_bull'] = mlmi_bull_aligned\n",
    "df_5m_aligned['mlmi_bear'] = mlmi_bear_aligned\n",
    "df_5m_aligned['nwrqk_bull'] = nwrqk_bull_aligned\n",
    "df_5m_aligned['nwrqk_bear'] = nwrqk_bear_aligned\n",
    "df_5m_aligned['fvg_bull'] = fvg_bull\n",
    "df_5m_aligned['fvg_bear'] = fvg_bear\n",
    "\n",
    "align_time = time.time() - start_time\n",
    "print(f\"Timeframe alignment completed in {align_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Synergy Signal Detection\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_mlmi_fvg_nwrqk_synergy(mlmi_bull: np.ndarray, mlmi_bear: np.ndarray,\n",
    "                                 fvg_bull: np.ndarray, fvg_bear: np.ndarray,\n",
    "                                 nwrqk_bull: np.ndarray, nwrqk_bear: np.ndarray,\n",
    "                                 window: int = 30) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Detect MLMI → FVG → NW-RQK synergy pattern\"\"\"\n",
    "    n = len(mlmi_bull)\n",
    "    long_signals = np.zeros(n, dtype=np.bool_)\n",
    "    short_signals = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    # State tracking arrays\n",
    "    mlmi_active_bull = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_active_bear = np.zeros(n, dtype=np.bool_)\n",
    "    fvg_confirmed_bull = np.zeros(n, dtype=np.bool_)\n",
    "    fvg_confirmed_bear = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    # Process each bar\n",
    "    for i in range(1, n):\n",
    "        # Carry forward states\n",
    "        if i > 0:\n",
    "            mlmi_active_bull[i] = mlmi_active_bull[i-1]\n",
    "            mlmi_active_bear[i] = mlmi_active_bear[i-1]\n",
    "            fvg_confirmed_bull[i] = fvg_confirmed_bull[i-1]\n",
    "            fvg_confirmed_bear[i] = fvg_confirmed_bear[i-1]\n",
    "        \n",
    "        # Reset on opposite signal\n",
    "        if mlmi_bear[i]:\n",
    "            mlmi_active_bull[i] = False\n",
    "            fvg_confirmed_bull[i] = False\n",
    "        if mlmi_bull[i]:\n",
    "            mlmi_active_bear[i] = False\n",
    "            fvg_confirmed_bear[i] = False\n",
    "        \n",
    "        # Step 1: MLMI signal activation\n",
    "        if mlmi_bull[i] and not mlmi_bull[i-1]:\n",
    "            mlmi_active_bull[i] = True\n",
    "            fvg_confirmed_bull[i] = False\n",
    "        \n",
    "        if mlmi_bear[i] and not mlmi_bear[i-1]:\n",
    "            mlmi_active_bear[i] = True\n",
    "            fvg_confirmed_bear[i] = False\n",
    "        \n",
    "        # Step 2: FVG confirmation\n",
    "        if mlmi_active_bull[i] and not fvg_confirmed_bull[i] and fvg_bull[i]:\n",
    "            fvg_confirmed_bull[i] = True\n",
    "        \n",
    "        if mlmi_active_bear[i] and not fvg_confirmed_bear[i] and fvg_bear[i]:\n",
    "            fvg_confirmed_bear[i] = True\n",
    "        \n",
    "        # Step 3: NW-RQK final confirmation\n",
    "        if fvg_confirmed_bull[i] and nwrqk_bull[i]:\n",
    "            long_signals[i] = True\n",
    "            # Reset states after signal\n",
    "            mlmi_active_bull[i] = False\n",
    "            fvg_confirmed_bull[i] = False\n",
    "        \n",
    "        if fvg_confirmed_bear[i] and nwrqk_bear[i]:\n",
    "            short_signals[i] = True\n",
    "            # Reset states after signal\n",
    "            mlmi_active_bear[i] = False\n",
    "            fvg_confirmed_bear[i] = False\n",
    "        \n",
    "        # Timeout mechanism\n",
    "        if i >= window:\n",
    "            # Check if states have been active too long\n",
    "            if mlmi_active_bull[i] and mlmi_active_bull[i-window]:\n",
    "                mlmi_active_bull[i] = False\n",
    "                fvg_confirmed_bull[i] = False\n",
    "            if mlmi_active_bear[i] and mlmi_active_bear[i-window]:\n",
    "                mlmi_active_bear[i] = False\n",
    "                fvg_confirmed_bear[i] = False\n",
    "    \n",
    "    return long_signals, short_signals\n",
    "\n",
    "print(\"\\nDetecting synergy signals...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract arrays for processing\n",
    "mlmi_bull_arr = df_5m_aligned['mlmi_bull'].values\n",
    "mlmi_bear_arr = df_5m_aligned['mlmi_bear'].values\n",
    "fvg_bull_arr = df_5m_aligned['fvg_bull'].values\n",
    "fvg_bear_arr = df_5m_aligned['fvg_bear'].values\n",
    "nwrqk_bull_arr = df_5m_aligned['nwrqk_bull'].values\n",
    "nwrqk_bear_arr = df_5m_aligned['nwrqk_bear'].values\n",
    "\n",
    "# Detect synergy\n",
    "long_entries, short_entries = detect_mlmi_fvg_nwrqk_synergy(\n",
    "    mlmi_bull_arr, mlmi_bear_arr, fvg_bull_arr, fvg_bear_arr,\n",
    "    nwrqk_bull_arr, nwrqk_bear_arr\n",
    ")\n",
    "\n",
    "# Add to dataframe\n",
    "df_5m_aligned['long_entry'] = long_entries\n",
    "df_5m_aligned['short_entry'] = short_entries\n",
    "\n",
    "signal_time = time.time() - start_time\n",
    "print(f\"Synergy detection completed in {signal_time:.3f} seconds\")\n",
    "print(f\"Long entries: {long_entries.sum():,}\")\n",
    "print(f\"Short entries: {short_entries.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Ultra-Fast VectorBT Backtesting\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ULTRA-FAST VECTORBT BACKTESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data for vectorbt\n",
    "close_prices = df_5m_aligned['Close']\n",
    "entries = df_5m_aligned['long_entry'] | df_5m_aligned['short_entry']\n",
    "direction = np.where(df_5m_aligned['long_entry'], 1, \n",
    "                    np.where(df_5m_aligned['short_entry'], -1, 0))\n",
    "\n",
    "# Simple exit logic - exit on opposite signal or after N bars\n",
    "exits = np.zeros(len(df_5m_aligned), dtype=bool)\n",
    "max_bars = 100  # Maximum bars to hold position\n",
    "\n",
    "print(\"\\nRunning vectorized backtest...\")\n",
    "backtest_start = time.time()\n",
    "\n",
    "# Run backtest with vectorbt\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=close_prices,\n",
    "    entries=entries,\n",
    "    exits=exits,\n",
    "    direction=direction,\n",
    "    size=100,  # Fixed size for simplicity\n",
    "    init_cash=100000,\n",
    "    fees=0.0001,  # 0.01% fees\n",
    "    slippage=0.0001,  # 0.01% slippage\n",
    "    freq='5T'\n",
    ")\n",
    "\n",
    "backtest_time = time.time() - backtest_start\n",
    "print(f\"\\nBacktest completed in {backtest_time:.3f} seconds!\")\n",
    "\n",
    "# Calculate metrics\n",
    "stats = portfolio.stats()\n",
    "returns = portfolio.returns()\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "print(f\"Annualized Return: {stats['Total Return [%]'] * (252*78/len(df_5m_aligned)):.2f}%\")\n",
    "print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "print(f\"Sortino Ratio: {stats['Sortino Ratio']:.2f}\")\n",
    "print(f\"Max Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "print(f\"Total Trades: {stats['Total Trades']:,.0f}\")\n",
    "print(f\"Profit Factor: {stats['Profit Factor']:.2f}\")\n",
    "print(f\"Average Win: {stats['Avg Winning Trade [%]']:.2f}%\")\n",
    "print(f\"Average Loss: {stats['Avg Losing Trade [%]']:.2f}%\")\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TRADE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "trades = portfolio.trades.records_readable\n",
    "if len(trades) > 0:\n",
    "    avg_duration = trades['Duration'].mean()\n",
    "    print(f\"Average Trade Duration: {avg_duration}\")\n",
    "    print(f\"Best Trade: {trades['PnL %'].max():.2f}%\")\n",
    "    print(f\"Worst Trade: {trades['PnL %'].min():.2f}%\")\n",
    "    print(f\"Daily Trades: {len(trades) / (len(df_5m_aligned) / 78):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Professional Visualizations\n",
    "\n",
    "print(\"\\nGenerating professional visualizations...\")\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.4, 0.2, 0.2, 0.2],\n",
    "    subplot_titles=(\n",
    "        'Cumulative Returns',\n",
    "        'Drawdown',\n",
    "        'Trade Distribution',\n",
    "        'Signal Overlay'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Cumulative Returns\n",
    "cumulative_returns = (1 + returns).cumprod() - 1\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cumulative_returns.index,\n",
    "        y=cumulative_returns.values * 100,\n",
    "        mode='lines',\n",
    "        name='Strategy Returns',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Drawdown\n",
    "drawdown = portfolio.drawdown() * 100\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=drawdown.index,\n",
    "        y=-drawdown.values,\n",
    "        mode='lines',\n",
    "        name='Drawdown',\n",
    "        fill='tozeroy',\n",
    "        line=dict(color='red', width=1)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 3. Trade Returns Distribution\n",
    "if len(trades) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=trades['PnL %'],\n",
    "            nbinsx=50,\n",
    "            name='Trade Returns',\n",
    "            marker_color='green'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# 4. Price with Signal Overlay\n",
    "# Sample data for visualization (last 1000 bars)\n",
    "sample_size = min(1000, len(df_5m_aligned))\n",
    "sample_df = df_5m_aligned.tail(sample_size)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Candlestick(\n",
    "        x=sample_df.index,\n",
    "        open=sample_df['Open'],\n",
    "        high=sample_df['High'],\n",
    "        low=sample_df['Low'],\n",
    "        close=sample_df['Close'],\n",
    "        name='Price',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "# Add entry markers\n",
    "long_entries_sample = sample_df[sample_df['long_entry']]\n",
    "short_entries_sample = sample_df[sample_df['short_entry']]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=long_entries_sample.index,\n",
    "        y=long_entries_sample['Low'] * 0.995,\n",
    "        mode='markers',\n",
    "        name='Long Entry',\n",
    "        marker=dict(symbol='triangle-up', size=10, color='green')\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=short_entries_sample.index,\n",
    "        y=short_entries_sample['High'] * 1.005,\n",
    "        mode='markers',\n",
    "        name='Short Entry',\n",
    "        marker=dict(symbol='triangle-down', size=10, color='red')\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='MLMI → FVG → NW-RQK Synergy Strategy Performance',\n",
    "    height=1200,\n",
    "    showlegend=True,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Price\", row=4, col=1)\n",
    "fig.update_xaxes(title_text=\"Date\", row=4, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Monte Carlo Validation\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def monte_carlo_parallel(returns: np.ndarray, n_sims: int = 1000, \n",
    "                        n_periods: int = 252*78) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Parallel Monte Carlo simulation\"\"\"\n",
    "    n_returns = len(returns)\n",
    "    sim_returns = np.zeros(n_sims)\n",
    "    sim_sharpes = np.zeros(n_sims)\n",
    "    sim_max_dd = np.zeros(n_sims)\n",
    "    sim_win_rates = np.zeros(n_sims)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    clean_returns = returns[~np.isnan(returns)]\n",
    "    if len(clean_returns) == 0:\n",
    "        return sim_returns, sim_sharpes, sim_max_dd, sim_win_rates\n",
    "    \n",
    "    for i in prange(n_sims):\n",
    "        # Random sampling with replacement\n",
    "        indices = np.random.randint(0, len(clean_returns), size=len(clean_returns))\n",
    "        sampled = clean_returns[indices]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = np.prod(1 + sampled) - 1\n",
    "        mean_return = np.mean(sampled)\n",
    "        std_return = np.std(sampled)\n",
    "        \n",
    "        sim_returns[i] = total_return\n",
    "        \n",
    "        if std_return > 0:\n",
    "            sim_sharpes[i] = mean_return / std_return * np.sqrt(n_periods)\n",
    "        \n",
    "        # Calculate max drawdown\n",
    "        cum_returns = np.cumprod(1 + sampled)\n",
    "        running_max = np.maximum.accumulate(cum_returns)\n",
    "        drawdown = (cum_returns - running_max) / running_max\n",
    "        sim_max_dd[i] = np.min(drawdown)\n",
    "        \n",
    "        # Win rate\n",
    "        sim_win_rates[i] = np.mean(sampled > 0) * 100\n",
    "    \n",
    "    return sim_returns, sim_sharpes, sim_max_dd, sim_win_rates\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MONTE CARLO VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mc_start = time.time()\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "returns_clean = returns.values[~np.isnan(returns.values)]\n",
    "sim_returns, sim_sharpes, sim_max_dd, sim_win_rates = monte_carlo_parallel(returns_clean, n_sims=10000)\n",
    "\n",
    "mc_time = time.time() - mc_start\n",
    "print(f\"\\nMonte Carlo simulation completed in {mc_time:.3f} seconds\")\n",
    "\n",
    "# Calculate percentiles\n",
    "actual_return = stats['Total Return [%]'] / 100\n",
    "actual_sharpe = stats['Sharpe Ratio']\n",
    "actual_max_dd = stats['Max Drawdown [%]'] / 100\n",
    "actual_win_rate = stats['Win Rate [%]']\n",
    "\n",
    "return_percentile = np.sum(sim_returns <= actual_return) / len(sim_returns) * 100\n",
    "sharpe_percentile = np.sum(sim_sharpes <= actual_sharpe) / len(sim_sharpes) * 100\n",
    "dd_percentile = np.sum(sim_max_dd >= actual_max_dd) / len(sim_max_dd) * 100\n",
    "wr_percentile = np.sum(sim_win_rates <= actual_win_rate) / len(sim_win_rates) * 100\n",
    "\n",
    "print(\"\\nStrategy Performance Percentiles:\")\n",
    "print(f\"Return: {return_percentile:.1f}th percentile\")\n",
    "print(f\"Sharpe: {sharpe_percentile:.1f}th percentile\")\n",
    "print(f\"Max Drawdown: {dd_percentile:.1f}th percentile\")\n",
    "print(f\"Win Rate: {wr_percentile:.1f}th percentile\")\n",
    "\n",
    "# Confidence intervals\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "print(f\"Return: [{np.percentile(sim_returns, 2.5)*100:.2f}%, {np.percentile(sim_returns, 97.5)*100:.2f}%]\")\n",
    "print(f\"Sharpe: [{np.percentile(sim_sharpes, 2.5):.2f}, {np.percentile(sim_sharpes, 97.5):.2f}]\")\n",
    "print(f\"Max DD: [{np.percentile(sim_max_dd, 2.5)*100:.2f}%, {np.percentile(sim_max_dd, 97.5)*100:.2f}%]\")\n",
    "print(f\"Win Rate: [{np.percentile(sim_win_rates, 2.5):.2f}%, {np.percentile(sim_win_rates, 97.5):.2f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Performance Summary and Timing Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Total execution time\n",
    "total_indicators_time = calc_time + mlmi_time + nwrqk_time\n",
    "total_backtest_time = align_time + signal_time + backtest_time\n",
    "total_time = total_indicators_time + total_backtest_time + mc_time\n",
    "\n",
    "print(\"\\nExecution Time Breakdown:\")\n",
    "print(f\"Indicator Calculations: {total_indicators_time:.3f} seconds\")\n",
    "print(f\"  - Basic indicators: {calc_time:.3f}s\")\n",
    "print(f\"  - MLMI with KNN: {mlmi_time:.3f}s\")\n",
    "print(f\"  - NW-RQK regression: {nwrqk_time:.3f}s\")\n",
    "print(f\"\\nBacktesting: {total_backtest_time:.3f} seconds\")\n",
    "print(f\"  - Timeframe alignment: {align_time:.3f}s\")\n",
    "print(f\"  - Synergy detection: {signal_time:.3f}s\")\n",
    "print(f\"  - VectorBT backtest: {backtest_time:.3f}s\")\n",
    "print(f\"\\nMonte Carlo: {mc_time:.3f} seconds\")\n",
    "print(f\"\\nTOTAL TIME: {total_time:.3f} seconds\")\n",
    "\n",
    "# Strategy characteristics\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"STRATEGY CHARACTERISTICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Data Period: {df_5m_aligned.index[0]} to {df_5m_aligned.index[-1]}\")\n",
    "print(f\"Total Bars: {len(df_5m_aligned):,}\")\n",
    "print(f\"Trading Days: {len(df_5m_aligned) / 78:.0f}\")\n",
    "print(f\"Years: {len(df_5m_aligned) / (78 * 252):.1f}\")\n",
    "\n",
    "# Signal analysis\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"SIGNAL ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"MLMI Bull Signals (30m): {df_30m['mlmi_bull'].sum():,}\")\n",
    "print(f\"MLMI Bear Signals (30m): {df_30m['mlmi_bear'].sum():,}\")\n",
    "print(f\"FVG Bull Zones (5m): {fvg_bull.sum():,}\")\n",
    "print(f\"FVG Bear Zones (5m): {fvg_bear.sum():,}\")\n",
    "print(f\"NW-RQK Bull Signals (30m): {df_30m['nwrqk_bull'].sum():,}\")\n",
    "print(f\"NW-RQK Bear Signals (30m): {df_30m['nwrqk_bear'].sum():,}\")\n",
    "print(f\"\\nSynergy Long Entries: {long_entries.sum():,}\")\n",
    "print(f\"Synergy Short Entries: {short_entries.sum():,}\")\n",
    "print(f\"Total Synergy Signals: {long_entries.sum() + short_entries.sum():,}\")\n",
    "\n",
    "# Final assessment\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if stats['Total Trades'] > 0:\n",
    "    if stats['Sharpe Ratio'] > 1.0:\n",
    "        assessment = \"EXCELLENT - Strong risk-adjusted returns\"\n",
    "    elif stats['Sharpe Ratio'] > 0.5:\n",
    "        assessment = \"GOOD - Positive risk-adjusted returns\"\n",
    "    elif stats['Sharpe Ratio'] > 0:\n",
    "        assessment = \"ACCEPTABLE - Positive but low risk-adjusted returns\"\n",
    "    else:\n",
    "        assessment = \"POOR - Negative risk-adjusted returns\"\n",
    "    \n",
    "    print(f\"Performance Rating: {assessment}\")\n",
    "    print(f\"\\nKey Strengths:\")\n",
    "    if stats['Win Rate [%]'] > 50:\n",
    "        print(f\"  - High win rate: {stats['Win Rate [%]']:.1f}%\")\n",
    "    if stats['Profit Factor'] > 1.5:\n",
    "        print(f\"  - Strong profit factor: {stats['Profit Factor']:.2f}\")\n",
    "    if abs(stats['Max Drawdown [%]']) < 20:\n",
    "        print(f\"  - Controlled drawdown: {stats['Max Drawdown [%]']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nAreas for Improvement:\")\n",
    "    if stats['Total Trades'] < 1000:\n",
    "        print(f\"  - Low trade frequency: {stats['Total Trades']} trades\")\n",
    "    if stats['Win Rate [%]'] < 45:\n",
    "        print(f\"  - Low win rate: {stats['Win Rate [%]']:.1f}%\")\n",
    "    if abs(stats['Max Drawdown [%]']) > 30:\n",
    "        print(f\"  - High drawdown: {stats['Max Drawdown [%]']:.1f}%\")\n",
    "else:\n",
    "    print(\"No trades generated - check signal logic\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}