{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRMS Communication LSTM Training\n",
    "\n",
    "This notebook trains the MRMS Communication LSTM layer that adds temporal context and uncertainty estimation to risk management decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from src.agents.mrms.communication import MRMSCommunicationLSTM\n",
    "from src.agents.mrms.losses import MRMSCommunicationLoss  \n",
    "from src.training.datasets.mrms_comm_dataset import (\n",
    "    MRMSCommunicationDataset,\n",
    "    create_synthetic_trade_history,\n",
    "    create_dataloaders\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Historical Trade Data\n",
    "# For development, we'll use synthetic data\n",
    "# In production, load your actual trade history\n",
    "\n",
    "# Check if we have real data\n",
    "data_path = '../data/processed/trade_history.parquet'\n",
    "if os.path.exists(data_path):\n",
    "    trade_history = pd.read_parquet(data_path)\n",
    "    print(\"Loaded real trade history\")\n",
    "else:\n",
    "    # Generate synthetic data for development\n",
    "    print(\"Generating synthetic trade history...\")\n",
    "    trade_history = create_synthetic_trade_history(\n",
    "        n_trades=5000,\n",
    "        win_rate=0.45,\n",
    "        avg_rr=2.0,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Total trades: {len(trade_history)}\")\n",
    "print(f\"Win rate: {trade_history['hit_target'].mean():.2%}\")\n",
    "print(f\"Average RR: {trade_history['tp_distance'].mean() / trade_history['sl_distance'].mean():.2f}\")\n",
    "print(f\"Date range: {trade_history['timestamp'].min()} to {trade_history['timestamp'].max()}\")\n",
    "\n",
    "# Create train/val/test splits\n",
    "n_total = len(trade_history)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_df = trade_history[:n_train]\n",
    "val_df = trade_history[n_train:n_train+n_val]  \n",
    "test_df = trade_history[n_train+n_val:]\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df)} trades\")\n",
    "print(f\"Val: {len(val_df)} trades\")\n",
    "print(f\"Test: {len(test_df)} trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets and Dataloaders\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'risk_vector_dim': 4,\n",
    "    'outcome_dim': 3,\n",
    "    'hidden_dim': 16,\n",
    "    'output_dim': 8,\n",
    "    'memory_size': 20,\n",
    "    'sequence_length': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-4,\n",
    "    'n_epochs': 100,\n",
    "    'weight_risk': 0.3,\n",
    "    'weight_outcome': 0.3,\n",
    "    'weight_uncertainty': 0.2,\n",
    "    'weight_temporal': 0.2\n",
    "}\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    batch_size=config['batch_size'],\n",
    "    sequence_length=config['sequence_length']\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model and Training Components\n",
    "\n",
    "# Initialize model\n",
    "model = MRMSCommunicationLSTM(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = MRMSCommunicationLoss(config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_risk_loss': [], 'val_risk_loss': [],\n",
    "    'train_uncertainty': [], 'val_uncertainty': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "\n",
    "def train_epoch(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    losses_dict = {'risk': 0, 'outcome': 0, 'uncertainty': 0, 'temporal': 0}\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        # Reset hidden state for each batch\n",
    "        model.reset_hidden_state()\n",
    "        \n",
    "        risk_seq = batch['risk_sequence'].to(device)\n",
    "        outcome_seq = batch['outcome_sequence'].to(device)\n",
    "        target_risk = batch['target_risk'].to(device)\n",
    "        target_outcome = batch['target_outcome'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process sequence\n",
    "        prev_mu = None\n",
    "        for t in range(risk_seq.size(1)):\n",
    "            mu, sigma = model(\n",
    "                risk_seq[:, t, :],\n",
    "                outcome_seq[:, t, :],\n",
    "                update_memory=True\n",
    "            )\n",
    "            \n",
    "            if t == risk_seq.size(1) - 1:  # Last timestep\n",
    "                # Calculate loss\n",
    "                losses = loss_fn(mu, sigma, target_risk, target_outcome, prev_mu)\n",
    "                \n",
    "                # Backward pass\n",
    "                losses['total'].backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                total_loss += losses['total'].item()\n",
    "                for k, v in losses.items():\n",
    "                    if k != 'total':\n",
    "                        losses_dict[k] += v.item()\n",
    "            \n",
    "            prev_mu = mu.detach()\n",
    "    \n",
    "    # Average losses\n",
    "    n_batches = len(loader)\n",
    "    avg_losses = {k: v / n_batches for k, v in losses_dict.items()}\n",
    "    avg_losses['total'] = total_loss / n_batches\n",
    "    \n",
    "    return avg_losses\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    losses_dict = {'risk': 0, 'outcome': 0, 'uncertainty': 0, 'temporal': 0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation'):\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            risk_seq = batch['risk_sequence'].to(device)\n",
    "            outcome_seq = batch['outcome_sequence'].to(device)\n",
    "            target_risk = batch['target_risk'].to(device)\n",
    "            target_outcome = batch['target_outcome'].to(device)\n",
    "            \n",
    "            # Process sequence\n",
    "            for t in range(risk_seq.size(1)):\n",
    "                mu, sigma = model(risk_seq[:, t, :], outcome_seq[:, t, :])\n",
    "                \n",
    "                if t == risk_seq.size(1) - 1:\n",
    "                    losses = loss_fn(mu, sigma, target_risk, target_outcome)\n",
    "                    total_loss += losses['total'].item()\n",
    "                    for k, v in losses.items():\n",
    "                        if k != 'total':\n",
    "                            losses_dict[k] += v.item()\n",
    "    \n",
    "    n_batches = len(loader)\n",
    "    avg_losses = {k: v / n_batches for k, v in losses_dict.items()}\n",
    "    avg_losses['total'] = total_loss / n_batches\n",
    "    \n",
    "    return avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 20\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "for epoch in range(config['n_epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config['n_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_losses = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_losses = validate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_losses['total'])\n",
    "    \n",
    "    # Track history\n",
    "    history['train_loss'].append(train_losses['total'])\n",
    "    history['val_loss'].append(val_losses['total'])\n",
    "    history['train_risk_loss'].append(train_losses['risk'])\n",
    "    history['val_risk_loss'].append(val_losses['risk'])\n",
    "    history['train_uncertainty'].append(train_losses['uncertainty'])\n",
    "    history['val_uncertainty'].append(val_losses['uncertainty'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Train Loss: {train_losses['total']:.4f} | Val Loss: {val_losses['total']:.4f}\")\n",
    "    print(f\"Risk Loss: Train {train_losses['risk']:.4f} | Val {val_losses['risk']:.4f}\")\n",
    "    print(f\"Uncertainty: Train {train_losses['uncertainty']:.4f} | Val {val_losses['uncertainty']:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_losses['total'] < best_val_loss:\n",
    "        best_val_loss = val_losses['total']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_losses['total'],\n",
    "            'config': config\n",
    "        }, '../models/mrms_communication_best.pth')\n",
    "        \n",
    "        print(\"✅ Saved best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Visualization\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Total loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val')\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Risk prediction loss\n",
    "axes[0, 1].plot(history['train_risk_loss'], label='Train')\n",
    "axes[0, 1].plot(history['val_risk_loss'], label='Val')\n",
    "axes[0, 1].set_title('Risk Prediction Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Uncertainty calibration\n",
    "axes[1, 0].plot(history['train_uncertainty'], label='Train')\n",
    "axes[1, 0].plot(history['val_uncertainty'], label='Val')\n",
    "axes[1, 0].set_title('Uncertainty Calibration Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Learning rate\n",
    "lrs = [optimizer.param_groups[0]['lr']] * len(history['train_loss'])\n",
    "axes[1, 1].plot(lrs)\n",
    "axes[1, 1].set_title('Learning Rate')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "plt.savefig('../results/mrms_communication_training.png')\n",
    "plt.show()\n",
    "\n",
    "# Test set evaluation\n",
    "test_losses = validate(model, test_loader, loss_fn, device)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Total Loss: {test_losses['total']:.4f}\")\n",
    "print(f\"Risk Prediction: {test_losses['risk']:.4f}\")\n",
    "print(f\"Outcome Prediction: {test_losses['outcome']:.4f}\")\n",
    "print(f\"Uncertainty Calibration: {test_losses['uncertainty']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Production Model\n",
    "\n",
    "# Save final production model\n",
    "production_path = '../models/m_rms_model_comm.pth'\n",
    "torch.save(model.state_dict(), production_path)\n",
    "print(f\"\\n✅ Production model saved to {production_path}\")\n",
    "\n",
    "# Create integration test\n",
    "print(\"\\nIntegration Test:\")\n",
    "model.eval()\n",
    "test_risk = torch.tensor([[2.0, 1.5, 2.5, 0.85]]).to(device)\n",
    "test_outcome = torch.tensor([[0.0, 1.0, 0.03]]).to(device)\n",
    "\n",
    "mu, sigma = model(test_risk, test_outcome)\n",
    "print(f\"Input risk: {test_risk}\")\n",
    "print(f\"Risk embedding: {mu}\")\n",
    "print(f\"Uncertainty: {sigma}\")\n",
    "print(f\"Mean uncertainty: {sigma.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Model Behavior\n",
    "\n",
    "# Test adaptation to losing streak\n",
    "print(\"\\nTesting adaptation to losing streak:\")\n",
    "model.eval()\n",
    "model.reset_hidden_state()\n",
    "\n",
    "uncertainties = []\n",
    "for i in range(10):\n",
    "    # Simulate losses\n",
    "    risk = torch.tensor([[3.0, 1.5, 3.0, 0.9]]).to(device)\n",
    "    outcome = torch.tensor([[1.0, 0.0, -0.03]]).to(device)  # Stop hit\n",
    "    \n",
    "    mu, sigma = model(risk, outcome, update_memory=True)\n",
    "    uncertainties.append(sigma.mean().item())\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        print(f\"Trade {i+1}: Uncertainty = {sigma.mean().item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(uncertainties, marker='o')\n",
    "plt.title('Uncertainty Evolution During Losing Streak')\n",
    "plt.xlabel('Trade Number')\n",
    "plt.ylabel('Mean Uncertainty')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUncertainty increased from {uncertainties[0]:.4f} to {uncertainties[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}