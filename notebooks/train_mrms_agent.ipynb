{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlgoSpace M-RMS Training Notebook\n",
    "\n",
    "This notebook trains the Multi-Agent Risk Management Subsystem (M-RMS) using a divide-and-conquer strategy.\n",
    "\n",
    "## Overview\n",
    "- Environment Setup & Data Loading\n",
    "- Custom Gymnasium Environment Implementation\n",
    "- M-RMS Agent Architecture (3 Sub-Agents + Ensemble)\n",
    "- MAPPO Training with RLlib\n",
    "- Results Analysis & Model Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Notebook Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/AlgoSpace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -r requirements.txt -q\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.annotations import override\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data\n",
    "print(\"Loading historical data...\")\n",
    "historical_data = pd.read_parquet('data/training_data_main.parquet')\n",
    "print(f\"Loaded {len(historical_data)} rows of historical data\")\n",
    "print(f\"Date range: {historical_data.index.min()} to {historical_data.index.max()}\")\n",
    "print(f\"Columns: {list(historical_data.columns)[:10]}...\")  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SynergyEvent:\n",
    "    \"\"\"Represents a detected synergy event in historical data.\"\"\"\n",
    "    timestamp: pd.Timestamp\n",
    "    index: int\n",
    "    direction: str  # 'LONG' or 'SHORT'\n",
    "    strength: float\n",
    "    indicators: Dict[str, float]\n",
    "    regime: str\n",
    "    \n",
    "class SynergyFinder:\n",
    "    \"\"\"Scans historical data to identify all valid synergy events.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, config: Dict[str, Any]):\n",
    "        self.data = data\n",
    "        self.config = config\n",
    "        self.synergy_events: List[SynergyEvent] = []\n",
    "        \n",
    "    def find_synergies(self) -> List[SynergyEvent]:\n",
    "        \"\"\"Identify all synergy events in the historical data.\"\"\"\n",
    "        print(\"Scanning for synergy events...\")\n",
    "        \n",
    "        # Define synergy detection logic based on multiple indicators\n",
    "        for i in range(100, len(self.data) - 100):  # Leave buffer for trade simulation\n",
    "            row = self.data.iloc[i]\n",
    "            \n",
    "            # Example synergy detection logic (customize based on your strategy)\n",
    "            # Check for bullish synergy\n",
    "            if self._check_bullish_synergy(row, i):\n",
    "                event = SynergyEvent(\n",
    "                    timestamp=self.data.index[i],\n",
    "                    index=i,\n",
    "                    direction='LONG',\n",
    "                    strength=self._calculate_synergy_strength(row, 'LONG'),\n",
    "                    indicators=self._extract_indicators(row),\n",
    "                    regime=self._determine_regime(row)\n",
    "                )\n",
    "                self.synergy_events.append(event)\n",
    "                \n",
    "            # Check for bearish synergy\n",
    "            elif self._check_bearish_synergy(row, i):\n",
    "                event = SynergyEvent(\n",
    "                    timestamp=self.data.index[i],\n",
    "                    index=i,\n",
    "                    direction='SHORT',\n",
    "                    strength=self._calculate_synergy_strength(row, 'SHORT'),\n",
    "                    indicators=self._extract_indicators(row),\n",
    "                    regime=self._determine_regime(row)\n",
    "                )\n",
    "                self.synergy_events.append(event)\n",
    "        \n",
    "        print(f\"Found {len(self.synergy_events)} synergy events\")\n",
    "        return self.synergy_events\n",
    "    \n",
    "    def _check_bullish_synergy(self, row: pd.Series, idx: int) -> bool:\n",
    "        \"\"\"Check if current row represents a bullish synergy.\"\"\"\n",
    "        # Example criteria (customize based on your strategy)\n",
    "        conditions = [\n",
    "            row.get('rsi_14', 50) < 30,  # Oversold RSI\n",
    "            row.get('macd_signal', 0) > 0,  # MACD bullish cross\n",
    "            row.get('close', 0) > row.get('sma_200', 0),  # Above long-term MA\n",
    "            row.get('volume', 0) > row.get('volume_sma_20', 0) * 1.5  # Volume spike\n",
    "        ]\n",
    "        return sum(conditions) >= 3\n",
    "    \n",
    "    def _check_bearish_synergy(self, row: pd.Series, idx: int) -> bool:\n",
    "        \"\"\"Check if current row represents a bearish synergy.\"\"\"\n",
    "        conditions = [\n",
    "            row.get('rsi_14', 50) > 70,  # Overbought RSI\n",
    "            row.get('macd_signal', 0) < 0,  # MACD bearish cross\n",
    "            row.get('close', 0) < row.get('sma_200', 0),  # Below long-term MA\n",
    "            row.get('volume', 0) > row.get('volume_sma_20', 0) * 1.5  # Volume spike\n",
    "        ]\n",
    "        return sum(conditions) >= 3\n",
    "    \n",
    "    def _calculate_synergy_strength(self, row: pd.Series, direction: str) -> float:\n",
    "        \"\"\"Calculate the strength of the synergy signal (0-1).\"\"\"\n",
    "        # Implement your synergy strength calculation\n",
    "        strength = 0.5  # Base strength\n",
    "        \n",
    "        if direction == 'LONG':\n",
    "            strength += (30 - row.get('rsi_14', 50)) / 100  # Stronger if more oversold\n",
    "        else:\n",
    "            strength += (row.get('rsi_14', 50) - 70) / 100  # Stronger if more overbought\n",
    "            \n",
    "        return np.clip(strength, 0.0, 1.0)\n",
    "    \n",
    "    def _extract_indicators(self, row: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"Extract relevant indicators for the synergy event.\"\"\"\n",
    "        return {\n",
    "            'rsi': row.get('rsi_14', 0),\n",
    "            'macd': row.get('macd', 0),\n",
    "            'macd_signal': row.get('macd_signal', 0),\n",
    "            'atr': row.get('atr_14', 0),\n",
    "            'volume_ratio': row.get('volume', 0) / row.get('volume_sma_20', 1),\n",
    "            'bb_position': (row.get('close', 0) - row.get('bb_lower', 0)) / \n",
    "                          (row.get('bb_upper', 1) - row.get('bb_lower', 0))\n",
    "        }\n",
    "    \n",
    "    def _determine_regime(self, row: pd.Series) -> str:\n",
    "        \"\"\"Determine market regime at the time of synergy.\"\"\"\n",
    "        # Simple regime detection (customize as needed)\n",
    "        if row.get('atr_14', 0) > row.get('atr_14_sma_50', 0) * 1.5:\n",
    "            return 'HIGH_VOLATILITY'\n",
    "        elif row.get('close', 0) > row.get('sma_50', 0) > row.get('sma_200', 0):\n",
    "            return 'TRENDING_UP'\n",
    "        elif row.get('close', 0) < row.get('sma_50', 0) < row.get('sma_200', 0):\n",
    "            return 'TRENDING_DOWN'\n",
    "        else:\n",
    "            return 'RANGING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find synergy events\n",
    "synergy_config = {\n",
    "    'min_synergy_strength': 0.6,\n",
    "    'lookback_periods': 20\n",
    "}\n",
    "\n",
    "synergy_finder = SynergyFinder(historical_data, synergy_config)\n",
    "synergy_events = synergy_finder.find_synergies()\n",
    "\n",
    "# Analyze synergy distribution\n",
    "print(f\"\\nSynergy Event Distribution:\")\n",
    "print(f\"Total events: {len(synergy_events)}\")\n",
    "print(f\"Long signals: {sum(1 for e in synergy_events if e.direction == 'LONG')}\")\n",
    "print(f\"Short signals: {sum(1 for e in synergy_events if e.direction == 'SHORT')}\")\n",
    "\n",
    "# Show regime distribution\n",
    "regime_counts = {}\n",
    "for event in synergy_events:\n",
    "    regime_counts[event.regime] = regime_counts.get(event.regime, 0) + 1\n",
    "print(f\"\\nRegime distribution: {regime_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Custom Gymnasium Environment for M-RMS Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TradeResult:\n",
    "    \"\"\"Results from simulating a trade.\"\"\"\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    exit_reason: str  # 'STOP_LOSS', 'TAKE_PROFIT', 'TIME_EXIT'\n",
    "    pnl: float\n",
    "    pnl_points: float\n",
    "    duration_bars: int\n",
    "    max_favorable_excursion: float\n",
    "    max_adverse_excursion: float\n",
    "    rule_violations: List[str]\n",
    "\n",
    "\n",
    "class VirtualAccount:\n",
    "    \"\"\"Simulates a trading account with TopStep-like rules.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_balance: float = 50000.0, config: Dict[str, Any] = None):\n",
    "        self.initial_balance = initial_balance\n",
    "        self.balance = initial_balance\n",
    "        self.config = config or self._default_config()\n",
    "        \n",
    "        # Track metrics\n",
    "        self.trades: List[TradeResult] = []\n",
    "        self.daily_pnl: Dict[str, float] = {}\n",
    "        self.peak_balance = initial_balance\n",
    "        self.current_drawdown = 0.0\n",
    "        self.max_drawdown = 0.0\n",
    "        \n",
    "    def _default_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Default TopStep-like rules.\"\"\"\n",
    "        return {\n",
    "            'max_daily_loss': 1000.0,  # $1000\n",
    "            'max_drawdown': 2000.0,    # $2000\n",
    "            'max_position_size': 5,     # contracts\n",
    "            'profit_target': 3000.0,    # $3000\n",
    "            'min_trading_days': 10,\n",
    "            'point_value': 5.0          # MES point value\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset account to initial state.\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.trades.clear()\n",
    "        self.daily_pnl.clear()\n",
    "        self.peak_balance = self.initial_balance\n",
    "        self.current_drawdown = 0.0\n",
    "        self.max_drawdown = 0.0\n",
    "    \n",
    "    def update_from_trade(self, trade_result: TradeResult, trade_date: pd.Timestamp) -> List[str]:\n",
    "        \"\"\"Update account with trade result and check for rule violations.\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        # Update balance\n",
    "        self.balance += trade_result.pnl\n",
    "        self.trades.append(trade_result)\n",
    "        \n",
    "        # Update daily P&L\n",
    "        date_str = trade_date.date().isoformat()\n",
    "        self.daily_pnl[date_str] = self.daily_pnl.get(date_str, 0) + trade_result.pnl\n",
    "        \n",
    "        # Check daily loss limit\n",
    "        if self.daily_pnl[date_str] < -self.config['max_daily_loss']:\n",
    "            violations.append('MAX_DAILY_LOSS_EXCEEDED')\n",
    "        \n",
    "        # Update drawdown\n",
    "        if self.balance > self.peak_balance:\n",
    "            self.peak_balance = self.balance\n",
    "        self.current_drawdown = self.peak_balance - self.balance\n",
    "        self.max_drawdown = max(self.max_drawdown, self.current_drawdown)\n",
    "        \n",
    "        # Check max drawdown\n",
    "        if self.current_drawdown > self.config['max_drawdown']:\n",
    "            violations.append('MAX_DRAWDOWN_EXCEEDED')\n",
    "        \n",
    "        return violations\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate account performance metrics.\"\"\"\n",
    "        if not self.trades:\n",
    "            return {\n",
    "                'total_pnl': 0.0,\n",
    "                'win_rate': 0.0,\n",
    "                'profit_factor': 0.0,\n",
    "                'sortino_ratio': 0.0,\n",
    "                'max_drawdown': 0.0\n",
    "            }\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_pnl = sum(t.pnl for t in self.trades)\n",
    "        winning_trades = [t for t in self.trades if t.pnl > 0]\n",
    "        losing_trades = [t for t in self.trades if t.pnl < 0]\n",
    "        \n",
    "        win_rate = len(winning_trades) / len(self.trades) if self.trades else 0\n",
    "        \n",
    "        gross_profit = sum(t.pnl for t in winning_trades)\n",
    "        gross_loss = abs(sum(t.pnl for t in losing_trades))\n",
    "        profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Sortino ratio (using downside deviation)\n",
    "        returns = [t.pnl / self.initial_balance for t in self.trades]\n",
    "        if len(returns) > 1:\n",
    "            avg_return = np.mean(returns)\n",
    "            downside_returns = [r for r in returns if r < 0]\n",
    "            downside_dev = np.std(downside_returns) if downside_returns else 0.001\n",
    "            sortino_ratio = (avg_return * 252) / (downside_dev * np.sqrt(252)) if downside_dev > 0 else 0\n",
    "        else:\n",
    "            sortino_ratio = 0.0\n",
    "        \n",
    "        return {\n",
    "            'total_pnl': total_pnl,\n",
    "            'win_rate': win_rate,\n",
    "            'profit_factor': profit_factor,\n",
    "            'sortino_ratio': sortino_ratio,\n",
    "            'max_drawdown': self.max_drawdown\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeSimulator:\n",
    "    \"\"\"Simulates trade execution using historical data.\"\"\"\n",
    "    \n",
    "    def __init__(self, historical_data: pd.DataFrame, config: Dict[str, Any]):\n",
    "        self.data = historical_data\n",
    "        self.config = config\n",
    "        \n",
    "    def simulate_trade(self, \n",
    "                      entry_idx: int, \n",
    "                      direction: str,\n",
    "                      stop_loss: float,\n",
    "                      take_profit: float,\n",
    "                      position_size: int) -> TradeResult:\n",
    "        \"\"\"Simulate a trade from entry to exit.\"\"\"\n",
    "        entry_bar = self.data.iloc[entry_idx]\n",
    "        entry_price = entry_bar['close']\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        max_favorable = 0.0\n",
    "        max_adverse = 0.0\n",
    "        \n",
    "        # Simulate trade bar by bar\n",
    "        for i in range(entry_idx + 1, min(entry_idx + 100, len(self.data))):  # Max 100 bars\n",
    "            bar = self.data.iloc[i]\n",
    "            \n",
    "            # Update excursions\n",
    "            if direction == 'LONG':\n",
    "                favorable = bar['high'] - entry_price\n",
    "                adverse = entry_price - bar['low']\n",
    "            else:\n",
    "                favorable = entry_price - bar['low']\n",
    "                adverse = bar['high'] - entry_price\n",
    "            \n",
    "            max_favorable = max(max_favorable, favorable)\n",
    "            max_adverse = max(max_adverse, adverse)\n",
    "            \n",
    "            # Check exit conditions\n",
    "            if direction == 'LONG':\n",
    "                if bar['low'] <= stop_loss:\n",
    "                    exit_price = stop_loss\n",
    "                    exit_reason = 'STOP_LOSS'\n",
    "                    break\n",
    "                elif bar['high'] >= take_profit:\n",
    "                    exit_price = take_profit\n",
    "                    exit_reason = 'TAKE_PROFIT'\n",
    "                    break\n",
    "            else:\n",
    "                if bar['high'] >= stop_loss:\n",
    "                    exit_price = stop_loss\n",
    "                    exit_reason = 'STOP_LOSS'\n",
    "                    break\n",
    "                elif bar['low'] <= take_profit:\n",
    "                    exit_price = take_profit\n",
    "                    exit_reason = 'TAKE_PROFIT'\n",
    "                    break\n",
    "        else:\n",
    "            # Time exit\n",
    "            exit_price = self.data.iloc[i]['close']\n",
    "            exit_reason = 'TIME_EXIT'\n",
    "        \n",
    "        # Calculate P&L\n",
    "        if direction == 'LONG':\n",
    "            pnl_points = exit_price - entry_price\n",
    "        else:\n",
    "            pnl_points = entry_price - exit_price\n",
    "            \n",
    "        pnl = pnl_points * position_size * self.config.get('point_value', 5.0)\n",
    "        \n",
    "        return TradeResult(\n",
    "            entry_price=entry_price,\n",
    "            exit_price=exit_price,\n",
    "            exit_reason=exit_reason,\n",
    "            pnl=pnl,\n",
    "            pnl_points=pnl_points,\n",
    "            duration_bars=i - entry_idx,\n",
    "            max_favorable_excursion=max_favorable,\n",
    "            max_adverse_excursion=max_adverse,\n",
    "            rule_violations=[]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskManagementEnv(gym.Env):\n",
    "    \"\"\"Custom Gymnasium environment for M-RMS training.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 historical_data: pd.DataFrame,\n",
    "                 synergy_events: List[SynergyEvent],\n",
    "                 config: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.historical_data = historical_data\n",
    "        self.synergy_events = synergy_events\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.virtual_account = VirtualAccount(config=config.get('account_config', {}))\n",
    "        self.trade_simulator = TradeSimulator(historical_data, config)\n",
    "        \n",
    "        # Episode tracking\n",
    "        self.current_synergy_idx = 0\n",
    "        self.episode_trades = 0\n",
    "        self.max_trades_per_episode = config.get('max_trades_per_episode', 20)\n",
    "        \n",
    "        # Define action space\n",
    "        self.action_space = spaces.Dict({\n",
    "            'position_size': spaces.Discrete(6),  # 0-5 contracts\n",
    "            'sl_atr_multiplier': spaces.Box(low=0.5, high=3.0, shape=(1,), dtype=np.float32),\n",
    "            'rr_ratio': spaces.Box(low=1.0, high=5.0, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        # Define observation space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'synergy_vector': spaces.Box(low=-np.inf, high=np.inf, shape=(30,), dtype=np.float32),\n",
    "            'account_state_vector': spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Reset environment for new episode.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset account\n",
    "        self.virtual_account.reset()\n",
    "        self.episode_trades = 0\n",
    "        \n",
    "        # Select random starting synergy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_synergy_idx = np.random.randint(0, len(self.synergy_events) - 1)\n",
    "        \n",
    "        # Build initial observation\n",
    "        observation = self._build_observation()\n",
    "        info = {'synergy_event': self.synergy_events[self.current_synergy_idx]}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action: Dict) -> Tuple[Dict, float, bool, bool, Dict]:\n",
    "        \"\"\"Execute one step in the environment.\"\"\"\n",
    "        # Extract action components\n",
    "        position_size = int(action['position_size'])\n",
    "        sl_atr_multiplier = float(action['sl_atr_multiplier'][0])\n",
    "        rr_ratio = float(action['rr_ratio'][0])\n",
    "        \n",
    "        # Get current synergy event\n",
    "        synergy = self.synergy_events[self.current_synergy_idx]\n",
    "        \n",
    "        # Calculate stop loss and take profit\n",
    "        atr = self.historical_data.iloc[synergy.index]['atr_14']\n",
    "        entry_price = self.historical_data.iloc[synergy.index]['close']\n",
    "        \n",
    "        if synergy.direction == 'LONG':\n",
    "            stop_loss = entry_price - (sl_atr_multiplier * atr)\n",
    "            take_profit = entry_price + (sl_atr_multiplier * atr * rr_ratio)\n",
    "        else:\n",
    "            stop_loss = entry_price + (sl_atr_multiplier * atr)\n",
    "            take_profit = entry_price - (sl_atr_multiplier * atr * rr_ratio)\n",
    "        \n",
    "        # Simulate trade if position size > 0\n",
    "        if position_size > 0:\n",
    "            trade_result = self.trade_simulator.simulate_trade(\n",
    "                entry_idx=synergy.index,\n",
    "                direction=synergy.direction,\n",
    "                stop_loss=stop_loss,\n",
    "                take_profit=take_profit,\n",
    "                position_size=position_size\n",
    "            )\n",
    "            \n",
    "            # Update account\n",
    "            violations = self.virtual_account.update_from_trade(\n",
    "                trade_result, \n",
    "                synergy.timestamp\n",
    "            )\n",
    "            trade_result.rule_violations = violations\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self._calculate_reward(trade_result, position_size)\n",
    "            self.episode_trades += 1\n",
    "        else:\n",
    "            # No trade taken\n",
    "            reward = -0.1  # Small penalty for not trading\n",
    "            trade_result = None\n",
    "        \n",
    "        # Find next synergy\n",
    "        self.current_synergy_idx += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.current_synergy_idx >= len(self.synergy_events) - 1 or \n",
    "                self.episode_trades >= self.max_trades_per_episode or\n",
    "                len(violations) > 0)  # End if rules violated\n",
    "        \n",
    "        truncated = False\n",
    "        \n",
    "        # Build next observation\n",
    "        if not done:\n",
    "            observation = self._build_observation()\n",
    "        else:\n",
    "            # Return zero observation if done\n",
    "            observation = {\n",
    "                'synergy_vector': np.zeros(30, dtype=np.float32),\n",
    "                'account_state_vector': np.zeros(10, dtype=np.float32)\n",
    "            }\n",
    "        \n",
    "        # Info dict\n",
    "        info = {\n",
    "            'trade_result': trade_result,\n",
    "            'account_metrics': self.virtual_account.calculate_metrics(),\n",
    "            'violations': violations if position_size > 0 else []\n",
    "        }\n",
    "        \n",
    "        return observation, reward, done, truncated, info\n",
    "    \n",
    "    def _build_observation(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Build observation from current state.\"\"\"\n",
    "        synergy = self.synergy_events[self.current_synergy_idx]\n",
    "        \n",
    "        # Build synergy vector (30 features)\n",
    "        synergy_vector = self._build_synergy_vector(synergy)\n",
    "        \n",
    "        # Build account state vector (10 features)\n",
    "        account_vector = self._build_account_vector()\n",
    "        \n",
    "        return {\n",
    "            'synergy_vector': synergy_vector.astype(np.float32),\n",
    "            'account_state_vector': account_vector.astype(np.float32)\n",
    "        }\n",
    "    \n",
    "    def _build_synergy_vector(self, synergy: SynergyEvent) -> np.ndarray:\n",
    "        \"\"\"Build feature vector from synergy event.\"\"\"\n",
    "        row = self.historical_data.iloc[synergy.index]\n",
    "        \n",
    "        # Extract features (customize based on your needs)\n",
    "        features = [\n",
    "            # Price action features\n",
    "            row['close'] / row['sma_20'] - 1,\n",
    "            row['close'] / row['sma_50'] - 1,\n",
    "            row['close'] / row['sma_200'] - 1,\n",
    "            \n",
    "            # Technical indicators\n",
    "            row['rsi_14'] / 100,\n",
    "            row['macd'] / row['atr_14'],\n",
    "            row['macd_signal'] / row['atr_14'],\n",
    "            \n",
    "            # Volatility\n",
    "            row['atr_14'] / row['close'],\n",
    "            row['bb_width'] / row['close'],\n",
    "            \n",
    "            # Volume\n",
    "            row['volume'] / row['volume_sma_20'],\n",
    "            \n",
    "            # Market structure\n",
    "            synergy.strength,\n",
    "            1.0 if synergy.direction == 'LONG' else -1.0,\n",
    "            \n",
    "            # Regime encoding (one-hot)\n",
    "            1.0 if synergy.regime == 'TRENDING_UP' else 0.0,\n",
    "            1.0 if synergy.regime == 'TRENDING_DOWN' else 0.0,\n",
    "            1.0 if synergy.regime == 'RANGING' else 0.0,\n",
    "            1.0 if synergy.regime == 'HIGH_VOLATILITY' else 0.0,\n",
    "        ]\n",
    "        \n",
    "        # Pad to 30 features\n",
    "        while len(features) < 30:\n",
    "            features.append(0.0)\n",
    "            \n",
    "        return np.array(features[:30])\n",
    "    \n",
    "    def _build_account_vector(self) -> np.ndarray:\n",
    "        \"\"\"Build feature vector from account state.\"\"\"\n",
    "        metrics = self.virtual_account.calculate_metrics()\n",
    "        \n",
    "        features = [\n",
    "            # Account balance\n",
    "            self.virtual_account.balance / self.virtual_account.initial_balance,\n",
    "            \n",
    "            # Drawdown\n",
    "            self.virtual_account.current_drawdown / self.virtual_account.config['max_drawdown'],\n",
    "            self.virtual_account.max_drawdown / self.virtual_account.config['max_drawdown'],\n",
    "            \n",
    "            # Performance metrics\n",
    "            metrics['win_rate'],\n",
    "            np.clip(metrics['profit_factor'], 0, 10) / 10,\n",
    "            np.clip(metrics['sortino_ratio'], -3, 3) / 3,\n",
    "            \n",
    "            # Trading activity\n",
    "            len(self.virtual_account.trades) / 100,\n",
    "            self.episode_trades / self.max_trades_per_episode,\n",
    "            \n",
    "            # Recent performance (last 5 trades)\n",
    "            self._get_recent_performance(5),\n",
    "            self._get_recent_performance(10),\n",
    "        ]\n",
    "        \n",
    "        return np.array(features[:10])\n",
    "    \n",
    "    def _get_recent_performance(self, n: int) -> float:\n",
    "        \"\"\"Get win rate of last n trades.\"\"\"\n",
    "        if len(self.virtual_account.trades) < n:\n",
    "            return 0.5  # Neutral if not enough trades\n",
    "        \n",
    "        recent_trades = self.virtual_account.trades[-n:]\n",
    "        wins = sum(1 for t in recent_trades if t.pnl > 0)\n",
    "        return wins / n\n",
    "    \n",
    "    def _calculate_reward(self, trade_result: Optional[TradeResult], position_size: int) -> float:\n",
    "        \"\"\"Calculate sophisticated reward based on Sortino ratio impact.\"\"\"\n",
    "        if trade_result is None:\n",
    "            return -0.1  # Small penalty for not trading\n",
    "        \n",
    "        # Base reward from P&L (normalized)\n",
    "        pnl_reward = trade_result.pnl / 1000  # Normalize by $1000\n",
    "        \n",
    "        # Calculate Sortino ratio before and after this trade\n",
    "        metrics_before = self._calculate_sortino_excluding_last()\n",
    "        metrics_after = self.virtual_account.calculate_metrics()\n",
    "        \n",
    "        sortino_before = metrics_before.get('sortino_ratio', 0)\n",
    "        sortino_after = metrics_after.get('sortino_ratio', 0)\n",
    "        sortino_impact = (sortino_after - sortino_before) * 2  # Scale impact\n",
    "        \n",
    "        # Risk-adjusted position sizing reward\n",
    "        optimal_size = self._calculate_optimal_position_size(trade_result)\n",
    "        size_penalty = -abs(position_size - optimal_size) * 0.1\n",
    "        \n",
    "        # Combine rewards\n",
    "        reward = pnl_reward + sortino_impact + size_penalty\n",
    "        \n",
    "        # Apply penalties\n",
    "        if trade_result.rule_violations:\n",
    "            reward -= 10.0  # Large penalty for rule violations\n",
    "            \n",
    "        # Penalty for excessive drawdown\n",
    "        if trade_result.max_adverse_excursion > trade_result.max_favorable_excursion * 2:\n",
    "            reward -= 0.5  # Poor risk management\n",
    "            \n",
    "        return float(reward)\n",
    "    \n",
    "    def _calculate_sortino_excluding_last(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate metrics excluding the last trade.\"\"\"\n",
    "        if len(self.virtual_account.trades) <= 1:\n",
    "            return {'sortino_ratio': 0.0}\n",
    "        \n",
    "        # Temporarily remove last trade\n",
    "        last_trade = self.virtual_account.trades.pop()\n",
    "        metrics = self.virtual_account.calculate_metrics()\n",
    "        self.virtual_account.trades.append(last_trade)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_optimal_position_size(self, trade_result: TradeResult) -> int:\n",
    "        \"\"\"Calculate what the optimal position size should have been.\"\"\"\n",
    "        # Simple Kelly-inspired sizing\n",
    "        win_rate = self.virtual_account.calculate_metrics()['win_rate']\n",
    "        if win_rate == 0:\n",
    "            return 1\n",
    "        \n",
    "        avg_win = np.mean([t.pnl for t in self.virtual_account.trades if t.pnl > 0]) if self.virtual_account.trades else 100\n",
    "        avg_loss = abs(np.mean([t.pnl for t in self.virtual_account.trades if t.pnl < 0])) if self.virtual_account.trades else 100\n",
    "        \n",
    "        if avg_loss > 0:\n",
    "            kelly_fraction = (win_rate * avg_win - (1 - win_rate) * avg_loss) / avg_win\n",
    "            optimal_size = int(np.clip(kelly_fraction * 10, 1, 5))  # Scale to 1-5 contracts\n",
    "        else:\n",
    "            optimal_size = 2\n",
    "            \n",
    "        return optimal_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: M-RMS Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionSizingAgent(nn.Module):\n",
    "    \"\"\"Sub-agent responsible for position sizing decisions.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 40, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 6)  # 6 position size options (0-5)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning position size logits.\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class StopLossAgent(nn.Module):\n",
    "    \"\"\"Sub-agent responsible for stop loss placement.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 40, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),  # ATR multiplier\n",
    "            nn.Sigmoid()  # Ensure positive output\n",
    "        )\n",
    "        \n",
    "        # Scale sigmoid output to desired range [0.5, 3.0]\n",
    "        self.min_multiplier = 0.5\n",
    "        self.max_multiplier = 3.0\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning stop loss ATR multiplier.\"\"\"\n",
    "        raw_output = self.network(state)\n",
    "        # Scale to desired range\n",
    "        scaled_output = self.min_multiplier + (self.max_multiplier - self.min_multiplier) * raw_output\n",
    "        return scaled_output\n",
    "\n",
    "\n",
    "class ProfitTargetAgent(nn.Module):\n",
    "    \"\"\"Sub-agent responsible for profit target placement.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 40, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),  # Risk-reward ratio\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Scale sigmoid output to desired range [1.0, 5.0]\n",
    "        self.min_rr = 1.0\n",
    "        self.max_rr = 5.0\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning risk-reward ratio.\"\"\"\n",
    "        raw_output = self.network(state)\n",
    "        # Scale to desired range\n",
    "        scaled_output = self.min_rr + (self.max_rr - self.min_rr) * raw_output\n",
    "        return scaled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskManagementEnsemble(TorchModelV2, nn.Module):\n",
    "    \"\"\"Ensemble coordinator for the three risk management sub-agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 obs_space: gym.Space,\n",
    "                 action_space: gym.Space,\n",
    "                 num_outputs: int,\n",
    "                 model_config: Dict[str, Any],\n",
    "                 name: str):\n",
    "        \n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        # Calculate input dimension\n",
    "        synergy_dim = obs_space['synergy_vector'].shape[0]\n",
    "        account_dim = obs_space['account_state_vector'].shape[0]\n",
    "        self.input_dim = synergy_dim + account_dim\n",
    "        \n",
    "        # Initialize sub-agents\n",
    "        self.position_agent = PositionSizingAgent(self.input_dim)\n",
    "        self.stop_loss_agent = StopLossAgent(self.input_dim)\n",
    "        self.profit_target_agent = ProfitTargetAgent(self.input_dim)\n",
    "        \n",
    "        # Value function head\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Store value for value function\n",
    "        self._value = None\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Forward pass through the ensemble.\"\"\"\n",
    "        # Extract observations\n",
    "        obs = input_dict[\"obs\"]\n",
    "        synergy_vector = obs['synergy_vector']\n",
    "        account_vector = obs['account_state_vector']\n",
    "        \n",
    "        # Concatenate state vectors\n",
    "        combined_state = torch.cat([synergy_vector, account_vector], dim=-1)\n",
    "        \n",
    "        # Get outputs from each sub-agent\n",
    "        position_logits = self.position_agent(combined_state)\n",
    "        sl_multiplier = self.stop_loss_agent(combined_state)\n",
    "        rr_ratio = self.profit_target_agent(combined_state)\n",
    "        \n",
    "        # Compute value\n",
    "        self._value = self.value_head(combined_state).squeeze(-1)\n",
    "        \n",
    "        # Combine outputs for RLlib\n",
    "        # RLlib expects a flat output tensor for all actions\n",
    "        outputs = torch.cat([\n",
    "            position_logits,  # 6 values for discrete action\n",
    "            sl_multiplier,    # 1 value for continuous action\n",
    "            rr_ratio         # 1 value for continuous action\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return outputs, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        \"\"\"Return the value function output.\"\"\"\n",
    "        return self._value\n",
    "    \n",
    "    def get_action_dict(self, obs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Convert observations to action dictionary (for inference).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Prepare state\n",
    "            synergy_vector = obs['synergy_vector']\n",
    "            account_vector = obs['account_state_vector']\n",
    "            combined_state = torch.cat([synergy_vector, account_vector], dim=-1)\n",
    "            \n",
    "            # Get actions from sub-agents\n",
    "            position_logits = self.position_agent(combined_state)\n",
    "            position_size = torch.argmax(position_logits, dim=-1)\n",
    "            \n",
    "            sl_multiplier = self.stop_loss_agent(combined_state)\n",
    "            rr_ratio = self.profit_target_agent(combined_state)\n",
    "            \n",
    "            return {\n",
    "                'position_size': position_size,\n",
    "                'sl_atr_multiplier': sl_multiplier,\n",
    "                'rr_ratio': rr_ratio\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Configure and Run RLlib Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom model\n",
    "ModelCatalog.register_custom_model(\"risk_management_ensemble\", RiskManagementEnsemble)\n",
    "\n",
    "# Environment configuration\n",
    "env_config = {\n",
    "    'historical_data': historical_data,\n",
    "    'synergy_events': synergy_events,\n",
    "    'max_trades_per_episode': 20,\n",
    "    'account_config': {\n",
    "        'initial_balance': 50000.0,\n",
    "        'max_daily_loss': 1000.0,\n",
    "        'max_drawdown': 2000.0,\n",
    "        'max_position_size': 5,\n",
    "        'point_value': 5.0\n",
    "    },\n",
    "    'point_value': 5.0\n",
    "}\n",
    "\n",
    "# Create environment instance for testing\n",
    "test_env = RiskManagementEnv(**env_config)\n",
    "print(f\"Environment created successfully\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "print(f\"Observation space: {test_env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# PPO configuration\n",
    "config = PPOConfig()\n",
    "config = config.training(\n",
    "    lr=3e-4,\n",
    "    train_batch_size=4000,\n",
    "    sgd_minibatch_size=128,\n",
    "    num_sgd_iter=10,\n",
    "    gamma=0.99,\n",
    "    lambda_=0.95,\n",
    "    clip_param=0.2,\n",
    "    vf_clip_param=10.0,\n",
    "    entropy_coeff=0.01,\n",
    "    vf_loss_coeff=0.5,\n",
    "    grad_clip=0.5,\n",
    "    model={\n",
    "        \"custom_model\": \"risk_management_ensemble\",\n",
    "        \"custom_model_config\": {},\n",
    "    }\n",
    ")\n",
    "\n",
    "config = config.resources(\n",
    "    num_gpus=1 if torch.cuda.is_available() else 0,\n",
    "    num_cpus_per_worker=2\n",
    ")\n",
    "\n",
    "config = config.rollouts(\n",
    "    num_rollout_workers=4,\n",
    "    rollout_fragment_length=200,\n",
    "    batch_mode=\"truncate_episodes\"\n",
    ")\n",
    "\n",
    "config = config.environment(\n",
    "    env=RiskManagementEnv,\n",
    "    env_config=env_config,\n",
    "    disable_env_checking=True\n",
    ")\n",
    "\n",
    "# Build trainer\n",
    "trainer = config.build()\n",
    "print(\"Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "N_ITERATIONS = 100\n",
    "results_list = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for i in range(N_ITERATIONS):\n",
    "    # Train for one iteration\n",
    "    result = trainer.train()\n",
    "    \n",
    "    # Extract key metrics\n",
    "    episode_reward_mean = result['episode_reward_mean']\n",
    "    episode_len_mean = result['episode_len_mean']\n",
    "    \n",
    "    # Store results\n",
    "    results_list.append({\n",
    "        'iteration': i,\n",
    "        'episode_reward_mean': episode_reward_mean,\n",
    "        'episode_len_mean': episode_len_mean,\n",
    "        'episodes_total': result.get('episodes_total', 0)\n",
    "    })\n",
    "    \n",
    "    # Print progress every 10 iterations\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: reward_mean={episode_reward_mean:.3f}, len_mean={episode_len_mean:.1f}\")\n",
    "        \n",
    "    # Optional: Save checkpoint periodically\n",
    "    if i % 20 == 0 and i > 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(f\"Checkpoint saved at: {checkpoint}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Analyze Results and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Reward curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['iteration'], results_df['episode_reward_mean'])\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Episode Reward Mean')\n",
    "plt.title('M-RMS Agent Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add moving average\n",
    "window = 10\n",
    "rolling_mean = results_df['episode_reward_mean'].rolling(window=window).mean()\n",
    "plt.plot(results_df['iteration'], rolling_mean, 'r-', linewidth=2, \n",
    "         label=f'{window}-iteration moving average')\n",
    "plt.legend()\n",
    "\n",
    "# Episode length curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_df['iteration'], results_df['episode_len_mean'])\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Episode Length Mean')\n",
    "plt.title('Average Episode Length')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mrms_training_curves.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal Training Statistics:\")\n",
    "print(f\"Final reward mean: {results_df['episode_reward_mean'].iloc[-1]:.3f}\")\n",
    "print(f\"Best reward mean: {results_df['episode_reward_mean'].max():.3f}\")\n",
    "print(f\"Average reward (last 20 iterations): {results_df['episode_reward_mean'].tail(20).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save the trained model\n",
    "policy = trainer.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "# Save the complete ensemble model\n",
    "save_path = '/content/drive/MyDrive/AlgoSpace/models/mrms_agent.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'position_agent': model.position_agent.state_dict(),\n",
    "    'stop_loss_agent': model.stop_loss_agent.state_dict(),\n",
    "    'profit_target_agent': model.profit_target_agent.state_dict(),\n",
    "    'value_head': model.value_head.state_dict(),\n",
    "    'config': env_config,\n",
    "    'training_iterations': N_ITERATIONS,\n",
    "    'final_reward_mean': results_df['episode_reward_mean'].iloc[-1]\n",
    "}, save_path)\n",
    "\n",
    "print(f\"\\nModel saved successfully to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "print(\"\\nTesting saved model...\")\n",
    "\n",
    "# Create a new model instance\n",
    "test_model = RiskManagementEnsemble(\n",
    "    obs_space=test_env.observation_space,\n",
    "    action_space=test_env.action_space,\n",
    "    num_outputs=8,  # 6 for position + 1 for SL + 1 for TP\n",
    "    model_config={},\n",
    "    name=\"test_model\"\n",
    ")\n",
    "\n",
    "# Load saved weights\n",
    "checkpoint = torch.load(save_path)\n",
    "test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_model.eval()\n",
    "\n",
    "# Test on a sample observation\n",
    "obs, _ = test_env.reset()\n",
    "obs_tensor = {\n",
    "    'synergy_vector': torch.tensor(obs['synergy_vector']).unsqueeze(0),\n",
    "    'account_state_vector': torch.tensor(obs['account_state_vector']).unsqueeze(0)\n",
    "}\n",
    "\n",
    "# Get action from model\n",
    "with torch.no_grad():\n",
    "    action_dict = test_model.get_action_dict(obs_tensor)\n",
    "    \n",
    "print(\"\\nSample model output:\")\n",
    "print(f\"Position size: {action_dict['position_size'].item()}\")\n",
    "print(f\"Stop loss ATR multiplier: {action_dict['sl_atr_multiplier'].item():.3f}\")\n",
    "print(f\"Risk-reward ratio: {action_dict['rr_ratio'].item():.3f}\")\n",
    "\n",
    "# Cleanup\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"M-RMS TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTraining completed with {N_ITERATIONS} iterations\")\n",
    "print(f\"Final model saved to: {save_path}\")\n",
    "print(f\"\\nThe trained M-RMS agent has learned to:\")\n",
    "print(\"1. Size positions based on account state and market conditions\")\n",
    "print(\"2. Place stop losses using dynamic ATR multipliers\")\n",
    "print(\"3. Set profit targets with adaptive risk-reward ratios\")\n",
    "print(\"\\nThis model can now be integrated into the AlgoSpace system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}