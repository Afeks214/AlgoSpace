{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Orchestrator - Master Pipeline for AlgoSpace-8\n",
    "\n",
    "This master notebook orchestrates the complete training pipeline for the AlgoSpace-8 MARL trading system. It manages:\n",
    "\n",
    "1. **Phase 1**: Data preparation and validation\n",
    "2. **Phase 2**: Frozen expert training (RDE, M-RMS)\n",
    "3. **Phase 3**: Embedder training (Market, Risk, Tactical)\n",
    "4. **Phase 4**: Main MARL Core training with all components\n",
    "\n",
    "Designed to complete full training in <24 hours on Google Colab Pro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports and setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import yaml\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import logging\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🚀 Running Training Orchestrator in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Running Training Orchestrator locally\")\n",
    "\n",
    "# Mount Drive and setup paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    PROJECT_PATH = Path('/content/drive/MyDrive/AlgoSpace-8')\n",
    "    NOTEBOOK_PATH = PROJECT_PATH / 'notebooks'\n",
    "    sys.path.insert(0, str(PROJECT_PATH))\n",
    "else:\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    NOTEBOOK_PATH = PROJECT_PATH / 'notebooks'\n",
    "    sys.path.insert(0, str(PROJECT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install -q numpy pandas h5py pyyaml tensorboard wandb\n",
    "    !pip install -q tqdm matplotlib seaborn psutil gputil\n",
    "    !pip install -q nbformat nbclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import orchestration utilities\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Notebook execution\n",
    "import nbformat\n",
    "from nbclient import NotebookClient\n",
    "\n",
    "# Custom utilities\n",
    "from notebooks.utils.colab_setup import ColabSetup, SessionMonitor, setup_colab_training\n",
    "from notebooks.utils.drive_manager import DriveManager\n",
    "from notebooks.utils.checkpoint_manager import CheckpointManager\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('TrainingOrchestrator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "if IN_COLAB:\n",
    "    colab_setup = setup_colab_training(\n",
    "        project_name=\"AlgoSpace-8\",\n",
    "        mount_drive=True,\n",
    "        setup_wandb=True,\n",
    "        keep_alive=True\n",
    "    )\n",
    "    \n",
    "    drive_manager = DriveManager(str(PROJECT_PATH))\n",
    "    checkpoint_manager = CheckpointManager(drive_manager)\n",
    "    session_monitor = SessionMonitor(max_runtime_hours=23.5)\n",
    "    \n",
    "    device = colab_setup.device\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"🎮 Using device: {device}\")\n",
    "print(f\"📁 Project path: {PROJECT_PATH}\")\n",
    "print(f\"📓 Notebook path: {NOTEBOOK_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Orchestration Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unified configuration\n",
    "config_path = NOTEBOOK_PATH / 'config' / 'unified_config.yaml'\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        unified_config = yaml.safe_load(f)\n",
    "    print(\"✅ Loaded unified configuration\")\n",
    "else:\n",
    "    # Default configuration\n",
    "    unified_config = {\n",
    "        'training': {\n",
    "            'phases': {\n",
    "                'data_prep': {'enabled': True, 'timeout_hours': 1.0},\n",
    "                'frozen_experts': {'enabled': True, 'timeout_hours': 6.0},\n",
    "                'embedders': {'enabled': True, 'timeout_hours': 8.0},\n",
    "                'main_core': {'enabled': True, 'timeout_hours': 8.0}\n",
    "            },\n",
    "            'max_total_hours': 23.0,\n",
    "            'checkpoint_interval_minutes': 30,\n",
    "            'early_stopping': {\n",
    "                'patience': 50,\n",
    "                'min_delta': 0.001\n",
    "            }\n",
    "        },\n",
    "        'notebooks': {\n",
    "            'data_prep': 'Data_Preparation_Colab.ipynb',\n",
    "            'regime_agent': 'agents/Regime_Agent_Training.ipynb',\n",
    "            'tactical_agent': 'agents/Tactical_Agent_Training.ipynb',\n",
    "            'structure_agent': 'agents/Structure_Agent_Training.ipynb',\n",
    "            'mrms_agent': 'agents/MRMS_Training_Colab.ipynb',\n",
    "            'main_core': 'MARL_Training_Master_Colab.ipynb'\n",
    "        },\n",
    "        'hardware': {\n",
    "            'gpu_memory_fraction': 0.9,\n",
    "            'cpu_threads': 4\n",
    "        }\n",
    "    }\n",
    "    print(\"⚠️ Using default configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPhase:\n",
    "    \"\"\"Represents a training phase in the pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, notebook: str, config: Dict[str, Any], \n",
    "                 timeout_hours: float = 6.0):\n",
    "        self.name = name\n",
    "        self.notebook = notebook\n",
    "        self.config = config\n",
    "        self.timeout_hours = timeout_hours\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.status = 'pending'\n",
    "        self.metrics = {}\n",
    "        self.error = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"TrainingPhase(name={self.name}, status={self.status})\"\n",
    "\n",
    "\n",
    "class OrchestrationState:\n",
    "    \"\"\"Tracks overall orchestration state.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.phases: List[TrainingPhase] = []\n",
    "        self.current_phase_idx = 0\n",
    "        self.start_time = datetime.now()\n",
    "        self.checkpoints = {}\n",
    "        self.total_runtime_hours = 0\n",
    "        self.is_resuming = False\n",
    "    \n",
    "    def add_phase(self, phase: TrainingPhase):\n",
    "        self.phases.append(phase)\n",
    "    \n",
    "    def get_current_phase(self) -> Optional[TrainingPhase]:\n",
    "        if self.current_phase_idx < len(self.phases):\n",
    "            return self.phases[self.current_phase_idx]\n",
    "        return None\n",
    "    \n",
    "    def advance_phase(self):\n",
    "        self.current_phase_idx += 1\n",
    "    \n",
    "    def get_elapsed_hours(self) -> float:\n",
    "        return (datetime.now() - self.start_time).total_seconds() / 3600\n",
    "    \n",
    "    def save_state(self, path: str):\n",
    "        \"\"\"Save orchestration state for recovery.\"\"\"\n",
    "        state_dict = {\n",
    "            'current_phase_idx': self.current_phase_idx,\n",
    "            'start_time': self.start_time.isoformat(),\n",
    "            'phases': [\n",
    "                {\n",
    "                    'name': p.name,\n",
    "                    'status': p.status,\n",
    "                    'metrics': p.metrics,\n",
    "                    'start_time': p.start_time.isoformat() if p.start_time else None,\n",
    "                    'end_time': p.end_time.isoformat() if p.end_time else None\n",
    "                }\n",
    "                for p in self.phases\n",
    "            ],\n",
    "            'checkpoints': self.checkpoints\n",
    "        }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(state_dict, f, indent=2)\n",
    "    \n",
    "    def load_state(self, path: str):\n",
    "        \"\"\"Load orchestration state for resuming.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            state_dict = json.load(f)\n",
    "        \n",
    "        self.current_phase_idx = state_dict['current_phase_idx']\n",
    "        self.start_time = datetime.fromisoformat(state_dict['start_time'])\n",
    "        self.checkpoints = state_dict['checkpoints']\n",
    "        self.is_resuming = True\n",
    "        \n",
    "        # Update phase statuses\n",
    "        for i, phase_data in enumerate(state_dict['phases']):\n",
    "            if i < len(self.phases):\n",
    "                self.phases[i].status = phase_data['status']\n",
    "                self.phases[i].metrics = phase_data['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Notebook Execution Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookExecutor:\n",
    "    \"\"\"Executes Jupyter notebooks with monitoring and error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, notebook_path: Path, timeout: int = 3600):\n",
    "        self.notebook_path = notebook_path\n",
    "        self.timeout = timeout\n",
    "        self.client = None\n",
    "        self.nb = None\n",
    "    \n",
    "    def execute(self, parameters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Execute notebook and return results.\"\"\"\n",
    "        logger.info(f\"Executing notebook: {self.notebook_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load notebook\n",
    "            with open(self.notebook_path, 'r') as f:\n",
    "                self.nb = nbformat.read(f, as_version=4)\n",
    "            \n",
    "            # Inject parameters if provided\n",
    "            if parameters:\n",
    "                self._inject_parameters(parameters)\n",
    "            \n",
    "            # Create client and execute\n",
    "            self.client = NotebookClient(\n",
    "                self.nb,\n",
    "                timeout=self.timeout,\n",
    "                kernel_name='python3',\n",
    "                resources={'metadata': {'path': str(self.notebook_path.parent)}}\n",
    "            )\n",
    "            \n",
    "            # Execute notebook\n",
    "            self.client.execute()\n",
    "            \n",
    "            # Extract results\n",
    "            results = self._extract_results()\n",
    "            \n",
    "            # Save executed notebook\n",
    "            executed_path = self.notebook_path.with_suffix('.executed.ipynb')\n",
    "            with open(executed_path, 'w') as f:\n",
    "                nbformat.write(self.nb, f)\n",
    "            \n",
    "            logger.info(f\"✅ Successfully executed: {self.notebook_path.name}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to execute {self.notebook_path.name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _inject_parameters(self, parameters: Dict[str, Any]):\n",
    "        \"\"\"Inject parameters into notebook.\"\"\"\n",
    "        param_cell = nbformat.v4.new_code_cell(\n",
    "            source=f\"# Injected parameters\\n\" + \n",
    "                   \"\\n\".join([f\"{k} = {repr(v)}\" for k, v in parameters.items()])\n",
    "        )\n",
    "        \n",
    "        # Insert after first cell\n",
    "        self.nb.cells.insert(1, param_cell)\n",
    "    \n",
    "    def _extract_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"Extract results from executed notebook.\"\"\"\n",
    "        results = {\n",
    "            'outputs': [],\n",
    "            'metrics': {},\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        for cell in self.nb.cells:\n",
    "            if cell.cell_type == 'code' and hasattr(cell, 'outputs'):\n",
    "                for output in cell.outputs:\n",
    "                    if output.output_type == 'error':\n",
    "                        results['errors'].append({\n",
    "                            'ename': output.ename,\n",
    "                            'evalue': output.evalue,\n",
    "                            'traceback': output.traceback\n",
    "                        })\n",
    "                    elif output.output_type == 'execute_result':\n",
    "                        # Look for metrics in output\n",
    "                        if 'data' in output and 'text/plain' in output.data:\n",
    "                            text = output.data['text/plain']\n",
    "                            if 'metrics' in text.lower():\n",
    "                                results['outputs'].append(text)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def execute_training_phase(phase: TrainingPhase, state: OrchestrationState) -> bool:\n",
    "    \"\"\"Execute a single training phase.\"\"\"\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Starting Phase: {phase.name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    phase.start_time = datetime.now()\n",
    "    phase.status = 'running'\n",
    "    \n",
    "    try:\n",
    "        # Check remaining time\n",
    "        elapsed = state.get_elapsed_hours()\n",
    "        remaining = unified_config['training']['max_total_hours'] - elapsed\n",
    "        \n",
    "        if remaining < 0.5:\n",
    "            logger.warning(f\"⚠️ Less than 30 minutes remaining, skipping {phase.name}\")\n",
    "            phase.status = 'skipped'\n",
    "            return False\n",
    "        \n",
    "        # Prepare notebook path\n",
    "        notebook_path = NOTEBOOK_PATH / phase.notebook\n",
    "        \n",
    "        if not notebook_path.exists():\n",
    "            logger.error(f\"❌ Notebook not found: {notebook_path}\")\n",
    "            phase.status = 'failed'\n",
    "            phase.error = 'Notebook not found'\n",
    "            return False\n",
    "        \n",
    "        # Execute notebook\n",
    "        executor = NotebookExecutor(\n",
    "            notebook_path,\n",
    "            timeout=int(min(phase.timeout_hours, remaining) * 3600)\n",
    "        )\n",
    "        \n",
    "        # Add phase-specific parameters\n",
    "        parameters = {\n",
    "            'max_runtime_hours': min(phase.timeout_hours, remaining),\n",
    "            'checkpoint_path': str(drive_manager.checkpoint_path),\n",
    "            'is_orchestrated': True,\n",
    "            'phase_name': phase.name\n",
    "        }\n",
    "        \n",
    "        # Execute\n",
    "        results = executor.execute(parameters)\n",
    "        \n",
    "        # Update phase\n",
    "        phase.end_time = datetime.now()\n",
    "        phase.metrics = results.get('metrics', {})\n",
    "        \n",
    "        if results['errors']:\n",
    "            phase.status = 'failed'\n",
    "            phase.error = results['errors'][0]\n",
    "            logger.error(f\"❌ Phase {phase.name} failed with errors\")\n",
    "            return False\n",
    "        else:\n",
    "            phase.status = 'completed'\n",
    "            logger.info(f\"✅ Phase {phase.name} completed successfully\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_name = f\"{phase.name}_completed\"\n",
    "            state.checkpoints[checkpoint_name] = {\n",
    "                'timestamp': phase.end_time.isoformat(),\n",
    "                'metrics': phase.metrics\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Exception in phase {phase.name}: {str(e)}\")\n",
    "        phase.status = 'failed'\n",
    "        phase.error = str(e)\n",
    "        phase.end_time = datetime.now()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize orchestration state\n",
    "orch_state = OrchestrationState()\n",
    "\n",
    "# Define training phases\n",
    "training_phases = [\n",
    "    TrainingPhase(\n",
    "        name=\"Data Preparation\",\n",
    "        notebook=unified_config['notebooks']['data_prep'],\n",
    "        config=unified_config['training']['phases']['data_prep'],\n",
    "        timeout_hours=unified_config['training']['phases']['data_prep']['timeout_hours']\n",
    "    ),\n",
    "    TrainingPhase(\n",
    "        name=\"Regime Detection Expert\",\n",
    "        notebook=unified_config['notebooks']['regime_agent'],\n",
    "        config=unified_config['training']['phases']['frozen_experts'],\n",
    "        timeout_hours=2.0\n",
    "    ),\n",
    "    TrainingPhase(\n",
    "        name=\"Tactical Agent\",\n",
    "        notebook=unified_config['notebooks']['tactical_agent'],\n",
    "        config=unified_config['training']['phases']['frozen_experts'],\n",
    "        timeout_hours=2.0\n",
    "    ),\n",
    "    TrainingPhase(\n",
    "        name=\"Structure Agent\",\n",
    "        notebook=unified_config['notebooks']['structure_agent'],\n",
    "        config=unified_config['training']['phases']['frozen_experts'],\n",
    "        timeout_hours=2.0\n",
    "    ),\n",
    "    TrainingPhase(\n",
    "        name=\"M-RMS Ensemble\",\n",
    "        notebook=unified_config['notebooks']['mrms_agent'],\n",
    "        config=unified_config['training']['phases']['frozen_experts'],\n",
    "        timeout_hours=3.0\n",
    "    ),\n",
    "    TrainingPhase(\n",
    "        name=\"Main MARL Core\",\n",
    "        notebook=unified_config['notebooks']['main_core'],\n",
    "        config=unified_config['training']['phases']['main_core'],\n",
    "        timeout_hours=unified_config['training']['phases']['main_core']['timeout_hours']\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add phases to orchestration state\n",
    "for phase in training_phases:\n",
    "    orch_state.add_phase(phase)\n",
    "\n",
    "print(f\"📋 Configured {len(training_phases)} training phases:\")\n",
    "for i, phase in enumerate(training_phases):\n",
    "    print(f\"   {i+1}. {phase.name} ({phase.timeout_hours:.1f}h max)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing orchestration state (resume capability)\n",
    "state_file = drive_manager.checkpoint_path / \"orchestration_state.json\"\n",
    "\n",
    "if state_file.exists() and IN_COLAB:\n",
    "    print(\"\\n📂 Found existing orchestration state\")\n",
    "    \n",
    "    # Load state\n",
    "    orch_state.load_state(str(state_file))\n",
    "    \n",
    "    print(f\"✅ Resuming from phase {orch_state.current_phase_idx + 1}\")\n",
    "    print(f\"   Elapsed time: {orch_state.get_elapsed_hours():.1f} hours\")\n",
    "    \n",
    "    # Show completed phases\n",
    "    print(\"\\n📊 Completed phases:\")\n",
    "    for phase in orch_state.phases[:orch_state.current_phase_idx]:\n",
    "        print(f\"   ✅ {phase.name} - {phase.status}\")\n",
    "else:\n",
    "    print(\"\\n🆕 Starting fresh orchestration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Orchestration Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(orch_state: OrchestrationState, \n",
    "                         dry_run: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"Execute the complete training pipeline.\"\"\"\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"🚀 Starting AlgoSpace-8 Training Pipeline\")\n",
    "    logger.info(f\"   Total phases: {len(orch_state.phases)}\")\n",
    "    logger.info(f\"   Max runtime: {unified_config['training']['max_total_hours']:.1f} hours\")\n",
    "    logger.info(f\"   Device: {device}\")\n",
    "    logger.info(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    pipeline_results = {\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'phases': [],\n",
    "        'success': True,\n",
    "        'total_runtime': 0\n",
    "    }\n",
    "    \n",
    "    # Main execution loop\n",
    "    while orch_state.current_phase_idx < len(orch_state.phases):\n",
    "        # Check session time\n",
    "        if IN_COLAB and session_monitor.is_ending_soon(buffer_minutes=30):\n",
    "            logger.warning(\"⚠️ Session ending soon! Saving state and stopping...\")\n",
    "            break\n",
    "        \n",
    "        # Get current phase\n",
    "        phase = orch_state.get_current_phase()\n",
    "        \n",
    "        if phase.status == 'completed':\n",
    "            logger.info(f\"⏭️ Skipping completed phase: {phase.name}\")\n",
    "            orch_state.advance_phase()\n",
    "            continue\n",
    "        \n",
    "        # Display phase info\n",
    "        elapsed = orch_state.get_elapsed_hours()\n",
    "        remaining = unified_config['training']['max_total_hours'] - elapsed\n",
    "        \n",
    "        print(f\"\\n📊 Phase {orch_state.current_phase_idx + 1}/{len(orch_state.phases)}\")\n",
    "        print(f\"   Name: {phase.name}\")\n",
    "        print(f\"   Notebook: {phase.notebook}\")\n",
    "        print(f\"   Timeout: {phase.timeout_hours:.1f}h\")\n",
    "        print(f\"   Elapsed: {elapsed:.1f}h / Remaining: {remaining:.1f}h\")\n",
    "        \n",
    "        if dry_run:\n",
    "            logger.info(f\"🔄 [DRY RUN] Would execute: {phase.name}\")\n",
    "            phase.status = 'dry_run'\n",
    "            orch_state.advance_phase()\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        # Execute phase\n",
    "        success = execute_training_phase(phase, orch_state)\n",
    "        \n",
    "        # Record results\n",
    "        phase_result = {\n",
    "            'name': phase.name,\n",
    "            'status': phase.status,\n",
    "            'start_time': phase.start_time.isoformat() if phase.start_time else None,\n",
    "            'end_time': phase.end_time.isoformat() if phase.end_time else None,\n",
    "            'duration_hours': (\n",
    "                (phase.end_time - phase.start_time).total_seconds() / 3600\n",
    "                if phase.start_time and phase.end_time else 0\n",
    "            ),\n",
    "            'metrics': phase.metrics,\n",
    "            'error': phase.error\n",
    "        }\n",
    "        pipeline_results['phases'].append(phase_result)\n",
    "        \n",
    "        # Save state after each phase\n",
    "        if IN_COLAB:\n",
    "            orch_state.save_state(str(state_file))\n",
    "            logger.info(\"💾 Saved orchestration state\")\n",
    "        \n",
    "        # Check if should continue\n",
    "        if not success and phase.status == 'failed':\n",
    "            logger.error(f\"❌ Pipeline stopped due to failure in {phase.name}\")\n",
    "            pipeline_results['success'] = False\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if IN_COLAB:\n",
    "            colab_setup.optimize_memory()\n",
    "        \n",
    "        # Advance to next phase\n",
    "        orch_state.advance_phase()\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_results['end_time'] = datetime.now().isoformat()\n",
    "    pipeline_results['total_runtime'] = orch_state.get_elapsed_hours()\n",
    "    \n",
    "    return pipeline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute pipeline (set dry_run=True for testing)\n",
    "DRY_RUN = False  # Set to False for actual execution\n",
    "\n",
    "print(f\"\\n🎯 Pipeline mode: {'DRY RUN' if DRY_RUN else 'FULL EXECUTION'}\")\n",
    "\n",
    "if not DRY_RUN:\n",
    "    response = input(\"\\n⚠️ This will run the complete training pipeline. Continue? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"❌ Pipeline execution cancelled\")\n",
    "    else:\n",
    "        pipeline_results = run_training_pipeline(orch_state, dry_run=DRY_RUN)\n",
    "else:\n",
    "    pipeline_results = run_training_pipeline(orch_state, dry_run=DRY_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline_results(results: Dict[str, Any]):\n",
    "    \"\"\"Create visualizations of pipeline execution.\"\"\"\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Timeline visualization\n",
    "    phases_data = results['phases']\n",
    "    phase_names = [p['name'] for p in phases_data]\n",
    "    durations = [p['duration_hours'] for p in phases_data]\n",
    "    statuses = [p['status'] for p in phases_data]\n",
    "    \n",
    "    # Color map for statuses\n",
    "    color_map = {\n",
    "        'completed': 'green',\n",
    "        'failed': 'red',\n",
    "        'skipped': 'orange',\n",
    "        'dry_run': 'lightblue',\n",
    "        'pending': 'gray'\n",
    "    }\n",
    "    colors = [color_map.get(s, 'gray') for s in statuses]\n",
    "    \n",
    "    # Duration bar chart\n",
    "    y_pos = np.arange(len(phase_names))\n",
    "    ax1.barh(y_pos, durations, color=colors)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(phase_names)\n",
    "    ax1.set_xlabel('Duration (hours)')\n",
    "    ax1.set_title('Training Phase Durations')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add duration labels\n",
    "    for i, (duration, status) in enumerate(zip(durations, statuses)):\n",
    "        if duration > 0:\n",
    "            ax1.text(duration + 0.1, i, f\"{duration:.1f}h\", \n",
    "                    va='center', fontsize=9)\n",
    "        ax1.text(0.1, i, status, va='center', fontsize=8, \n",
    "                style='italic', alpha=0.7)\n",
    "    \n",
    "    # Cumulative timeline\n",
    "    cumulative_times = []\n",
    "    current_time = 0\n",
    "    for phase in phases_data:\n",
    "        cumulative_times.append(current_time)\n",
    "        current_time += phase['duration_hours']\n",
    "    \n",
    "    ax2.plot(cumulative_times, range(len(phases_data)), 'bo-', linewidth=2)\n",
    "    ax2.set_xlabel('Cumulative Time (hours)')\n",
    "    ax2.set_ylabel('Phase Index')\n",
    "    ax2.set_title('Training Progress Timeline')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axvline(x=unified_config['training']['max_total_hours'], \n",
    "               color='red', linestyle='--', label='Max Runtime')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if IN_COLAB:\n",
    "        fig_path = drive_manager.results_path / \"plots\" / \"pipeline_execution.png\"\n",
    "        fig.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n📊 Saved pipeline visualization to: {fig_path}\")\n",
    "\n",
    "# Visualize results\n",
    "if 'pipeline_results' in locals():\n",
    "    visualize_pipeline_results(pipeline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "def generate_summary_report(results: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate a markdown summary report.\"\"\"\n",
    "    \n",
    "    report = f\"\"\"# AlgoSpace-8 Training Pipeline Report\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overview\n",
    "- **Status**: {'✅ SUCCESS' if results['success'] else '❌ FAILED'}\n",
    "- **Total Runtime**: {results['total_runtime']:.2f} hours\n",
    "- **Phases Completed**: {sum(1 for p in results['phases'] if p['status'] == 'completed')}/{len(results['phases'])}\n",
    "\n",
    "## Phase Details\n",
    "\n",
    "| Phase | Status | Duration | Start Time | End Time |\n",
    "|-------|--------|----------|------------|----------|\n",
    "\"\"\"\n",
    "    \n",
    "    for phase in results['phases']:\n",
    "        status_emoji = {\n",
    "            'completed': '✅',\n",
    "            'failed': '❌',\n",
    "            'skipped': '⏭️',\n",
    "            'dry_run': '🔄',\n",
    "            'pending': '⏸️'\n",
    "        }.get(phase['status'], '❓')\n",
    "        \n",
    "        start = phase['start_time'][:16] if phase['start_time'] else 'N/A'\n",
    "        end = phase['end_time'][:16] if phase['end_time'] else 'N/A'\n",
    "        \n",
    "        report += f\"| {phase['name']} | {status_emoji} {phase['status']} | \"\n",
    "        report += f\"{phase['duration_hours']:.1f}h | {start} | {end} |\\n\"\n",
    "    \n",
    "    # Add metrics if available\n",
    "    report += \"\\n## Key Metrics\\n\\n\"\n",
    "    \n",
    "    for phase in results['phases']:\n",
    "        if phase['metrics']:\n",
    "            report += f\"### {phase['name']}\\n\"\n",
    "            for key, value in phase['metrics'].items():\n",
    "                report += f\"- {key}: {value}\\n\"\n",
    "            report += \"\\n\"\n",
    "    \n",
    "    # Add errors if any\n",
    "    errors = [p for p in results['phases'] if p['error']]\n",
    "    if errors:\n",
    "        report += \"\\n## Errors\\n\\n\"\n",
    "        for phase in errors:\n",
    "            report += f\"### {phase['name']}\\n\"\n",
    "            report += f\"```\\n{phase['error']}\\n```\\n\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save report\n",
    "if 'pipeline_results' in locals():\n",
    "    report = generate_summary_report(pipeline_results)\n",
    "    print(report)\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        report_path = drive_manager.results_path / \"pipeline_report.md\"\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        print(f\"\\n📄 Saved report to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post-Pipeline Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_training_outputs():\n",
    "    \"\"\"Validate that all expected outputs exist.\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 Validating training outputs...\")\n",
    "    \n",
    "    expected_outputs = [\n",
    "        ('Data files', drive_manager.data_path / 'processed', ['.h5', '.parquet']),\n",
    "        ('Regime model', drive_manager.model_path / 'production', ['regime_expert']),\n",
    "        ('Tactical model', drive_manager.model_path / 'production', ['tactical_agent']),\n",
    "        ('Structure model', drive_manager.model_path / 'production', ['structure_agent']),\n",
    "        ('MRMS ensemble', drive_manager.model_path / 'production', ['mrms_ensemble']),\n",
    "        ('Main Core', drive_manager.model_path / 'production', ['main_marl_core']),\n",
    "        ('Checkpoints', drive_manager.checkpoint_path, ['.pt']),\n",
    "        ('Results', drive_manager.results_path, ['.json', '.png'])\n",
    "    ]\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    for name, path, patterns in expected_outputs:\n",
    "        if not path.exists():\n",
    "            validation_results.append((name, '❌ Path not found', str(path)))\n",
    "            continue\n",
    "        \n",
    "        # Check for files matching patterns\n",
    "        found_files = []\n",
    "        for pattern in patterns:\n",
    "            if pattern.startswith('.'):\n",
    "                # File extension\n",
    "                found_files.extend(list(path.glob(f'*{pattern}')))\n",
    "            else:\n",
    "                # File name pattern\n",
    "                found_files.extend(list(path.glob(f'*{pattern}*')))\n",
    "        \n",
    "        if found_files:\n",
    "            validation_results.append((name, '✅ Found', f\"{len(found_files)} files\"))\n",
    "        else:\n",
    "            validation_results.append((name, '⚠️ No files', str(path)))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n📋 Validation Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    for name, status, info in validation_results:\n",
    "        print(f\"{name:20} {status:15} {info}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_valid = all('✅' in status for _, status, _ in validation_results)\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Overall: {'✅ All outputs valid' if all_valid else '⚠️ Some outputs missing'}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Run validation\n",
    "if IN_COLAB and 'pipeline_results' in locals():\n",
    "    validation_results = validate_training_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Steps & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final deployment package\n",
    "if IN_COLAB and 'pipeline_results' in locals() and pipeline_results['success']:\n",
    "    print(\"\\n📦 Creating final deployment package...\")\n",
    "    \n",
    "    try:\n",
    "        deployment_package = drive_manager.create_training_package(\n",
    "            f\"algospace8_deployment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        print(f\"✅ Deployment package created: {deployment_package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create deployment package: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 AlgoSpace-8 Training Pipeline Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'pipeline_results' in locals():\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"   Status: {'SUCCESS' if pipeline_results['success'] else 'FAILED'}\")\n",
    "    print(f\"   Runtime: {pipeline_results['total_runtime']:.2f} hours\")\n",
    "    print(f\"   Phases: {sum(1 for p in pipeline_results['phases'] if p['status'] == 'completed')}/{len(pipeline_results['phases'])} completed\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"\\n💾 All outputs saved to: {drive_manager.base_path}\")\n",
    "    print(f\"\\n⏱️ Session time remaining: {session_monitor.get_remaining_time()['remaining_hours']:.1f} hours\")\n",
    "\n",
    "print(\"\\n🚀 Next steps:\")\n",
    "print(\"   1. Review the pipeline report and validation results\")\n",
    "print(\"   2. Run the Integration Test notebook to verify all components\")\n",
    "print(\"   3. Use the Production Export notebook to prepare for deployment\")\n",
    "print(\"   4. Deploy to production environment\")\n",
    "\n",
    "print(\"\\n✨ Happy trading!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Training Orchestrator notebook successfully manages the complete AlgoSpace-8 training pipeline:\n",
    "\n",
    "1. **Automated Execution**: Runs all training phases in sequence\n",
    "2. **Time Management**: Optimizes for <24 hour Colab runtime\n",
    "3. **Error Handling**: Graceful failure recovery and state persistence\n",
    "4. **Progress Monitoring**: Real-time status updates and visualizations\n",
    "5. **Resume Capability**: Can resume from any interruption point\n",
    "6. **Validation**: Ensures all outputs are created correctly\n",
    "7. **Deployment Ready**: Creates final package for production\n",
    "\n",
    "The pipeline is now ready for production use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}