{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Regime Detector Agent Training - Google Colab\n",
    "\n",
    "This notebook trains the Regime Detector agent individually before multi-agent training.\n",
    "\n",
    "## Agent Overview:\n",
    "The Regime Detector identifies market regimes (trending, mean-reverting, volatile) using:\n",
    "- Correlation matrices\n",
    "- Volatility patterns\n",
    "- Market microstructure signals\n",
    "\n",
    "## Training Strategy:\n",
    "- Pre-train on historical regime labels\n",
    "- Fine-tune with reinforcement learning\n",
    "- Optimize for regime detection accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_setup"
   },
   "outputs": [],
   "source": [
    "# Check GPU and mount Drive\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# GPU check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available\")\n",
    "\n",
    "# Mount Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/AlgoSpace\"\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    DRIVE_BASE = \"./drive_simulation\"\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Clone repo\n",
    "REPO_PATH = \"/content/AlgoSpace\" if IN_COLAB else \".\"\n",
    "if IN_COLAB and not os.path.exists(REPO_PATH):\n",
    "    !git clone https://github.com/QuantNova/AlgoSpace.git {REPO_PATH}\n",
    "\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "sys.path.insert(0, os.path.join(REPO_PATH, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy pandas h5py matplotlib seaborn\n",
    "!pip install -q wandb tensorboard tqdm\n",
    "!pip install -q scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "data_file = f\"{DRIVE_BASE}/data/processed/marl_training_data_20231201.h5\"  # Update with your file\n",
    "\n",
    "print(f\"üìÇ Loading data from: {data_file}\")\n",
    "\n",
    "with h5py.File(data_file, 'r') as f:\n",
    "    # Load regime detector specific data\n",
    "    train_data = f['regime_detector/train/data'][:]\n",
    "    train_dates = [d.decode('utf-8') for d in f['regime_detector/train/dates'][:]]\n",
    "    \n",
    "    val_data = f['regime_detector/val/data'][:]\n",
    "    val_dates = [d.decode('utf-8') for d in f['regime_detector/val/dates'][:]]\n",
    "    \n",
    "    test_data = f['regime_detector/test/data'][:]\n",
    "    test_dates = [d.decode('utf-8') for d in f['regime_detector/test/dates'][:]]\n",
    "    \n",
    "    # Load correlation matrices\n",
    "    corr_train = f['correlation_matrices/train/data'][:]\n",
    "    corr_val = f['correlation_matrices/val/data'][:]\n",
    "    corr_test = f['correlation_matrices/test/data'][:]\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata = {key: f['metadata'].attrs[key] for key in f['metadata'].attrs}\n",
    "\n",
    "print(f\"‚úÖ Data loaded\")\n",
    "print(f\"   Train: {train_data.shape}\")\n",
    "print(f\"   Val: {val_data.shape}\")\n",
    "print(f\"   Test: {test_data.shape}\")\n",
    "print(f\"   Correlation matrices: {corr_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regime_labeling"
   },
   "source": [
    "## 3. Generate Regime Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_labels"
   },
   "outputs": [],
   "source": [
    "# Generate regime labels based on market characteristics\n",
    "def generate_regime_labels(data, lookback=20):\n",
    "    \"\"\"Generate regime labels from market data.\n",
    "    \n",
    "    Regimes:\n",
    "    0: Low volatility / Trending\n",
    "    1: High volatility / Choppy\n",
    "    2: Mean reverting\n",
    "    3: Crisis / Extreme volatility\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # Assuming features include: returns, volume_ratio, atr, adx, vix_proxy\n",
    "    # Shape: (samples, assets, features)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # Get average metrics across assets\n",
    "        returns = data[i, :, 0]  # First feature is returns\n",
    "        volatility = np.std(returns) * np.sqrt(252)  # Annualized vol\n",
    "        \n",
    "        # Simple regime classification\n",
    "        if volatility < 0.15:  # Low vol\n",
    "            label = 0\n",
    "        elif volatility > 0.30:  # High vol\n",
    "            label = 3 if volatility > 0.50 else 1\n",
    "        else:  # Medium vol\n",
    "            # Check for mean reversion using autocorrelation\n",
    "            if i >= lookback:\n",
    "                recent_returns = data[i-lookback:i, :, 0].mean(axis=1)\n",
    "                autocorr = np.corrcoef(recent_returns[:-1], recent_returns[1:])[0, 1]\n",
    "                label = 2 if autocorr < -0.1 else 1\n",
    "            else:\n",
    "                label = 1\n",
    "        \n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(labels)\n",
    "\n",
    "# Generate labels\n",
    "train_labels = generate_regime_labels(train_data)\n",
    "val_labels = generate_regime_labels(val_data)\n",
    "test_labels = generate_regime_labels(test_data)\n",
    "\n",
    "# Show label distribution\n",
    "regime_names = ['Low Vol/Trend', 'High Vol/Choppy', 'Mean Reverting', 'Crisis']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (labels, name) in enumerate([(train_labels, 'Train'), \n",
    "                                    (val_labels, 'Val'), \n",
    "                                    (test_labels, 'Test')]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    counts = np.bincount(labels, minlength=4)\n",
    "    plt.bar(range(4), counts)\n",
    "    plt.title(f'{name} Set')\n",
    "    plt.xlabel('Regime')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(4), regime_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 4. Initialize Regime Detector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_model"
   },
   "outputs": [],
   "source": [
    "# Import Regime Detector\n",
    "from agents.regime_detector import RegimeDetector\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "config_path = os.path.join(REPO_PATH, 'config/training_config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Get regime detector config\n",
    "regime_config = config['agents']['regime_detector']\n",
    "\n",
    "# Update config for individual training\n",
    "regime_config['n_features'] = train_data.shape[-1]\n",
    "regime_config['n_assets'] = train_data.shape[1]\n",
    "regime_config['n_regimes'] = 4  # Number of regime classes\n",
    "\n",
    "# Initialize model\n",
    "model = RegimeDetector(regime_config).to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model initialized\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "supervised_training"
   },
   "source": [
    "## 5. Supervised Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_setup"
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(train_data),\n",
    "    torch.FloatTensor(corr_train),\n",
    "    torch.LongTensor(train_labels)\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(val_data),\n",
    "    torch.FloatTensor(corr_val),\n",
    "    torch.LongTensor(val_labels)\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "print(f\"‚úÖ Training setup complete\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for features, corr_matrix, labels in tqdm(loader, desc=\"Training\"):\n",
    "        features = features.to(device)\n",
    "        corr_matrix = corr_matrix.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process through regime detector\n",
    "        market_state = {'features': features, 'correlation_matrix': corr_matrix}\n",
    "        output = model(market_state)\n",
    "        \n",
    "        # Get regime logits (assuming model outputs include 'regime_logits')\n",
    "        if hasattr(model, 'regime_classifier'):\n",
    "            regime_logits = output['regime_logits']\n",
    "        else:\n",
    "            # If not, use the main output\n",
    "            regime_logits = output['regime_probabilities']\n",
    "        \n",
    "        loss = criterion(regime_logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(regime_logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, corr_matrix, labels in loader:\n",
    "            features = features.to(device)\n",
    "            corr_matrix = corr_matrix.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            market_state = {'features': features, 'correlation_matrix': corr_matrix}\n",
    "            output = model(market_state)\n",
    "            \n",
    "            if hasattr(model, 'regime_classifier'):\n",
    "                regime_logits = output['regime_logits']\n",
    "            else:\n",
    "                regime_logits = output['regime_probabilities']\n",
    "            \n",
    "            loss = criterion(regime_logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(regime_logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training\n",
    "n_epochs = 30\n",
    "best_val_acc = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"üöÄ Starting supervised pre-training...\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'config': regime_config\n",
    "        }, f\"{DRIVE_BASE}/models/regime_detector_pretrained.pt\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['val_loss'], label='Validation')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'], label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Regime Detection Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_BASE}/results/regime_detector_training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_evaluation"
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(f\"{DRIVE_BASE}/models/regime_detector_pretrained.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(test_data),\n",
    "    torch.FloatTensor(corr_test),\n",
    "    torch.LongTensor(test_labels)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nüìä Test Set Performance:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, corr_matrix, labels in test_loader:\n",
    "        features = features.to(device)\n",
    "        corr_matrix = corr_matrix.to(device)\n",
    "        \n",
    "        market_state = {'features': features, 'correlation_matrix': corr_matrix}\n",
    "        output = model(market_state)\n",
    "        \n",
    "        if hasattr(model, 'regime_classifier'):\n",
    "            regime_logits = output['regime_logits']\n",
    "        else:\n",
    "            regime_logits = output['regime_probabilities']\n",
    "        \n",
    "        _, predicted = torch.max(regime_logits, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=regime_names, yticklabels=regime_names)\n",
    "plt.title('Regime Detection Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_BASE}/results/regime_detector_confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=regime_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl_finetuning"
   },
   "source": [
    "## 7. Reinforcement Learning Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl_setup"
   },
   "outputs": [],
   "source": [
    "# RL fine-tuning setup\n",
    "from training.environment import MultiAgentTradingEnv\n",
    "from training.rewards import RegimeDetectionReward\n",
    "\n",
    "# Create simplified environment for single agent\n",
    "env_config = {\n",
    "    'n_agents': 1,\n",
    "    'episode_length': 100,\n",
    "    'initial_capital': 100000,\n",
    "    'transaction_cost': 0.001\n",
    "}\n",
    "\n",
    "env = MultiAgentTradingEnv(env_config)\n",
    "reward_fn = RegimeDetectionReward()\n",
    "\n",
    "# PPO setup for fine-tuning\n",
    "class RegimePPO:\n",
    "    def __init__(self, model, lr=3e-4):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.entropy_coef = 0.01\n",
    "    \n",
    "    def compute_returns(self, rewards, dones, values, next_value):\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R = rewards[step] + self.gamma * R * (1 - dones[step])\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        return torch.tensor(returns)\n",
    "    \n",
    "    def update(self, states, actions, returns, old_log_probs):\n",
    "        # Forward pass\n",
    "        output = self.model(states)\n",
    "        \n",
    "        # Compute new log probs and values\n",
    "        dist = torch.distributions.Categorical(logits=output['regime_logits'])\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        values = output.get('value', torch.zeros_like(returns))\n",
    "        \n",
    "        # PPO loss\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "        \n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = nn.MSELoss()(values, returns)\n",
    "        \n",
    "        loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item(), policy_loss.item(), value_loss.item()\n",
    "\n",
    "# Initialize PPO\n",
    "ppo = RegimePPO(model, lr=1e-4)\n",
    "\n",
    "print(\"‚úÖ RL fine-tuning setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl_training"
   },
   "outputs": [],
   "source": [
    "# RL fine-tuning loop (simplified)\n",
    "n_episodes = 100\n",
    "episode_rewards = []\n",
    "\n",
    "print(\"üöÄ Starting RL fine-tuning...\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Collect trajectory\n",
    "    states, actions, rewards, log_probs, dones = [], [], [], [], []\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from model\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "            output = model({'features': state_tensor})\n",
    "            \n",
    "            dist = torch.distributions.Categorical(logits=output['regime_logits'])\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Step environment\n",
    "        next_obs, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        # Store trajectory\n",
    "        states.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        dones.append(done)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    # Update model\n",
    "    if len(states) > 0:\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        actions_tensor = torch.cat(actions)\n",
    "        log_probs_tensor = torch.cat(log_probs)\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = ppo.compute_returns(rewards, dones, \n",
    "                                     torch.zeros(len(rewards)), 0)\n",
    "        \n",
    "        # Update\n",
    "        loss, p_loss, v_loss = ppo.update(\n",
    "            {'features': states_tensor},\n",
    "            actions_tensor,\n",
    "            returns.to(device),\n",
    "            log_probs_tensor\n",
    "        )\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"Episode {episode}: Avg Reward = {avg_reward:.4f}\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'episode': episode,\n",
    "    'avg_reward': np.mean(episode_rewards[-10:]),\n",
    "    'config': regime_config\n",
    "}, f\"{DRIVE_BASE}/models/regime_detector_finetuned.pt\")\n",
    "\n",
    "print(\"\\n‚úÖ RL fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_summary"
   },
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "summary = f\"\"\"\n",
    "# Regime Detector Training Summary\n",
    "\n",
    "## Training Details\n",
    "- Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- Device: {device}\n",
    "- Model Parameters: {total_params:,}\n",
    "\n",
    "## Supervised Pre-training\n",
    "- Epochs: {n_epochs}\n",
    "- Best Validation Accuracy: {best_val_acc:.2f}%\n",
    "- Test Accuracy: {test_acc:.2f}%\n",
    "\n",
    "## RL Fine-tuning\n",
    "- Episodes: {n_episodes}\n",
    "- Final Average Reward: {np.mean(episode_rewards[-10:]):.4f}\n",
    "\n",
    "## Model Files\n",
    "- Pre-trained: regime_detector_pretrained.pt\n",
    "- Fine-tuned: regime_detector_finetuned.pt\n",
    "\n",
    "## Performance by Regime\n",
    "\"\"\"\n",
    "\n",
    "# Add per-class accuracy\n",
    "for i, regime in enumerate(regime_names):\n",
    "    mask = np.array(all_labels) == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (np.array(all_preds)[mask] == i).mean() * 100\n",
    "        summary += f\"\\n- {regime}: {acc:.1f}% accuracy\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{DRIVE_BASE}/results/regime_detector_training_summary.txt\", 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Model saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}