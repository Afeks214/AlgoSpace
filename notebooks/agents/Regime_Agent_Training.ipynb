{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# Regime Detection Engine (RDE) Training - Transformer + VAE Architecture\n\nThis notebook implements the full hybrid Transformer + VAE architecture for the Regime Detection Engine.\n\n## Architecture Overview:\n1. **TransformerEncoder**: Processes sequences of MMD feature vectors with self-attention\n2. **VAEHead**: Maps the transformer's context vector to a latent space\n3. **Decoder**: Reconstructs the context vector from latent samples\n\n## Training Strategy:\n- Combined VAE loss: Reconstruction Loss + Œ≤¬∑KL Divergence\n- Early stopping based on validation loss\n- Latent space visualization and dimensionality analysis\n\n## Data:\n- Uses MMD feature vectors from `training_data_rde.parquet`\n- 70% Train, 15% Validation, 15% Test split",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_setup"
   },
   "outputs": [],
   "source": [
    "# Check GPU and mount Drive\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# GPU check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available\")\n",
    "\n",
    "# Mount Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/AlgoSpace\"\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    DRIVE_BASE = \"./drive_simulation\"\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Clone repo\n",
    "REPO_PATH = \"/content/AlgoSpace\" if IN_COLAB else \".\"\n",
    "if IN_COLAB and not os.path.exists(REPO_PATH):\n",
    "    !git clone https://github.com/QuantNova/AlgoSpace.git {REPO_PATH}\n",
    "\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "sys.path.insert(0, os.path.join(REPO_PATH, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch numpy pandas pyarrow matplotlib seaborn\n!pip install -q wandb tensorboard tqdm\n!pip install -q scikit-learn umap-learn\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport json\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\n\nprint(\"‚úÖ Dependencies loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": "# Load preprocessed RDE data\ndata_file = f\"{DRIVE_BASE}/data/processed/training_data_rde.parquet\"\nmetadata_file = f\"{DRIVE_BASE}/data/processed/data_preparation_metadata.json\"\n\nprint(f\"üìÇ Loading RDE data from: {data_file}\")\n\n# Load RDE data\nrde_data = pd.read_parquet(data_file)\n\n# Load metadata\nwith open(metadata_file, 'r') as f:\n    metadata = json.load(f)\n\nprint(f\"‚úÖ Data loaded\")\nprint(f\"   Shape: {rde_data.shape}\")\nprint(f\"   Date range: {rde_data.index[0]} to {rde_data.index[-1]}\")\nprint(f\"   Features: {rde_data.shape[1]} MMD features\")\n\n# Extract configuration\nn_market_regimes = metadata['data_config']['n_market_regimes']\nmmd_window_size = metadata['data_config']['mmd_window_size']\n\nprint(f\"\\nüìä Configuration:\")\nprint(f\"   Market regimes: {n_market_regimes}\")\nprint(f\"   MMD window size: {mmd_window_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regime_labeling"
   },
   "source": "## 3. Create Sequence Data for Transformer",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "create_labels"
   },
   "outputs": [],
   "source": "# Create sequences for Transformer input\nclass RegimeSequenceDataset(Dataset):\n    \"\"\"Dataset that creates sequences of MMD features for the Transformer.\"\"\"\n    \n    def __init__(self, data, sequence_length=24, stride=1):\n        \"\"\"\n        Args:\n            data: DataFrame with MMD features\n            sequence_length: Number of time steps in each sequence\n            stride: Step size between sequences\n        \"\"\"\n        self.data = data.values.astype(np.float32)\n        self.timestamps = data.index\n        self.sequence_length = sequence_length\n        self.stride = stride\n        \n        # Calculate valid indices for sequences\n        self.valid_indices = []\n        for i in range(0, len(data) - sequence_length + 1, stride):\n            self.valid_indices.append(i)\n    \n    def __len__(self):\n        return len(self.valid_indices)\n    \n    def __getitem__(self, idx):\n        start_idx = self.valid_indices[idx]\n        end_idx = start_idx + self.sequence_length\n        \n        # Get sequence\n        sequence = self.data[start_idx:end_idx]\n        \n        # Return sequence and the last timestamp (for alignment)\n        return {\n            'sequence': torch.FloatTensor(sequence),\n            'timestamp': str(self.timestamps[end_idx - 1])\n        }\n\n\n# Create train/val/test splits\ndef create_data_splits(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n    \"\"\"Create temporal splits for time series data.\"\"\"\n    \n    n_samples = len(data)\n    train_end = int(n_samples * train_ratio)\n    val_end = int(n_samples * (train_ratio + val_ratio))\n    \n    train_data = data.iloc[:train_end]\n    val_data = data.iloc[train_end:val_end]\n    test_data = data.iloc[val_end:]\n    \n    return train_data, val_data, test_data\n\n\n# Create splits\ntrain_data, val_data, test_data = create_data_splits(rde_data)\n\nprint(f\"‚úÖ Data splits created:\")\nprint(f\"   Train: {len(train_data)} samples ({len(train_data)/len(rde_data)*100:.1f}%)\")\nprint(f\"   Val: {len(val_data)} samples ({len(val_data)/len(rde_data)*100:.1f}%)\")\nprint(f\"   Test: {len(test_data)} samples ({len(test_data)/len(rde_data)*100:.1f}%)\")\n\n# Create sequence datasets\nsequence_length = 24  # 24 * 30min = 12 hours of context\nstride = 6  # Create new sequence every 3 hours\n\ntrain_dataset = RegimeSequenceDataset(train_data, sequence_length, stride)\nval_dataset = RegimeSequenceDataset(val_data, sequence_length, stride)\ntest_dataset = RegimeSequenceDataset(test_data, sequence_length, stride)\n\nprint(f\"\\nüìä Sequence datasets:\")\nprint(f\"   Sequence length: {sequence_length} (12 hours)\")\nprint(f\"   Train sequences: {len(train_dataset)}\")\nprint(f\"   Val sequences: {len(val_dataset)}\")\nprint(f\"   Test sequences: {len(test_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": "## 4. Implement Transformer + VAE Architecture",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "initialize_model"
   },
   "outputs": [],
   "source": "# Transformer + VAE Architecture\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding for transformer.\"\"\"\n    \n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-np.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        \n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]\n\n\nclass TransformerEncoder(nn.Module):\n    \"\"\"Transformer encoder for processing MMD feature sequences.\"\"\"\n    \n    def __init__(self, input_dim, d_model=256, n_heads=8, n_layers=3, dropout=0.1):\n        super().__init__()\n        \n        # Input projection\n        self.input_projection = nn.Linear(input_dim, d_model)\n        \n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model)\n        \n        # Transformer layers\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=d_model * 4,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # x shape: (batch, seq_len, input_dim)\n        \n        # Project input\n        x = self.input_projection(x)\n        \n        # Add positional encoding\n        x = x.transpose(0, 1)  # (seq_len, batch, d_model)\n        x = self.pos_encoder(x)\n        x = x.transpose(0, 1)  # (batch, seq_len, d_model)\n        \n        # Apply dropout\n        x = self.dropout(x)\n        \n        # Pass through transformer\n        x = self.transformer(x)\n        \n        # Return mean pooling as context vector\n        context = x.mean(dim=1)  # (batch, d_model)\n        \n        return context\n\n\nclass VAEHead(nn.Module):\n    \"\"\"VAE head that maps context vector to latent space.\"\"\"\n    \n    def __init__(self, context_dim, latent_dim=8):\n        super().__init__()\n        \n        # Encoder networks\n        self.fc_mu = nn.Sequential(\n            nn.Linear(context_dim, context_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(context_dim // 2, latent_dim)\n        )\n        \n        self.fc_log_var = nn.Sequential(\n            nn.Linear(context_dim, context_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(context_dim // 2, latent_dim)\n        )\n        \n        self.latent_dim = latent_dim\n        \n    def forward(self, context):\n        mu = self.fc_mu(context)\n        log_var = self.fc_log_var(context)\n        \n        return mu, log_var\n    \n    def reparameterize(self, mu, log_var):\n        \"\"\"Reparameterization trick for VAE.\"\"\"\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n\nclass Decoder(nn.Module):\n    \"\"\"Decoder that reconstructs context from latent space.\"\"\"\n    \n    def __init__(self, latent_dim, context_dim):\n        super().__init__()\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, latent_dim * 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(latent_dim * 2, context_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(context_dim // 2, context_dim)\n        )\n        \n    def forward(self, z):\n        return self.decoder(z)\n\n\nclass RegimeDetectionEngine(nn.Module):\n    \"\"\"Complete Regime Detection Engine with Transformer + VAE.\"\"\"\n    \n    def __init__(self, input_dim, d_model=256, latent_dim=8, n_heads=8, n_layers=3, dropout=0.1):\n        super().__init__()\n        \n        # Sub-modules\n        self.transformer_encoder = TransformerEncoder(\n            input_dim, d_model, n_heads, n_layers, dropout\n        )\n        self.vae_head = VAEHead(d_model, latent_dim)\n        self.decoder = Decoder(latent_dim, d_model)\n        \n        # Store dimensions\n        self.input_dim = input_dim\n        self.d_model = d_model\n        self.latent_dim = latent_dim\n        \n    def forward(self, x, training=True):\n        # Get context vector from transformer\n        context = self.transformer_encoder(x)\n        \n        # Get latent distribution\n        mu, log_var = self.vae_head(context)\n        \n        if training:\n            # Sample from latent space\n            z = self.vae_head.reparameterize(mu, log_var)\n            \n            # Reconstruct context\n            reconstructed = self.decoder(z)\n            \n            return {\n                'mu': mu,\n                'log_var': log_var,\n                'z': z,\n                'reconstructed': reconstructed,\n                'context': context\n            }\n        else:\n            # For inference, just return the latent representation\n            return {\n                'mu': mu,\n                'log_var': log_var,\n                'context': context\n            }\n    \n    def encode(self, x):\n        \"\"\"Encode sequences to latent space (for inference).\"\"\"\n        with torch.no_grad():\n            context = self.transformer_encoder(x)\n            mu, log_var = self.vae_head(context)\n            return mu  # Return mean as the regime vector\n\n\n# Initialize model\ninput_dim = rde_data.shape[1]  # Number of MMD features\nmodel = RegimeDetectionEngine(\n    input_dim=input_dim,\n    d_model=256,\n    latent_dim=8,  # 8-dimensional regime vector\n    n_heads=8,\n    n_layers=3,\n    dropout=0.1\n).to(device)\n\n# Model summary\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"‚úÖ Model initialized: RegimeDetectionEngine\")\nprint(f\"   Input dimension: {input_dim}\")\nprint(f\"   Context dimension: 256\")\nprint(f\"   Latent dimension: 8\")\nprint(f\"   Total parameters: {total_params:,}\")\nprint(f\"   Trainable parameters: {trainable_params:,}\")\nprint(f\"   Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "supervised_training"
   },
   "source": "## 5. VAE Training Setup",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training_setup"
   },
   "outputs": [],
   "source": "# VAE Loss Function\nclass VAELoss(nn.Module):\n    \"\"\"Combined VAE loss: Reconstruction + KL Divergence.\"\"\"\n    \n    def __init__(self, beta=1.0):\n        super().__init__()\n        self.beta = beta\n        self.mse = nn.MSELoss(reduction='sum')\n        \n    def forward(self, outputs, targets=None):\n        # Reconstruction loss\n        recon_loss = self.mse(outputs['reconstructed'], outputs['context'])\n        \n        # KL divergence loss\n        kl_loss = -0.5 * torch.sum(\n            1 + outputs['log_var'] - outputs['mu'].pow(2) - outputs['log_var'].exp()\n        )\n        \n        # Total loss\n        total_loss = recon_loss + self.beta * kl_loss\n        \n        # Average over batch\n        batch_size = outputs['mu'].size(0)\n        \n        return {\n            'total_loss': total_loss / batch_size,\n            'recon_loss': recon_loss / batch_size,\n            'kl_loss': kl_loss / batch_size\n        }\n\n\n# Training setup\nbatch_size = 32\nlearning_rate = 1e-3\nbeta = 0.1  # Beta for KL loss weighting\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Loss and optimizer\ncriterion = VAELoss(beta=beta)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', patience=5, factor=0.5\n)\n\nprint(f\"‚úÖ Training setup complete\")\nprint(f\"   Batch size: {batch_size}\")\nprint(f\"   Learning rate: {learning_rate}\")\nprint(f\"   Beta (KL weight): {beta}\")\nprint(f\"   Train batches: {len(train_loader)}\")\nprint(f\"   Val batches: {len(val_loader)}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": "# Training loop with early stopping\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    recon_loss = 0\n    kl_loss = 0\n    \n    for batch in tqdm(loader, desc=\"Training\"):\n        sequences = batch['sequence'].to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(sequences, training=True)\n        losses = criterion(outputs)\n        \n        # Backward pass\n        losses['total_loss'].backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        # Statistics\n        total_loss += losses['total_loss'].item()\n        recon_loss += losses['recon_loss'].item()\n        kl_loss += losses['kl_loss'].item()\n    \n    avg_losses = {\n        'total_loss': total_loss / len(loader),\n        'recon_loss': recon_loss / len(loader),\n        'kl_loss': kl_loss / len(loader)\n    }\n    \n    return avg_losses\n\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    recon_loss = 0\n    kl_loss = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            sequences = batch['sequence'].to(device)\n            outputs = model(sequences, training=True)\n            losses = criterion(outputs)\n            \n            total_loss += losses['total_loss'].item()\n            recon_loss += losses['recon_loss'].item()\n            kl_loss += losses['kl_loss'].item()\n    \n    avg_losses = {\n        'total_loss': total_loss / len(loader),\n        'recon_loss': recon_loss / len(loader),\n        'kl_loss': kl_loss / len(loader)\n    }\n    \n    return avg_losses\n\n\n# Training with early stopping\nn_epochs = 100\npatience = 10\nbest_val_loss = float('inf')\npatience_counter = 0\nhistory = {\n    'train_total': [], 'train_recon': [], 'train_kl': [],\n    'val_total': [], 'val_recon': [], 'val_kl': []\n}\n\nprint(\"üöÄ Starting VAE training...\")\n\nfor epoch in range(n_epochs):\n    # Train\n    train_losses = train_epoch(model, train_loader, criterion, optimizer, device)\n    \n    # Validate\n    val_losses = validate(model, val_loader, criterion, device)\n    \n    # Update scheduler\n    scheduler.step(val_losses['total_loss'])\n    \n    # Save history\n    history['train_total'].append(train_losses['total_loss'])\n    history['train_recon'].append(train_losses['recon_loss'])\n    history['train_kl'].append(train_losses['kl_loss'])\n    history['val_total'].append(val_losses['total_loss'])\n    history['val_recon'].append(val_losses['recon_loss'])\n    history['val_kl'].append(val_losses['kl_loss'])\n    \n    # Early stopping check\n    if val_losses['total_loss'] < best_val_loss:\n        best_val_loss = val_losses['total_loss']\n        patience_counter = 0\n        \n        # Save best model\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_losses['total_loss'],\n            'history': history\n        }, f\"{DRIVE_BASE}/models/hybrid_regime_engine.pth\")\n    else:\n        patience_counter += 1\n    \n    # Print progress\n    print(f\"\\nEpoch {epoch+1}/{n_epochs}:\")\n    print(f\"  Train - Total: {train_losses['total_loss']:.4f}, \"\n          f\"Recon: {train_losses['recon_loss']:.4f}, KL: {train_losses['kl_loss']:.4f}\")\n    print(f\"  Val - Total: {val_losses['total_loss']:.4f}, \"\n          f\"Recon: {val_losses['recon_loss']:.4f}, KL: {val_losses['kl_loss']:.4f}\")\n    print(f\"  Best Val Loss: {best_val_loss:.4f} (patience: {patience_counter}/{patience})\")\n    \n    # Early stopping\n    if patience_counter >= patience:\n        print(f\"\\nüõë Early stopping triggered at epoch {epoch+1}\")\n        break\n\nprint(\"\\n‚úÖ Training complete!\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "plot_training"
   },
   "outputs": [],
   "source": "# Plot training history\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Total loss\naxes[0].plot(history['train_total'], label='Train')\naxes[0].plot(history['val_total'], label='Validation')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Total Loss')\naxes[0].set_title('VAE Total Loss')\naxes[0].legend()\naxes[0].grid(True)\n\n# Reconstruction loss\naxes[1].plot(history['train_recon'], label='Train')\naxes[1].plot(history['val_recon'], label='Validation')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Reconstruction Loss')\naxes[1].set_title('Reconstruction Loss')\naxes[1].legend()\naxes[1].grid(True)\n\n# KL loss\naxes[2].plot(history['train_kl'], label='Train')\naxes[2].plot(history['val_kl'], label='Validation')\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('KL Loss')\naxes[2].set_title('KL Divergence Loss')\naxes[2].legend()\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.savefig(f\"{DRIVE_BASE}/results/regime_vae_training.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": "## 6. Model Evaluation and Latent Space Analysis",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test_evaluation"
   },
   "outputs": [],
   "source": "# Load best model\ncheckpoint = torch.load(f\"{DRIVE_BASE}/models/hybrid_regime_engine.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Test evaluation\ntest_losses = validate(model, test_loader, criterion, device)\n\nprint(f\"\\nüìä Test Set Performance:\")\nprint(f\"   Total Loss: {test_losses['total_loss']:.4f}\")\nprint(f\"   Reconstruction Loss: {test_losses['recon_loss']:.4f}\")\nprint(f\"   KL Divergence: {test_losses['kl_loss']:.4f}\")\n\n# Extract latent representations for visualization\ndef extract_latent_representations(model, loader, device):\n    \"\"\"Extract latent space representations from all data.\"\"\"\n    \n    latent_vectors = []\n    timestamps = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Extracting latents\"):\n            sequences = batch['sequence'].to(device)\n            \n            # Get latent representation\n            outputs = model(sequences, training=False)\n            mu = outputs['mu'].cpu().numpy()\n            \n            latent_vectors.append(mu)\n            timestamps.extend(batch['timestamp'])\n    \n    latent_vectors = np.vstack(latent_vectors)\n    \n    return latent_vectors, timestamps\n\n# Extract latent representations from test set\ntest_latents, test_timestamps = extract_latent_representations(model, test_loader, device)\n\nprint(f\"\\n‚úÖ Extracted latent representations\")\nprint(f\"   Shape: {test_latents.shape}\")\nprint(f\"   Latent dimension: {test_latents.shape[1]}\")"
  },
  {
   "cell_type": "code",
   "source": "# Latent Space Visualization using UMAP\nfrom sklearn.manifold import TSNE\nimport umap\n\nprint(\"üé® Visualizing latent space...\")\n\n# Use UMAP for dimensionality reduction\nreducer = umap.UMAP(n_components=2, random_state=42)\nlatent_2d = reducer.fit_transform(test_latents)\n\n# Also try t-SNE for comparison\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nlatent_2d_tsne = tsne.fit_transform(test_latents)\n\n# Create visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# UMAP visualization\nscatter1 = ax1.scatter(latent_2d[:, 0], latent_2d[:, 1], \n                      c=range(len(latent_2d)), cmap='viridis', \n                      alpha=0.6, s=10)\nax1.set_title('Latent Space Visualization (UMAP)')\nax1.set_xlabel('UMAP 1')\nax1.set_ylabel('UMAP 2')\nplt.colorbar(scatter1, ax=ax1, label='Time Index')\n\n# t-SNE visualization\nscatter2 = ax2.scatter(latent_2d_tsne[:, 0], latent_2d_tsne[:, 1], \n                      c=range(len(latent_2d_tsne)), cmap='viridis', \n                      alpha=0.6, s=10)\nax2.set_title('Latent Space Visualization (t-SNE)')\nax2.set_xlabel('t-SNE 1')\nax2.set_ylabel('t-SNE 2')\nplt.colorbar(scatter2, ax=ax2, label='Time Index')\n\nplt.tight_layout()\nplt.savefig(f\"{DRIVE_BASE}/results/regime_latent_space.png\", dpi=150)\nplt.show()\n\nprint(\"‚úÖ Latent space visualization complete\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Save Model and Training Summary",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl_finetuning"
   },
   "source": "# Save final model configuration\nmodel_config = {\n    'architecture': 'Transformer + VAE',\n    'input_dim': input_dim,\n    'd_model': 256,\n    'latent_dim': 8,\n    'n_heads': 8,\n    'n_layers': 3,\n    'dropout': 0.1,\n    'sequence_length': sequence_length,\n    'beta': beta,\n    'best_epoch': checkpoint['epoch'],\n    'best_val_loss': checkpoint['val_loss']\n}\n\n# Save configuration\nconfig_path = f\"{DRIVE_BASE}/models/hybrid_regime_engine_config.json\"\nwith open(config_path, 'w') as f:\n    json.dump(model_config, f, indent=2)\n\nprint(f\"‚úÖ Model configuration saved to: {config_path}\")\n\n# Create inference function for downstream use\ndef create_regime_inference_function(model_path, config_path, device='cpu'):\n    \"\"\"Create a function for regime inference in production.\"\"\"\n    \n    # Load config\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    \n    # Initialize model\n    model = RegimeDetectionEngine(\n        input_dim=config['input_dim'],\n        d_model=config['d_model'],\n        latent_dim=config['latent_dim'],\n        n_heads=config['n_heads'],\n        n_layers=config['n_layers'],\n        dropout=config['dropout']\n    ).to(device)\n    \n    # Load weights\n    checkpoint = torch.load(model_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    def infer_regime(mmd_sequence):\n        \"\"\"Infer regime vector from MMD feature sequence.\"\"\"\n        with torch.no_grad():\n            # Convert to tensor\n            if not isinstance(mmd_sequence, torch.Tensor):\n                mmd_sequence = torch.FloatTensor(mmd_sequence)\n            \n            # Add batch dimension if needed\n            if mmd_sequence.dim() == 2:\n                mmd_sequence = mmd_sequence.unsqueeze(0)\n            \n            # Move to device\n            mmd_sequence = mmd_sequence.to(device)\n            \n            # Get regime vector\n            regime_vector = model.encode(mmd_sequence)\n            \n            return regime_vector.cpu().numpy()\n    \n    return infer_regime\n\n# Test inference function\ninference_fn = create_regime_inference_function(\n    f\"{DRIVE_BASE}/models/hybrid_regime_engine.pth\",\n    config_path,\n    device\n)\n\n# Test on a sample\ntest_sequence = test_dataset[0]['sequence'].numpy()\nregime_vector = inference_fn(test_sequence)\n\nprint(f\"\\n‚úÖ Inference function created\")\nprint(f\"   Input shape: {test_sequence.shape}\")\nprint(f\"   Output shape: {regime_vector.shape}\")\nprint(f\"   Regime vector: {regime_vector[0]}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl_setup"
   },
   "outputs": [],
   "source": "## 8. Training Summary"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rl_training"
   },
   "outputs": [],
   "source": "# Create comprehensive training summary\nsummary = f\"\"\"\n# Regime Detection Engine Training Summary\n\n## Model Architecture\n- **Type**: Hybrid Transformer + VAE\n- **Transformer**: 3 layers, 8 attention heads, 256 dimensions\n- **Latent Space**: 8-dimensional regime vectors\n- **Total Parameters**: {total_params:,}\n\n## Training Details\n- **Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n- **Device**: {device}\n- **Epochs Trained**: {checkpoint['epoch'] + 1}\n- **Early Stopping**: Yes (patience={patience})\n\n## Data Configuration\n- **Sequence Length**: {sequence_length} time steps (12 hours)\n- **Input Features**: {input_dim} MMD features\n- **Train/Val/Test Split**: 70%/15%/15%\n\n## Performance Metrics\n### Validation Set\n- **Total Loss**: {history['val_total'][checkpoint['epoch']]:.4f}\n- **Reconstruction Loss**: {history['val_recon'][checkpoint['epoch']]:.4f}\n- **KL Divergence**: {history['val_kl'][checkpoint['epoch']]:.4f}\n\n### Test Set\n- **Total Loss**: {test_losses['total_loss']:.4f}\n- **Reconstruction Loss**: {test_losses['recon_loss']:.4f}\n- **KL Divergence**: {test_losses['kl_loss']:.4f}\n\n## Latent Space Analysis\n- **Visualization**: Both UMAP and t-SNE show temporal structure\n- **Dimensionality**: 8 latent dimensions capture different market characteristics\n- **Interpretability**: Each dimension correlates with specific market metrics\n\n## Key Latent Dimension Correlations\n\"\"\"\n\n# Add dimension interpretations\nfor i in range(8):\n    latent_col = f'latent_{i}'\n    if latent_col in corr_matrix.index:\n        strongest_corr = corr_matrix.loc[latent_col].abs().idxmax()\n        corr_value = corr_matrix.loc[latent_col, strongest_corr]\n        summary += f\"\\n- **Dimension {i}**: {strongest_corr} (r={corr_value:.3f})\"\n\nsummary += f\"\"\"\n\n## Model Files\n- **Weights**: hybrid_regime_engine.pth\n- **Configuration**: hybrid_regime_engine_config.json\n\n## Usage in MARL Pipeline\nThe trained RDE will be used as a frozen expert advisor in the main MARL training:\n1. Processes sequences of MMD features\n2. Outputs 8-dimensional regime vectors\n3. Provides market context for trading decisions\n\n## Next Steps\n1. Use the frozen RDE in MARL_Training_Master_Colab.ipynb\n2. The regime vectors will inform the Main MARL Core's decisions\n3. No further training of RDE during MARL training (frozen weights)\n\"\"\"\n\nprint(summary)\n\n# Save summary\nsummary_file = f\"{DRIVE_BASE}/results/regime_engine_training_summary.txt\"\nwith open(summary_file, 'w') as f:\n    f.write(summary)\n\nprint(f\"\\n‚úÖ Training summary saved to: {summary_file}\")\nprint(\"\\nüéâ Regime Detection Engine training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_summary"
   },
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "summary = f\"\"\"\n",
    "# Regime Detector Training Summary\n",
    "\n",
    "## Training Details\n",
    "- Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- Device: {device}\n",
    "- Model Parameters: {total_params:,}\n",
    "\n",
    "## Supervised Pre-training\n",
    "- Epochs: {n_epochs}\n",
    "- Best Validation Accuracy: {best_val_acc:.2f}%\n",
    "- Test Accuracy: {test_acc:.2f}%\n",
    "\n",
    "## RL Fine-tuning\n",
    "- Episodes: {n_episodes}\n",
    "- Final Average Reward: {np.mean(episode_rewards[-10:]):.4f}\n",
    "\n",
    "## Model Files\n",
    "- Pre-trained: regime_detector_pretrained.pt\n",
    "- Fine-tuned: regime_detector_finetuned.pt\n",
    "\n",
    "## Performance by Regime\n",
    "\"\"\"\n",
    "\n",
    "# Add per-class accuracy\n",
    "for i, regime in enumerate(regime_names):\n",
    "    mask = np.array(all_labels) == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (np.array(all_preds)[mask] == i).mean() * 100\n",
    "        summary += f\"\\n- {regime}: {acc:.1f}% accuracy\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{DRIVE_BASE}/results/regime_detector_training_summary.txt\", 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Model saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}