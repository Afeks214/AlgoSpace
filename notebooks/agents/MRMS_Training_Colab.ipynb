{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-RMS (Multi-Agent Risk Management System) Training - Google Colab\n",
    "\n",
    "This notebook trains the M-RMS agent ensemble consisting of:\n",
    "- **PositionSizer**: Determines optimal position sizes based on risk\n",
    "- **StopLossAgent**: Sets dynamic stop-loss levels\n",
    "- **TakeProfitAgent**: Optimizes take-profit targets\n",
    "- **RiskCoordinator**: Ensemble coordinator for unified risk decisions\n",
    "\n",
    "Optimized for Google Colab Pro with GPU support and 24-hour runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and setup environment\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ðŸš€ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ðŸ’» Running locally\")\n",
    "\n",
    "# Mount Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set project path\n",
    "    PROJECT_PATH = Path('/content/drive/MyDrive/AlgoSpace-8')\n",
    "    sys.path.insert(0, str(PROJECT_PATH))\n",
    "else:\n",
    "    PROJECT_PATH = Path.cwd().parent.parent\n",
    "    sys.path.insert(0, str(PROJECT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install -q numpy pandas h5py pyyaml tensorboard wandb optuna mlflow\n",
    "    !pip install -q tqdm matplotlib seaborn scikit-learn psutil gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup imports\n",
    "from notebooks.utils.colab_setup import ColabSetup, SessionMonitor, setup_colab_training\n",
    "from notebooks.utils.drive_manager import DriveManager, DataStreamer\n",
    "from notebooks.utils.checkpoint_manager import CheckpointManager, CheckpointScheduler\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Colab environment\n",
    "if IN_COLAB:\n",
    "    colab_setup = setup_colab_training(\n",
    "        project_name=\"AlgoSpace-8\",\n",
    "        mount_drive=True,\n",
    "        setup_wandb=True,\n",
    "        keep_alive=True\n",
    "    )\n",
    "    \n",
    "    # Initialize managers\n",
    "    drive_manager = DriveManager(str(PROJECT_PATH))\n",
    "    checkpoint_manager = CheckpointManager(drive_manager)\n",
    "    session_monitor = SessionMonitor(max_runtime_hours=23.5)\n",
    "    \n",
    "    device = colab_setup.device\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"ðŸŽ® Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. M-RMS Agent Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MRMSConfig:\n",
    "    \"\"\"Configuration for M-RMS agents.\"\"\"\n",
    "    # Network dimensions\n",
    "    state_dim: int = 256  # From Main Core embeddings\n",
    "    hidden_dim: int = 128\n",
    "    action_dim: int = 10  # Discretized risk levels\n",
    "    \n",
    "    # Training parameters\n",
    "    learning_rate: float = 1e-4\n",
    "    batch_size: int = 256\n",
    "    gamma: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    \n",
    "    # Risk parameters\n",
    "    max_position_size: float = 0.1  # 10% max position\n",
    "    max_stop_loss: float = 0.02  # 2% max stop\n",
    "    max_take_profit: float = 0.05  # 5% max profit\n",
    "    \n",
    "    # Memory settings\n",
    "    memory_size: int = 100000\n",
    "    min_memory_size: int = 10000\n",
    "    \n",
    "    # Ensemble settings\n",
    "    ensemble_hidden_dim: int = 64\n",
    "    coordination_weight: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionSizer(nn.Module):\n",
    "    \"\"\"Determines optimal position sizes based on risk assessment.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MRMSConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Feature extraction\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(config.state_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(config.hidden_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(config.hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Risk assessment heads\n",
    "        self.volatility_head = nn.Linear(config.hidden_dim, 32)\n",
    "        self.confidence_head = nn.Linear(config.hidden_dim, 32)\n",
    "        \n",
    "        # Position size output\n",
    "        self.position_net = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim + 64, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Forward pass returning position sizes and risk metrics.\"\"\"\n",
    "        features = self.feature_net(state)\n",
    "        \n",
    "        # Assess risk factors\n",
    "        volatility = torch.sigmoid(self.volatility_head(features))\n",
    "        confidence = torch.sigmoid(self.confidence_head(features))\n",
    "        \n",
    "        # Combine for position sizing\n",
    "        risk_features = torch.cat([features, volatility, confidence], dim=-1)\n",
    "        position_probs = self.position_net(risk_features)\n",
    "        \n",
    "        # Convert to actual position sizes\n",
    "        position_sizes = position_probs * self.config.max_position_size\n",
    "        \n",
    "        metrics = {\n",
    "            'volatility': volatility.mean(dim=-1),\n",
    "            'confidence': confidence.mean(dim=-1),\n",
    "            'position_probs': position_probs\n",
    "        }\n",
    "        \n",
    "        return position_sizes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopLossAgent(nn.Module):\n",
    "    \"\"\"Sets dynamic stop-loss levels based on market conditions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MRMSConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Market condition analyzer\n",
    "        self.market_analyzer = nn.Sequential(\n",
    "            nn.Linear(config.state_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Volatility-based adjustment\n",
    "        self.volatility_net = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Sigmoid()  # 0-1 volatility score\n",
    "        )\n",
    "        \n",
    "        # Stop-loss level predictor\n",
    "        self.stop_loss_net = nn.Sequential(\n",
    "            nn.Linear(64 + 16, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(config.hidden_dim, config.action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, position_size: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Forward pass returning stop-loss levels.\"\"\"\n",
    "        # Analyze market conditions\n",
    "        market_features = self.market_analyzer(state)\n",
    "        volatility_score = self.volatility_net(market_features)\n",
    "        \n",
    "        # Combine features for stop-loss decision\n",
    "        combined = torch.cat([market_features, volatility_score], dim=-1)\n",
    "        stop_loss_probs = self.stop_loss_net(combined)\n",
    "        \n",
    "        # Convert to actual stop-loss percentages\n",
    "        stop_loss_levels = stop_loss_probs * self.config.max_stop_loss\n",
    "        \n",
    "        # Adjust based on position size if provided\n",
    "        if position_size is not None:\n",
    "            # Tighter stops for larger positions\n",
    "            size_factor = 1.0 - (position_size / self.config.max_position_size) * 0.3\n",
    "            stop_loss_levels = stop_loss_levels * size_factor\n",
    "        \n",
    "        metrics = {\n",
    "            'volatility_score': volatility_score.mean(dim=-1),\n",
    "            'stop_loss_probs': stop_loss_probs,\n",
    "            'adjusted_stops': stop_loss_levels\n",
    "        }\n",
    "        \n",
    "        return stop_loss_levels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TakeProfitAgent(nn.Module):\n",
    "    \"\"\"Optimizes take-profit targets based on market momentum.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MRMSConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Momentum analyzer\n",
    "        self.momentum_net = nn.Sequential(\n",
    "            nn.Linear(config.state_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Trend strength estimator\n",
    "        self.trend_net = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Tanh()  # -1 to 1 trend score\n",
    "        )\n",
    "        \n",
    "        # Take-profit predictor\n",
    "        self.profit_net = nn.Sequential(\n",
    "            nn.Linear(64 + 16, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(config.hidden_dim, config.action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, stop_loss: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Forward pass returning take-profit levels.\"\"\"\n",
    "        # Analyze momentum\n",
    "        momentum_features = self.momentum_net(state)\n",
    "        trend_score = self.trend_net(momentum_features)\n",
    "        \n",
    "        # Combine for profit target decision\n",
    "        combined = torch.cat([momentum_features, trend_score], dim=-1)\n",
    "        profit_probs = self.profit_net(combined)\n",
    "        \n",
    "        # Convert to actual take-profit percentages\n",
    "        take_profit_levels = profit_probs * self.config.max_take_profit\n",
    "        \n",
    "        # Ensure minimum risk-reward ratio\n",
    "        if stop_loss is not None:\n",
    "            min_rr_ratio = 2.0  # 2:1 risk-reward minimum\n",
    "            min_profit = stop_loss * min_rr_ratio\n",
    "            take_profit_levels = torch.maximum(take_profit_levels, min_profit)\n",
    "        \n",
    "        metrics = {\n",
    "            'trend_score': trend_score.mean(dim=-1),\n",
    "            'profit_probs': profit_probs,\n",
    "            'risk_reward_ratio': (take_profit_levels / (stop_loss + 1e-6)).mean() if stop_loss is not None else None\n",
    "        }\n",
    "        \n",
    "        return take_profit_levels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskCoordinator(nn.Module):\n",
    "    \"\"\"Ensemble coordinator for unified risk management decisions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MRMSConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Individual agent processors\n",
    "        self.position_processor = nn.Linear(config.action_dim, config.ensemble_hidden_dim)\n",
    "        self.stop_processor = nn.Linear(config.action_dim, config.ensemble_hidden_dim)\n",
    "        self.profit_processor = nn.Linear(config.action_dim, config.ensemble_hidden_dim)\n",
    "        \n",
    "        # Cross-agent attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=config.ensemble_hidden_dim,\n",
    "            num_heads=4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Final decision network\n",
    "        self.decision_net = nn.Sequential(\n",
    "            nn.Linear(config.ensemble_hidden_dim * 3, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, config.ensemble_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.ensemble_hidden_dim, 3)  # Scaling factors for each agent\n",
    "        )\n",
    "        \n",
    "    def forward(self, position_output: torch.Tensor, stop_output: torch.Tensor, \n",
    "                profit_output: torch.Tensor) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"Coordinate agent outputs into final risk parameters.\"\"\"\n",
    "        # Process individual outputs\n",
    "        pos_features = self.position_processor(position_output)\n",
    "        stop_features = self.stop_processor(stop_output)\n",
    "        profit_features = self.profit_processor(profit_output)\n",
    "        \n",
    "        # Stack for attention (seq_len, batch, features)\n",
    "        stacked = torch.stack([pos_features, stop_features, profit_features], dim=0)\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        attended, attention_weights = self.cross_attention(stacked, stacked, stacked)\n",
    "        \n",
    "        # Flatten and make final decision\n",
    "        flattened = attended.transpose(0, 1).reshape(position_output.size(0), -1)\n",
    "        coordination_factors = torch.sigmoid(self.decision_net(flattened))\n",
    "        \n",
    "        # Apply coordination\n",
    "        coordinated_position = position_output * coordination_factors[:, 0:1]\n",
    "        coordinated_stop = stop_output * coordination_factors[:, 1:2]\n",
    "        coordinated_profit = profit_output * coordination_factors[:, 2:3]\n",
    "        \n",
    "        final_params = {\n",
    "            'position_size': coordinated_position.max(dim=-1)[0],\n",
    "            'stop_loss': coordinated_stop.max(dim=-1)[0],\n",
    "            'take_profit': coordinated_profit.max(dim=-1)[0]\n",
    "        }\n",
    "        \n",
    "        return final_params, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. M-RMS Ensemble Training System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRMSEnsemble:\n",
    "    \"\"\"Complete M-RMS ensemble with training capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MRMSConfig, device: torch.device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.position_sizer = PositionSizer(config).to(device)\n",
    "        self.stop_loss_agent = StopLossAgent(config).to(device)\n",
    "        self.take_profit_agent = TakeProfitAgent(config).to(device)\n",
    "        self.coordinator = RiskCoordinator(config).to(device)\n",
    "        \n",
    "        # Target networks for stable training\n",
    "        self.target_position = PositionSizer(config).to(device)\n",
    "        self.target_stop = StopLossAgent(config).to(device)\n",
    "        self.target_profit = TakeProfitAgent(config).to(device)\n",
    "        \n",
    "        # Initialize target networks\n",
    "        self._update_targets(tau=1.0)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.position_opt = optim.Adam(self.position_sizer.parameters(), lr=config.learning_rate)\n",
    "        self.stop_opt = optim.Adam(self.stop_loss_agent.parameters(), lr=config.learning_rate)\n",
    "        self.profit_opt = optim.Adam(self.take_profit_agent.parameters(), lr=config.learning_rate)\n",
    "        self.coord_opt = optim.Adam(self.coordinator.parameters(), lr=config.learning_rate * 0.5)\n",
    "        \n",
    "        # Memory buffer\n",
    "        self.memory = []\n",
    "        \n",
    "        # Training metrics\n",
    "        self.metrics = {\n",
    "            'position_loss': [],\n",
    "            'stop_loss': [],\n",
    "            'profit_loss': [],\n",
    "            'coord_loss': [],\n",
    "            'risk_reward_ratio': [],\n",
    "            'position_accuracy': []\n",
    "        }\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass through entire ensemble.\"\"\"\n",
    "        # Get individual agent outputs\n",
    "        position_output, pos_metrics = self.position_sizer(state)\n",
    "        stop_output, stop_metrics = self.stop_loss_agent(state, position_output.max(dim=-1)[0])\n",
    "        profit_output, profit_metrics = self.take_profit_agent(state, stop_output.max(dim=-1)[0])\n",
    "        \n",
    "        # Coordinate decisions\n",
    "        final_params, attention = self.coordinator(position_output, stop_output, profit_output)\n",
    "        \n",
    "        # Add metrics\n",
    "        final_params['attention_weights'] = attention\n",
    "        final_params.update({f'pos_{k}': v for k, v in pos_metrics.items()})\n",
    "        final_params.update({f'stop_{k}': v for k, v in stop_metrics.items()})\n",
    "        final_params.update({f'profit_{k}': v for k, v in profit_metrics.items()})\n",
    "        \n",
    "        return final_params\n",
    "    \n",
    "    def remember(self, state: torch.Tensor, action: Dict[str, torch.Tensor], \n",
    "                 reward: float, next_state: torch.Tensor, done: bool):\n",
    "        \"\"\"Store experience in memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Limit memory size\n",
    "        if len(self.memory) > self.config.memory_size:\n",
    "            self.memory.pop(0)\n",
    "    \n",
    "    def train_step(self, batch_size: Optional[int] = None) -> Dict[str, float]:\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        if len(self.memory) < self.config.min_memory_size:\n",
    "            return {}\n",
    "        \n",
    "        batch_size = batch_size or self.config.batch_size\n",
    "        \n",
    "        # Sample batch\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        batch = [self.memory[i] for i in indices]\n",
    "        \n",
    "        # Prepare batch tensors\n",
    "        states = torch.stack([b[0] for b in batch]).to(self.device)\n",
    "        actions = {k: torch.stack([b[1][k] for b in batch]) for k in ['position_size', 'stop_loss', 'take_profit']}\n",
    "        rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.stack([b[3] for b in batch]).to(self.device)\n",
    "        dones = torch.tensor([b[4] for b in batch], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Train individual agents\n",
    "        losses = {}\n",
    "        \n",
    "        # Position Sizer\n",
    "        pos_loss = self._train_position_sizer(states, actions['position_size'], rewards, next_states, dones)\n",
    "        losses['position_loss'] = pos_loss\n",
    "        \n",
    "        # Stop Loss Agent\n",
    "        stop_loss = self._train_stop_loss(states, actions['stop_loss'], rewards, next_states, dones)\n",
    "        losses['stop_loss'] = stop_loss\n",
    "        \n",
    "        # Take Profit Agent\n",
    "        profit_loss = self._train_take_profit(states, actions['take_profit'], rewards, next_states, dones)\n",
    "        losses['profit_loss'] = profit_loss\n",
    "        \n",
    "        # Coordinator\n",
    "        coord_loss = self._train_coordinator(states, actions, rewards)\n",
    "        losses['coord_loss'] = coord_loss\n",
    "        \n",
    "        # Update target networks\n",
    "        self._update_targets()\n",
    "        \n",
    "        # Update metrics\n",
    "        for k, v in losses.items():\n",
    "            self.metrics[k].append(v)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _train_position_sizer(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Train position sizing agent.\"\"\"\n",
    "        self.position_opt.zero_grad()\n",
    "        \n",
    "        # Current Q values\n",
    "        current_output, _ = self.position_sizer(states)\n",
    "        current_q = (current_output * actions.unsqueeze(-1)).sum(dim=-1)\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_output, _ = self.target_position(next_states)\n",
    "            next_q = next_output.max(dim=-1)[0]\n",
    "            target_q = rewards + self.config.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Loss with risk penalty\n",
    "        base_loss = F.mse_loss(current_q, target_q)\n",
    "        risk_penalty = (actions - 0.05).clamp(min=0).mean() * 0.1  # Penalize large positions\n",
    "        \n",
    "        loss = base_loss + risk_penalty\n",
    "        loss.backward()\n",
    "        self.position_opt.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _train_stop_loss(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Train stop loss agent.\"\"\"\n",
    "        self.stop_opt.zero_grad()\n",
    "        \n",
    "        current_output, _ = self.stop_loss_agent(states)\n",
    "        current_q = (current_output * actions.unsqueeze(-1)).sum(dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_output, _ = self.target_stop(next_states)\n",
    "            next_q = next_output.max(dim=-1)[0]\n",
    "            target_q = rewards + self.config.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Loss with protection bonus\n",
    "        base_loss = F.mse_loss(current_q, target_q)\n",
    "        protection_bonus = -actions.mean() * 0.05  # Reward tighter stops\n",
    "        \n",
    "        loss = base_loss + protection_bonus\n",
    "        loss.backward()\n",
    "        self.stop_opt.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _train_take_profit(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Train take profit agent.\"\"\"\n",
    "        self.profit_opt.zero_grad()\n",
    "        \n",
    "        current_output, _ = self.take_profit_agent(states)\n",
    "        current_q = (current_output * actions.unsqueeze(-1)).sum(dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_output, _ = self.target_profit(next_states)\n",
    "            next_q = next_output.max(dim=-1)[0]\n",
    "            target_q = rewards + self.config.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        loss.backward()\n",
    "        self.profit_opt.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _train_coordinator(self, states, actions, rewards):\n",
    "        \"\"\"Train ensemble coordinator.\"\"\"\n",
    "        self.coord_opt.zero_grad()\n",
    "        \n",
    "        # Get current predictions\n",
    "        with torch.no_grad():\n",
    "            pos_out, _ = self.position_sizer(states)\n",
    "            stop_out, _ = self.stop_loss_agent(states)\n",
    "            profit_out, _ = self.take_profit_agent(states)\n",
    "        \n",
    "        # Coordinate\n",
    "        final_params, _ = self.coordinator(pos_out, stop_out, profit_out)\n",
    "        \n",
    "        # Loss based on reward alignment\n",
    "        predicted_rr = final_params['take_profit'] / (final_params['stop_loss'] + 1e-6)\n",
    "        rr_loss = F.mse_loss(predicted_rr, rewards / 10)  # Scale rewards\n",
    "        \n",
    "        # Consistency loss\n",
    "        consistency_loss = (\n",
    "            F.mse_loss(final_params['position_size'], actions['position_size']) +\n",
    "            F.mse_loss(final_params['stop_loss'], actions['stop_loss']) +\n",
    "            F.mse_loss(final_params['take_profit'], actions['take_profit'])\n",
    "        ) * 0.1\n",
    "        \n",
    "        loss = rr_loss + consistency_loss\n",
    "        loss.backward()\n",
    "        self.coord_opt.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _update_targets(self, tau: Optional[float] = None):\n",
    "        \"\"\"Soft update target networks.\"\"\"\n",
    "        tau = tau or self.config.tau\n",
    "        \n",
    "        for target_param, param in zip(self.target_position.parameters(), self.position_sizer.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "        for target_param, param in zip(self.target_stop.parameters(), self.stop_loss_agent.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "        for target_param, param in zip(self.target_profit.parameters(), self.take_profit_agent.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    def save_models(self, path: str):\n",
    "        \"\"\"Save all models.\"\"\"\n",
    "        torch.save({\n",
    "            'position_sizer': self.position_sizer.state_dict(),\n",
    "            'stop_loss_agent': self.stop_loss_agent.state_dict(),\n",
    "            'take_profit_agent': self.take_profit_agent.state_dict(),\n",
    "            'coordinator': self.coordinator.state_dict(),\n",
    "            'config': asdict(self.config)\n",
    "        }, path)\n",
    "    \n",
    "    def load_models(self, path: str):\n",
    "        \"\"\"Load all models.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.position_sizer.load_state_dict(checkpoint['position_sizer'])\n",
    "        self.stop_loss_agent.load_state_dict(checkpoint['stop_loss_agent'])\n",
    "        self.take_profit_agent.load_state_dict(checkpoint['take_profit_agent'])\n",
    "        self.coordinator.load_state_dict(checkpoint['coordinator'])\n",
    "        \n",
    "        # Update targets\n",
    "        self._update_targets(tau=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Environment & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskEnvironment:\n",
    "    \"\"\"Training environment for M-RMS agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, config: MRMSConfig):\n",
    "        self.config = config\n",
    "        self.data = self._load_data(data_path)\n",
    "        self.current_idx = 0\n",
    "        self.episode_return = 0\n",
    "        self.episode_length = 0\n",
    "        \n",
    "    def _load_data(self, path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load training data.\"\"\"\n",
    "        # This would load actual market data\n",
    "        # For now, create synthetic data\n",
    "        n_samples = 100000\n",
    "        \n",
    "        data = pd.DataFrame({\n",
    "            'timestamp': pd.date_range('2020-01-01', periods=n_samples, freq='5min'),\n",
    "            'price': 100 * (1 + np.random.randn(n_samples).cumsum() * 0.001),\n",
    "            'volume': np.random.lognormal(10, 1, n_samples),\n",
    "            'volatility': np.abs(np.random.randn(n_samples) * 0.01),\n",
    "            'momentum': np.random.randn(n_samples) * 0.1\n",
    "        })\n",
    "        \n",
    "        # Add features\n",
    "        data['returns'] = data['price'].pct_change()\n",
    "        data['log_volume'] = np.log1p(data['volume'])\n",
    "        data['price_ma'] = data['price'].rolling(20).mean()\n",
    "        data['vol_ma'] = data['volatility'].rolling(10).mean()\n",
    "        \n",
    "        return data.dropna()\n",
    "    \n",
    "    def reset(self) -> torch.Tensor:\n",
    "        \"\"\"Reset environment and return initial state.\"\"\"\n",
    "        self.current_idx = np.random.randint(1000, len(self.data) - 1000)\n",
    "        self.episode_return = 0\n",
    "        self.episode_length = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self) -> torch.Tensor:\n",
    "        \"\"\"Get current state representation.\"\"\"\n",
    "        # In real implementation, this would use embeddings from Main Core\n",
    "        # For now, create a dummy state vector\n",
    "        state_vector = torch.randn(self.config.state_dim)\n",
    "        return state_vector\n",
    "    \n",
    "    def step(self, action: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, float, bool, Dict]:\n",
    "        \"\"\"Execute action and return next state, reward, done, info.\"\"\"\n",
    "        self.current_idx += 1\n",
    "        self.episode_length += 1\n",
    "        \n",
    "        # Calculate reward based on risk-adjusted returns\n",
    "        current_price = self.data.iloc[self.current_idx]['price']\n",
    "        next_price = self.data.iloc[self.current_idx + 1]['price']\n",
    "        price_change = (next_price - current_price) / current_price\n",
    "        \n",
    "        # Position-weighted return\n",
    "        position_size = action['position_size'].item()\n",
    "        gross_return = price_change * position_size\n",
    "        \n",
    "        # Apply stop loss and take profit\n",
    "        stop_loss = action['stop_loss'].item()\n",
    "        take_profit = action['take_profit'].item()\n",
    "        \n",
    "        if price_change <= -stop_loss:\n",
    "            actual_return = -stop_loss * position_size\n",
    "        elif price_change >= take_profit:\n",
    "            actual_return = take_profit * position_size\n",
    "        else:\n",
    "            actual_return = gross_return\n",
    "        \n",
    "        # Risk-adjusted reward\n",
    "        volatility = self.data.iloc[self.current_idx]['volatility']\n",
    "        sharpe_component = actual_return / (volatility + 1e-6)\n",
    "        \n",
    "        # Reward includes return, risk adjustment, and cost\n",
    "        trading_cost = position_size * 0.0002  # 2 bps\n",
    "        reward = sharpe_component - trading_cost\n",
    "        \n",
    "        # Add risk management bonus/penalty\n",
    "        rr_ratio = take_profit / (stop_loss + 1e-6)\n",
    "        if rr_ratio >= 2.0:\n",
    "            reward += 0.01  # Bonus for good risk-reward\n",
    "        \n",
    "        self.episode_return += actual_return\n",
    "        \n",
    "        # Check if done\n",
    "        done = (\n",
    "            self.episode_length >= 1000 or\n",
    "            self.current_idx >= len(self.data) - 100 or\n",
    "            self.episode_return <= -0.1  # 10% drawdown\n",
    "        )\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        info = {\n",
    "            'return': actual_return,\n",
    "            'position_size': position_size,\n",
    "            'stop_loss': stop_loss,\n",
    "            'take_profit': take_profit,\n",
    "            'rr_ratio': rr_ratio,\n",
    "            'sharpe': sharpe_component\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mrms_ensemble(config: MRMSConfig, \n",
    "                       n_episodes: int = 10000,\n",
    "                       save_interval: int = 100,\n",
    "                       eval_interval: int = 50):\n",
    "    \"\"\"Main training loop for M-RMS ensemble.\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    ensemble = MRMSEnsemble(config, device)\n",
    "    env = RiskEnvironment('data/market_data.h5', config)\n",
    "    \n",
    "    # Training tracking\n",
    "    episode_rewards = []\n",
    "    episode_returns = []\n",
    "    training_losses = []\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    start_episode = 0\n",
    "    if IN_COLAB:\n",
    "        resume_info = checkpoint_manager.get_resume_info()\n",
    "        if resume_info['available']:\n",
    "            print(f\"ðŸ“‚ Resuming from episode {resume_info['episode']}\")\n",
    "            checkpoint = checkpoint_manager.load_latest()\n",
    "            ensemble.load_models(checkpoint['state']['model_path'])\n",
    "            start_episode = resume_info['episode']\n",
    "    \n",
    "    # Training loop\n",
    "    pbar = tqdm(range(start_episode, n_episodes), desc=\"Training M-RMS\")\n",
    "    \n",
    "    for episode in pbar:\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_info = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            # Get action from ensemble\n",
    "            with torch.no_grad():\n",
    "                action = ensemble.forward(state.unsqueeze(0).to(device))\n",
    "                \n",
    "            # Clean action for environment\n",
    "            env_action = {\n",
    "                'position_size': action['position_size'],\n",
    "                'stop_loss': action['stop_loss'],\n",
    "                'take_profit': action['take_profit']\n",
    "            }\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, done, info = env.step(env_action)\n",
    "            \n",
    "            # Store experience\n",
    "            ensemble.remember(state, env_action, reward, next_state, done)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_info.append(info)\n",
    "            \n",
    "            # Train if enough experience\n",
    "            if len(ensemble.memory) >= config.min_memory_size:\n",
    "                losses = ensemble.train_step()\n",
    "                if losses:\n",
    "                    training_losses.append(losses)\n",
    "        \n",
    "        # Record episode metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_returns.append(env.episode_return)\n",
    "        \n",
    "        # Calculate episode statistics\n",
    "        avg_position = np.mean([info['position_size'] for info in episode_info])\n",
    "        avg_rr_ratio = np.mean([info['rr_ratio'] for info in episode_info])\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'reward': f\"{episode_reward:.4f}\",\n",
    "            'return': f\"{env.episode_return:.4f}\",\n",
    "            'position': f\"{avg_position:.3f}\",\n",
    "            'RR': f\"{avg_rr_ratio:.2f}\"\n",
    "        })\n",
    "        \n",
    "        # Evaluation\n",
    "        if episode % eval_interval == 0:\n",
    "            eval_metrics = evaluate_ensemble(ensemble, env, n_episodes=10)\n",
    "            print(f\"\\nðŸ“Š Episode {episode} Evaluation:\")\n",
    "            print(f\"   Avg Return: {eval_metrics['avg_return']:.4f}\")\n",
    "            print(f\"   Sharpe Ratio: {eval_metrics['sharpe_ratio']:.3f}\")\n",
    "            print(f\"   Max Drawdown: {eval_metrics['max_drawdown']:.4f}\")\n",
    "            print(f\"   Win Rate: {eval_metrics['win_rate']:.3f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if IN_COLAB and episode % save_interval == 0:\n",
    "            # Check session time\n",
    "            if session_monitor.is_ending_soon():\n",
    "                print(\"\\nâš ï¸ Session ending soon! Saving final checkpoint...\")\n",
    "                save_checkpoint = True\n",
    "            else:\n",
    "                save_checkpoint = checkpoint_manager.should_save(episode)\n",
    "            \n",
    "            if save_checkpoint:\n",
    "                # Save models\n",
    "                model_path = f\"/tmp/mrms_models_ep{episode}.pt\"\n",
    "                ensemble.save_models(model_path)\n",
    "                \n",
    "                # Create checkpoint\n",
    "                checkpoint_state = {\n",
    "                    'episode': episode,\n",
    "                    'model_path': model_path,\n",
    "                    'metrics': {\n",
    "                        'episode_reward': episode_reward,\n",
    "                        'episode_return': env.episode_return,\n",
    "                        'avg_position': avg_position,\n",
    "                        'avg_rr_ratio': avg_rr_ratio\n",
    "                    },\n",
    "                    'training_losses': training_losses[-100:]  # Last 100\n",
    "                }\n",
    "                \n",
    "                # Determine if best\n",
    "                is_best = episode_reward > checkpoint_manager.best_metric\n",
    "                if is_best:\n",
    "                    checkpoint_manager.best_metric = episode_reward\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint_manager.save(\n",
    "                    state=checkpoint_state,\n",
    "                    metrics=checkpoint_state['metrics'],\n",
    "                    is_best=is_best,\n",
    "                    tag='mrms_training'\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nðŸ’¾ Checkpoint saved (episode {episode}, best={is_best})\")\n",
    "            \n",
    "            # Check if should stop\n",
    "            if session_monitor.is_ending_soon(buffer_minutes=10):\n",
    "                print(\"\\nðŸ›‘ Stopping training - session ending in 10 minutes\")\n",
    "                break\n",
    "    \n",
    "    return ensemble, episode_rewards, episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(ensemble: MRMSEnsemble, env: RiskEnvironment, \n",
    "                     n_episodes: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate ensemble performance.\"\"\"\n",
    "    \n",
    "    ensemble.position_sizer.eval()\n",
    "    ensemble.stop_loss_agent.eval()\n",
    "    ensemble.take_profit_agent.eval()\n",
    "    ensemble.coordinator.eval()\n",
    "    \n",
    "    episode_returns = []\n",
    "    episode_lengths = []\n",
    "    all_returns = []\n",
    "    win_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_trades = []\n",
    "            \n",
    "            while not done:\n",
    "                action = ensemble.forward(state.unsqueeze(0).to(device))\n",
    "                \n",
    "                env_action = {\n",
    "                    'position_size': action['position_size'],\n",
    "                    'stop_loss': action['stop_loss'],\n",
    "                    'take_profit': action['take_profit']\n",
    "                }\n",
    "                \n",
    "                next_state, reward, done, info = env.step(env_action)\n",
    "                state = next_state\n",
    "                \n",
    "                episode_trades.append(info['return'])\n",
    "                all_returns.append(info['return'])\n",
    "                \n",
    "                if info['return'] > 0:\n",
    "                    win_count += 1\n",
    "            \n",
    "            episode_returns.append(env.episode_return)\n",
    "            episode_lengths.append(env.episode_length)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    returns_array = np.array(all_returns)\n",
    "    \n",
    "    metrics = {\n",
    "        'avg_return': np.mean(episode_returns),\n",
    "        'std_return': np.std(episode_returns),\n",
    "        'sharpe_ratio': np.mean(returns_array) / (np.std(returns_array) + 1e-6) * np.sqrt(252),\n",
    "        'max_drawdown': calculate_max_drawdown(np.cumsum(returns_array)),\n",
    "        'win_rate': win_count / len(all_returns),\n",
    "        'avg_episode_length': np.mean(episode_lengths)\n",
    "    }\n",
    "    \n",
    "    # Set back to training mode\n",
    "    ensemble.position_sizer.train()\n",
    "    ensemble.stop_loss_agent.train()\n",
    "    ensemble.take_profit_agent.train()\n",
    "    ensemble.coordinator.train()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_max_drawdown(cumulative_returns: np.ndarray) -> float:\n",
    "    \"\"\"Calculate maximum drawdown from cumulative returns.\"\"\"\n",
    "    running_max = np.maximum.accumulate(cumulative_returns)\n",
    "    drawdown = (cumulative_returns - running_max) / (running_max + 1e-6)\n",
    "    return np.min(drawdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = MRMSConfig(\n",
    "    state_dim=256,\n",
    "    hidden_dim=128,\n",
    "    action_dim=10,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    max_position_size=0.1,\n",
    "    max_stop_loss=0.02,\n",
    "    max_take_profit=0.05\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting M-RMS Ensemble Training\")\n",
    "print(f\"Configuration: {json.dumps(asdict(config), indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "ensemble, episode_rewards, episode_returns = train_mrms_ensemble(\n",
    "    config=config,\n",
    "    n_episodes=10000,\n",
    "    save_interval=100,\n",
    "    eval_interval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(episode_rewards)\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "\n",
    "# Episode returns\n",
    "axes[0, 1].plot(episode_returns)\n",
    "axes[0, 1].set_title('Episode Returns')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Return')\n",
    "\n",
    "# Training losses\n",
    "if ensemble.metrics['position_loss']:\n",
    "    axes[1, 0].plot(ensemble.metrics['position_loss'], label='Position')\n",
    "    axes[1, 0].plot(ensemble.metrics['stop_loss'], label='Stop Loss')\n",
    "    axes[1, 0].plot(ensemble.metrics['profit_loss'], label='Take Profit')\n",
    "    axes[1, 0].plot(ensemble.metrics['coord_loss'], label='Coordinator')\n",
    "    axes[1, 0].set_title('Training Losses')\n",
    "    axes[1, 0].set_xlabel('Training Step')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Risk metrics\n",
    "if ensemble.metrics['risk_reward_ratio']:\n",
    "    axes[1, 1].plot(ensemble.metrics['risk_reward_ratio'])\n",
    "    axes[1, 1].set_title('Risk-Reward Ratio')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('RR Ratio')\n",
    "    axes[1, 1].axhline(y=2.0, color='r', linestyle='--', label='Target RR')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nðŸ“Š Final Evaluation (100 episodes):\")\n",
    "final_metrics = evaluate_ensemble(ensemble, RiskEnvironment('data/market_data.h5', config), n_episodes=100)\n",
    "\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "if IN_COLAB:\n",
    "    print(\"\\nðŸ’¾ Saving final models to Drive...\")\n",
    "    \n",
    "    # Save ensemble\n",
    "    final_model_path = \"/tmp/mrms_ensemble_final.pt\"\n",
    "    ensemble.save_models(final_model_path)\n",
    "    \n",
    "    # Save to drive\n",
    "    models_dict = {\n",
    "        'position_sizer': ensemble.position_sizer,\n",
    "        'stop_loss_agent': ensemble.stop_loss_agent,\n",
    "        'take_profit_agent': ensemble.take_profit_agent,\n",
    "        'coordinator': ensemble.coordinator\n",
    "    }\n",
    "    \n",
    "    drive_manager.save_model(\n",
    "        models=models_dict,\n",
    "        name=\"mrms_ensemble\",\n",
    "        configs=asdict(config),\n",
    "        metrics=final_metrics,\n",
    "        production=True\n",
    "    )\n",
    "    \n",
    "    # Save training summary\n",
    "    summary = colab_setup.create_training_summary(\n",
    "        metrics=final_metrics,\n",
    "        save_path=str(drive_manager.results_path / \"mrms_training_summary.md\")\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… All models and results saved to Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration Testing with Main Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_integration_with_main_core():\n",
    "    \"\"\"Test M-RMS integration with Main MARL Core embeddings.\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”§ Testing M-RMS Integration...\")\n",
    "    \n",
    "    # Simulate Main Core embeddings\n",
    "    batch_size = 32\n",
    "    mock_embeddings = torch.randn(batch_size, config.state_dim).to(device)\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        risk_params = ensemble.forward(mock_embeddings)\n",
    "    \n",
    "    # Verify outputs\n",
    "    assert risk_params['position_size'].shape == (batch_size,)\n",
    "    assert risk_params['stop_loss'].shape == (batch_size,)\n",
    "    assert risk_params['take_profit'].shape == (batch_size,)\n",
    "    \n",
    "    # Check value ranges\n",
    "    assert torch.all(risk_params['position_size'] >= 0)\n",
    "    assert torch.all(risk_params['position_size'] <= config.max_position_size)\n",
    "    assert torch.all(risk_params['stop_loss'] >= 0)\n",
    "    assert torch.all(risk_params['stop_loss'] <= config.max_stop_loss)\n",
    "    assert torch.all(risk_params['take_profit'] >= 0)\n",
    "    assert torch.all(risk_params['take_profit'] <= config.max_take_profit)\n",
    "    \n",
    "    # Check risk-reward ratios\n",
    "    rr_ratios = risk_params['take_profit'] / (risk_params['stop_loss'] + 1e-6)\n",
    "    avg_rr = rr_ratios.mean().item()\n",
    "    \n",
    "    print(f\"âœ… Integration test passed!\")\n",
    "    print(f\"   Avg Position Size: {risk_params['position_size'].mean():.4f}\")\n",
    "    print(f\"   Avg Stop Loss: {risk_params['stop_loss'].mean():.4f}\")\n",
    "    print(f\"   Avg Take Profit: {risk_params['take_profit'].mean():.4f}\")\n",
    "    print(f\"   Avg RR Ratio: {avg_rr:.2f}\")\n",
    "\n",
    "# Run integration test\n",
    "test_integration_with_main_core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export models for production deployment\n",
    "if IN_COLAB:\n",
    "    print(\"\\nðŸ“¦ Exporting models for production...\")\n",
    "    \n",
    "    # Create TorchScript versions\n",
    "    ensemble.position_sizer.eval()\n",
    "    ensemble.stop_loss_agent.eval() \n",
    "    ensemble.take_profit_agent.eval()\n",
    "    ensemble.coordinator.eval()\n",
    "    \n",
    "    # Example inputs for tracing\n",
    "    example_state = torch.randn(1, config.state_dim).to(device)\n",
    "    example_position = torch.randn(1, config.action_dim).to(device)\n",
    "    example_stop = torch.randn(1, config.action_dim).to(device)\n",
    "    example_profit = torch.randn(1, config.action_dim).to(device)\n",
    "    \n",
    "    # Script models\n",
    "    scripted_models = {\n",
    "        'position_sizer': torch.jit.trace(ensemble.position_sizer, example_state),\n",
    "        'stop_loss_agent': torch.jit.trace(ensemble.stop_loss_agent, (example_state, torch.tensor([0.05]).to(device))),\n",
    "        'take_profit_agent': torch.jit.trace(ensemble.take_profit_agent, (example_state, torch.tensor([0.01]).to(device))),\n",
    "        'coordinator': torch.jit.trace(ensemble.coordinator, (example_position, example_stop, example_profit))\n",
    "    }\n",
    "    \n",
    "    # Save scripted models\n",
    "    production_dir = drive_manager.model_path / \"production\" / \"mrms_ensemble\"\n",
    "    production_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for name, model in scripted_models.items():\n",
    "        model.save(str(production_dir / f\"{name}_scripted.pt\"))\n",
    "    \n",
    "    # Create deployment package\n",
    "    deployment_package = drive_manager.create_training_package(\"mrms_deployment\")\n",
    "    \n",
    "    print(f\"âœ… Production models exported to: {production_dir}\")\n",
    "    print(f\"ðŸ“¦ Deployment package created: {deployment_package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully trained the M-RMS (Multi-Agent Risk Management System) ensemble consisting of:\n",
    "\n",
    "1. **PositionSizer**: Determines optimal position sizes based on market conditions\n",
    "2. **StopLossAgent**: Sets dynamic stop-loss levels for risk protection\n",
    "3. **TakeProfitAgent**: Optimizes take-profit targets based on momentum\n",
    "4. **RiskCoordinator**: Coordinates all agents for unified risk decisions\n",
    "\n",
    "The ensemble is now ready for integration with the Main MARL Core system and production deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}