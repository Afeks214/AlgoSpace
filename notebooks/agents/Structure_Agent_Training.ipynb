{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Example integration code\nintegration_example = \"\"\"\n# Integration with AlgoSpace MARL System\n\n## 1. Load the trained model in your trading system:\n\n```python\nfrom agents.marl.agents.structure_analyzer import StructureAnalyzer\nimport torch\n\n# Initialize structure analyzer\nconfig = {\n    'window': 48,\n    'input_features': 8,\n    'hidden_dim': 256,\n    'n_heads': 8,\n    'n_layers': 4,\n    'dropout': 0.1\n}\n\nstructure_agent = StructureAnalyzer(config)\n\n# Load pre-trained weights\ncheckpoint = torch.load('models/agents/structure_analyzer_finetuned.pt')\nstructure_agent.load_state_dict(checkpoint['model_state_dict'])\nstructure_agent.eval()\n```\n\n## 2. Use in MARL consensus:\n\n```python\nfrom training.marl_trainer import MARLTrainer\n\n# Configure MARL system\nmarl_config = {\n    'agents': {\n        'structure_analyzer': structure_agent,\n        'regime_detector': regime_agent,\n        'tactical_agent': tactical_agent\n    },\n    'consensus_weights': {\n        'structure_analyzer': 0.4,\n        'regime_detector': 0.3,\n        'tactical_agent': 0.3\n    }\n}\n\n# Initialize MARL trainer\ntrainer = MARLTrainer(marl_config)\n```\n\n## 3. Real-time trading integration:\n\n```python\n# Process incoming market data\nmarket_data_30m = matrix_assembler.process_30m_data(raw_data)\n\n# Get structure analysis\nwith torch.no_grad():\n    structure_output = structure_agent({\n        'market_matrix': market_data_30m,\n        'regime_embedding': regime_embedding,\n        'synergy_context': synergy_context\n    })\n\n# Extract trading signals\naction = structure_output['action']  # [pass, long, short]\nconfidence = structure_output['confidence']\nstructure_score = structure_agent._calculate_structure_score(synergy_context)\n\n# Use in decision making\nif confidence > 0.7 and structure_score > 0.6:\n    execute_trade(action, position_size=calculate_size(confidence))\n```\n\n## 4. Monitoring and logging:\n\n```python\n# Log structure metrics\nlogger.info(\n    \"Structure Analysis\",\n    action=action,\n    confidence=confidence,\n    structure_score=structure_score,\n    structure_type=get_structure_type(market_data_30m),\n    synergy_alignment=synergy_context['synergy_type']\n)\n```\n\"\"\"\n\nprint(integration_example)\n\n# Save integration guide\nwith open(RESULTS_PATH / 'integration_guide.md', 'w') as f:\n    f.write(integration_example)\n\nprint(\"\\n‚úÖ Structure Agent Training notebook complete!\")\nprint(\"\\nüìö Next Steps:\")\nprint(\"1. Review the training summary and metrics\")\nprint(\"2. Test the model with live market data\")\nprint(\"3. Integrate with MARL consensus mechanism\")\nprint(\"4. Monitor performance in production\")\nprint(\"5. Retrain periodically with new data\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Model Deployment Integration\n\nThis section shows how to integrate the trained Structure Analyzer into the AlgoSpace MARL system.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate comprehensive training summary\nsummary = f\"\"\"\n# Structure Analyzer Training Summary\n\n## Training Configuration\n- Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n- Device: {device}\n- Model Parameters: {total_params:,}\n- Window Size: {config['window']} bars (30-minute)\n- Input Features: {config['input_features']}\n\n## Supervised Pre-training Results\n- Epochs: {len(history['train_loss'])}\n- Best Validation F1: {best_val_f1:.4f}\n- Test Set Performance:\n  - Accuracy: {test_acc:.2f}%\n  - F1 Score: {test_f1:.4f}\n\n## Structure Detection Performance\n\"\"\"\n\n# Add per-class performance\nfor i, name in enumerate(structure_names):\n    class_mask = np.array(test_labels) == i\n    if class_mask.sum() > 0:\n        class_acc = (np.array(test_preds)[class_mask] == i).mean() * 100\n        summary += f\"- {name}: {class_acc:.1f}% accuracy\\n\"\n\nsummary += f\"\"\"\n## Reinforcement Learning Fine-tuning\n- Episodes: {n_episodes}\n- Final Average Return: {np.mean(episode_returns[-10:])*100:.2f}%\n- Final Win Rate: {np.mean(win_rates[-10:])*100:.1f}%\n- Average Trades per Episode: {np.mean(episode_lengths[-10:]):.1f}\n\n## Model Architecture\n- Embedder: Transformer-based (8 features ‚Üí 256 dim)\n- Attention: {config['n_heads']} heads, {config['n_layers']} layers\n- Policy Head: 3 actions (hold, long, short)\n- Classification Head: 4 structure types\n\n## Training Features\n- Volume Profile Analysis: ‚úì\n- Market Depth Proxy: ‚úì\n- Microstructure Features: ‚úì\n- Support/Resistance Detection: ‚úì\n- Trend Structure Evaluation: ‚úì\n\n## Output Files\n- Pre-trained Model: {MODELS_PATH}/structure_analyzer_pretrained.pt\n- Fine-tuned Model: {MODELS_PATH}/structure_analyzer_finetuned.pt\n- Training History: {RESULTS_PATH}/structure_training_history.png\n- Confusion Matrix: {RESULTS_PATH}/structure_confusion_matrix.png\n- RL Metrics: {RESULTS_PATH}/rl_training_metrics.png\n\n## Integration Notes\n- Compatible with MatrixAssembler30m output\n- Integrates with SynergyDetector patterns\n- Provides structure scores for MARL consensus\n- Weights 40% in multi-agent decision making\n\"\"\"\n\nprint(summary)\n\n# Save summary\nwith open(RESULTS_PATH / 'training_summary.txt', 'w') as f:\n    f.write(summary)\n\n# Export model configuration\nmodel_config = {\n    'architecture': 'StructureAnalyzer',\n    'config': config,\n    'input_shape': (48, 8),  # 30m bars √ó features\n    'output_heads': {\n        'action': 3,\n        'confidence': 1,\n        'reasoning': 64,\n        'structure_class': 4\n    },\n    'training_metrics': {\n        'pretrain_f1': best_val_f1,\n        'test_accuracy': test_acc,\n        'rl_return': np.mean(episode_returns[-10:]),\n        'rl_win_rate': np.mean(win_rates[-10:])\n    },\n    'synergy_integration': True,\n    'agent_weight': 0.4\n}\n\nwith open(MODELS_PATH / 'structure_analyzer_config.json', 'w') as f:\n    json.dump(model_config, f, indent=2)\n\nprint(\"\\n‚úÖ Training complete! All models and results saved.\")\nprint(f\"üìÅ Models directory: {MODELS_PATH}\")\nprint(f\"üìä Results directory: {RESULTS_PATH}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Training Summary and Model Export",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze learned features\ndef analyze_structure_features(model, dataset, n_samples=100):\n    \"\"\"Analyze what features the model has learned.\"\"\"\n    model.eval()\n    \n    feature_importance = {\n        'Strong Trend': [],\n        'Range Bound': [],\n        'Breakout': [],\n        'Reversal': []\n    }\n    \n    attention_patterns = {i: [] for i in range(4)}\n    \n    with torch.no_grad():\n        for idx in range(min(n_samples, len(dataset))):\n            sample = dataset[idx]\n            market_data = sample['market_data'].unsqueeze(0).to(device)\n            label = sample['label'].item()\n            \n            # Forward pass with attention\n            x = market_data.transpose(1, 2)\n            embedded = model.embedder(x)\n            \n            # Extract attention weights if available\n            if hasattr(model.embedder, 'attention_weights'):\n                attention = model.embedder.attention_weights\n                if attention is not None:\n                    attention_patterns[label].append(attention.cpu().numpy())\n            \n            # Get representation\n            representation = embedded.mean(dim=1)\n            \n            # Store features by structure type\n            structure_name = structure_names[label]\n            feature_importance[structure_name].append(representation.cpu().numpy())\n    \n    return feature_importance, attention_patterns\n\n# Run analysis\nprint(\"üîç Analyzing learned features...\")\nfeature_importance, attention_patterns = analyze_structure_features(model, test_dataset)\n\n# Visualize feature importance\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor idx, (structure_type, features) in enumerate(feature_importance.items()):\n    if features:\n        features_array = np.vstack(features)\n        mean_features = features_array.mean(axis=0)\n        std_features = features_array.std(axis=0)\n        \n        # Plot top 20 features\n        top_indices = np.argsort(np.abs(mean_features))[-20:]\n        \n        ax = axes[idx]\n        ax.barh(range(20), mean_features[top_indices], xerr=std_features[top_indices])\n        ax.set_yticks(range(20))\n        ax.set_yticklabels([f'F{i}' for i in top_indices])\n        ax.set_xlabel('Feature Importance')\n        ax.set_title(f'{structure_type} - Top Features', fontweight='bold')\n        ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'structure_feature_importance.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Feature Analysis and Interpretability",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot RL training results\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Episode returns\nax1 = axes[0]\nax1.plot(episode_returns, alpha=0.3, label='Episode Return')\nax1.plot(pd.Series(episode_returns).rolling(10).mean(), linewidth=2, label='10-Episode MA')\nax1.axhline(y=0, color='k', linestyle='--', alpha=0.5)\nax1.set_xlabel('Episode')\nax1.set_ylabel('Return (%)')\nax1.set_title('Episode Returns', fontsize=14, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Win rates\nax2 = axes[1]\nax2.plot(win_rates, alpha=0.3, label='Win Rate')\nax2.plot(pd.Series(win_rates).rolling(10).mean(), linewidth=2, label='10-Episode MA')\nax2.axhline(y=0.5, color='k', linestyle='--', alpha=0.5)\nax2.set_xlabel('Episode')\nax2.set_ylabel('Win Rate')\nax2.set_title('Trading Win Rate', fontsize=14, fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Trade frequency\nax3 = axes[2]\nax3.plot(episode_lengths, alpha=0.3, label='Trades per Episode')\nax3.plot(pd.Series(episode_lengths).rolling(10).mean(), linewidth=2, label='10-Episode MA')\nax3.set_xlabel('Episode')\nax3.set_ylabel('Number of Trades')\nax3.set_title('Trading Activity', fontsize=14, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'rl_training_metrics.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# RL training loop\nn_episodes = 100\nepisode_returns = []\nepisode_lengths = []\nwin_rates = []\n\n# Use test data for RL fine-tuning\nenv_data = test_dataset.data[0]  # Use first sample for demonstration\nenv = StructureTradingEnv(env_data)\n\nprint(\"\\nüöÄ Starting RL fine-tuning...\\n\")\n\nfor episode in range(n_episodes):\n    # Reset environment\n    obs = env.reset()\n    done = False\n    trajectory = {\n        'states': [],\n        'actions': [],\n        'rewards': [],\n        'log_probs': [],\n        'values': []\n    }\n    \n    # Collect trajectory\n    while not done:\n        # Convert observation to tensor\n        state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n        \n        # Get action\n        with torch.no_grad():\n            # Forward pass\n            x = state_tensor.transpose(1, 2)\n            embedded = model.embedder(x)\n            representation = embedded.mean(dim=1)\n            \n            # Get action from policy\n            output = model.policy_head({\n                'embedded': embedded,\n                'regime_embedding': torch.zeros(1, 8).to(device),\n                'synergy_features': torch.zeros(1, 32).to(device)\n            })\n            \n            action_logits = output['action']\n            dist = torch.distributions.Categorical(logits=action_logits)\n            action = dist.sample()\n            log_prob = dist.log_prob(action)\n            \n            # Get value\n            value = ppo.value_head(representation)\n        \n        # Step environment\n        next_obs, reward, done, info = env.step(action.item())\n        \n        # Store trajectory\n        trajectory['states'].append(state_tensor)\n        trajectory['actions'].append(action)\n        trajectory['rewards'].append(torch.tensor([reward]))\n        trajectory['log_probs'].append(log_prob)\n        trajectory['values'].append(value.squeeze())\n        \n        obs = next_obs\n    \n    # Process trajectory\n    for key in trajectory:\n        if trajectory[key]:\n            trajectory[key] = torch.stack(trajectory[key]).to(device)\n    \n    # Update policy\n    if len(trajectory['states']) > 0:\n        policy_loss, value_loss = ppo.update([trajectory])\n    \n    # Record metrics\n    episode_return = info['total_return']\n    episode_returns.append(episode_return)\n    episode_lengths.append(len(env.trades))\n    \n    # Calculate win rate\n    winning_trades = sum(1 for r in env.episode_rewards if r > 0)\n    total_trades = len([r for r in env.episode_rewards if r != 0])\n    win_rate = winning_trades / total_trades if total_trades > 0 else 0\n    win_rates.append(win_rate)\n    \n    # Log progress\n    if episode % 10 == 0:\n        avg_return = np.mean(episode_returns[-10:])\n        avg_win_rate = np.mean(win_rates[-10:])\n        print(f\"Episode {episode}:\")\n        print(f\"  Avg Return: {avg_return*100:.2f}%\")\n        print(f\"  Avg Win Rate: {avg_win_rate*100:.1f}%\")\n        print(f\"  Trades: {len(env.trades)}\")\n\n# Save fine-tuned model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'value_head_state_dict': ppo.value_head.state_dict(),\n    'episode': episode,\n    'avg_return': np.mean(episode_returns[-10:]),\n    'config': config\n}, MODELS_PATH / 'structure_analyzer_finetuned.pt')\n\nprint(f\"\\n‚úÖ RL fine-tuning complete!\")\nprint(f\"   Final average return: {np.mean(episode_returns[-10:])*100:.2f}%\")\nprint(f\"   Final win rate: {np.mean(win_rates[-10:])*100:.1f}%\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# RL Environment for Structure Analysis\nclass StructureTradingEnv:\n    \"\"\"Simplified trading environment for structure-based trading.\"\"\"\n    \n    def __init__(self, data, initial_capital=100000, transaction_cost=0.001):\n        self.data = data\n        self.initial_capital = initial_capital\n        self.transaction_cost = transaction_cost\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset environment to initial state.\"\"\"\n        self.capital = self.initial_capital\n        self.position = 0  # -1: short, 0: neutral, 1: long\n        self.current_idx = 48  # Start after first window\n        self.episode_rewards = []\n        self.trades = []\n        \n        # Get initial observation\n        return self._get_observation()\n    \n    def _get_observation(self):\n        \"\"\"Get current market observation.\"\"\"\n        # Get 48-bar window\n        window = self.data[self.current_idx - 48:self.current_idx]\n        return window\n    \n    def step(self, action):\n        \"\"\"Execute action and return next state.\"\"\"\n        # Actions: 0=hold, 1=buy/long, 2=sell/short\n        prev_capital = self.capital\n        \n        # Get current and next price\n        current_price = self.data[self.current_idx, 3]  # Close price\n        next_price = self.data[self.current_idx + 1, 3] if self.current_idx + 1 < len(self.data) else current_price\n        \n        # Execute trade\n        if action == 1 and self.position <= 0:  # Buy\n            # Close short if exists\n            if self.position < 0:\n                self.capital = self.capital * (2 - next_price / current_price)\n                self.capital *= (1 - self.transaction_cost)\n            # Open long\n            self.position = 1\n            self.capital *= (1 - self.transaction_cost)\n            self.trades.append(('BUY', self.current_idx, current_price))\n            \n        elif action == 2 and self.position >= 0:  # Sell/Short\n            # Close long if exists\n            if self.position > 0:\n                self.capital = self.capital * (next_price / current_price)\n                self.capital *= (1 - self.transaction_cost)\n            # Open short\n            self.position = -1\n            self.capital *= (1 - self.transaction_cost)\n            self.trades.append(('SELL', self.current_idx, current_price))\n        \n        # Update position value\n        if self.position == 1:  # Long\n            self.capital = self.capital * (next_price / current_price)\n        elif self.position == -1:  # Short\n            self.capital = self.capital * (2 - next_price / current_price)\n        \n        # Calculate reward\n        reward = (self.capital - prev_capital) / prev_capital\n        self.episode_rewards.append(reward)\n        \n        # Move to next step\n        self.current_idx += 1\n        done = self.current_idx >= len(self.data) - 1\n        \n        # Get next observation\n        next_obs = self._get_observation() if not done else None\n        \n        info = {\n            'capital': self.capital,\n            'position': self.position,\n            'total_return': (self.capital - self.initial_capital) / self.initial_capital\n        }\n        \n        return next_obs, reward, done, info\n\n# PPO implementation for fine-tuning\nclass StructurePPO:\n    \"\"\"PPO algorithm for structure-based trading.\"\"\"\n    \n    def __init__(self, model, lr=3e-4, gamma=0.99, eps_clip=0.2):\n        self.model = model\n        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.value_loss_coef = 0.5\n        self.entropy_coef = 0.01\n        \n        # Add value head for RL\n        self.value_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        ).to(device)\n        \n        self.value_optimizer = optim.Adam(self.value_head.parameters(), lr=lr)\n    \n    def get_action(self, state, deterministic=False):\n        \"\"\"Get action from policy.\"\"\"\n        with torch.no_grad():\n            # Forward pass\n            x = state.transpose(1, 2)\n            embedded = self.model.embedder(x)\n            representation = embedded.mean(dim=1)\n            \n            # Get action logits from policy head\n            output = self.model.policy_head({\n                'embedded': embedded,\n                'regime_embedding': torch.zeros(1, 8).to(device),  # Dummy regime\n                'synergy_features': torch.zeros(1, 32).to(device)  # Dummy synergy\n            })\n            \n            action_logits = output['action']\n            \n            # Sample action\n            if deterministic:\n                action = torch.argmax(action_logits, dim=1)\n            else:\n                dist = torch.distributions.Categorical(logits=action_logits)\n                action = dist.sample()\n            \n            # Get value\n            value = self.value_head(representation)\n            \n            return action.item(), value.item()\n    \n    def compute_returns(self, rewards, values, dones):\n        \"\"\"Compute discounted returns.\"\"\"\n        returns = []\n        R = 0\n        \n        for step in reversed(range(len(rewards))):\n            R = rewards[step] + self.gamma * R * (1 - dones[step])\n            returns.insert(0, R)\n        \n        return torch.tensor(returns, dtype=torch.float32)\n    \n    def update(self, trajectories):\n        \"\"\"Update policy using collected trajectories.\"\"\"\n        # Prepare data\n        states = torch.cat([t['states'] for t in trajectories])\n        actions = torch.cat([t['actions'] for t in trajectories])\n        rewards = torch.cat([t['rewards'] for t in trajectories])\n        old_log_probs = torch.cat([t['log_probs'] for t in trajectories])\n        values = torch.cat([t['values'] for t in trajectories])\n        \n        # Compute returns\n        returns = self.compute_returns(rewards, values, torch.zeros_like(rewards))\n        advantages = returns - values\n        \n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        \n        # PPO update\n        for _ in range(4):  # PPO epochs\n            # Forward pass\n            x = states.transpose(1, 2)\n            embedded = self.model.embedder(x)\n            representation = embedded.mean(dim=1)\n            \n            # Policy output\n            output = self.model.policy_head({\n                'embedded': embedded,\n                'regime_embedding': torch.zeros(len(states), 8).to(device),\n                'synergy_features': torch.zeros(len(states), 32).to(device)\n            })\n            \n            action_logits = output['action']\n            dist = torch.distributions.Categorical(logits=action_logits)\n            new_log_probs = dist.log_prob(actions)\n            entropy = dist.entropy().mean()\n            \n            # Value output\n            new_values = self.value_head(representation).squeeze()\n            \n            # PPO loss\n            ratio = torch.exp(new_log_probs - old_log_probs)\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n            \n            policy_loss = -torch.min(surr1, surr2).mean()\n            value_loss = F.mse_loss(new_values, returns)\n            \n            total_loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy\n            \n            # Update\n            self.optimizer.zero_grad()\n            self.value_optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n            self.optimizer.step()\n            self.value_optimizer.step()\n        \n        return policy_loss.item(), value_loss.item()\n\n# Initialize PPO\nppo = StructurePPO(model, lr=1e-4)\n\nprint(\"‚úÖ RL fine-tuning setup complete\")\nprint(\"   Algorithm: PPO\")\nprint(\"   Learning rate: 1e-4\")\nprint(\"   Gamma: 0.99\")\nprint(\"   Epsilon clip: 0.2\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Reinforcement Learning Fine-tuning",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load best model\ncheckpoint = torch.load(MODELS_PATH / 'structure_analyzer_pretrained.pt', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f\"üìä Evaluating best model from epoch {checkpoint['epoch'] + 1}\")\nprint(f\"   Validation F1: {checkpoint['val_f1']:.4f}\")\nprint(f\"   Validation Accuracy: {checkpoint['val_acc']:.2f}%\")\n\n# Test set evaluation\ntest_loss, test_acc, test_f1, test_preds, test_labels, test_probs = validate(\n    model, test_loader, criterion, device\n)\n\nprint(f\"\\nüìà Test Set Performance:\")\nprint(f\"   Loss: {test_loss:.4f}\")\nprint(f\"   Accuracy: {test_acc:.2f}%\")\nprint(f\"   F1 Score: {test_f1:.4f}\")\n\n# Confusion matrix\ncm = confusion_matrix(test_labels, test_preds)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=structure_names, yticklabels=structure_names)\nplt.title('Structure Detection Confusion Matrix', fontsize=16, fontweight='bold')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'structure_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Classification report\nprint(\"\\nüìã Classification Report:\")\nprint(classification_report(test_labels, test_preds, target_names=structure_names))\n\n# Per-class performance analysis\nprint(\"\\nüìä Per-Class Performance:\")\nfor i, name in enumerate(structure_names):\n    class_mask = np.array(test_labels) == i\n    if class_mask.sum() > 0:\n        class_acc = (np.array(test_preds)[class_mask] == i).mean() * 100\n        class_samples = class_mask.sum()\n        print(f\"   {name}: {class_acc:.1f}% accuracy ({class_samples} samples)\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Model Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot training history\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Loss\nax1 = axes[0]\nax1.plot(history['train_loss'], label='Train', linewidth=2)\nax1.plot(history['val_loss'], label='Validation', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss', fontsize=14, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Accuracy\nax2 = axes[1]\nax2.plot(history['train_acc'], label='Train', linewidth=2)\nax2.plot(history['val_acc'], label='Validation', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Structure Detection Accuracy', fontsize=14, fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# F1 Score\nax3 = axes[2]\nax3.plot(history['train_f1'], label='Train', linewidth=2)\nax3.plot(history['val_f1'], label='Validation', linewidth=2)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('F1 Score')\nax3.set_title('F1 Score (Weighted)', fontsize=14, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'structure_training_history.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training functions\ndef train_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n    for batch in progress_bar:\n        # Move data to device\n        market_data = batch['market_data'].to(device)\n        labels = batch['label'].to(device)\n        \n        # Forward pass through embedder\n        x = market_data.transpose(1, 2)  # Shape: (batch, features, sequence)\n        embedded = model.embedder(x)  # Shape: (batch, sequence, hidden_dim)\n        \n        # Global pooling to get representation\n        representation = embedded.mean(dim=1)  # Shape: (batch, hidden_dim)\n        \n        # Classification\n        logits = model.classification_head(representation)\n        loss = criterion(logits, labels)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        # Track metrics\n        total_loss += loss.item()\n        _, preds = torch.max(logits, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \n        # Update progress bar\n        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    # Calculate metrics\n    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    avg_loss = total_loss / len(loader)\n    \n    # Calculate F1 score\n    from sklearn.metrics import f1_score\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef validate(model, loader, criterion, device):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n            market_data = batch['market_data'].to(device)\n            labels = batch['label'].to(device)\n            \n            # Forward pass\n            x = market_data.transpose(1, 2)\n            embedded = model.embedder(x)\n            representation = embedded.mean(dim=1)\n            logits = model.classification_head(representation)\n            \n            # Calculate loss\n            loss = criterion(logits, labels)\n            total_loss += loss.item()\n            \n            # Get predictions\n            probs = F.softmax(logits, dim=1)\n            _, preds = torch.max(logits, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    # Calculate metrics\n    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    avg_loss = total_loss / len(loader)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1, all_preds, all_labels, all_probs\n\n# Training loop\nn_epochs = 50\nbest_val_f1 = 0\npatience = 10\npatience_counter = 0\n\nprint(\"\\nüöÄ Starting supervised pre-training...\\n\")\n\nfor epoch in range(n_epochs):\n    # Train\n    train_loss, train_acc, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n    \n    # Validate\n    val_loss, val_acc, val_f1, _, _, _ = validate(model, val_loader, criterion, device)\n    \n    # Update scheduler\n    scheduler.step()\n    \n    # Save history\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['train_f1'].append(train_f1)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_f1'].append(val_f1)\n    \n    # Save best model\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_f1': val_f1,\n            'val_acc': val_acc,\n            'config': config\n        }, MODELS_PATH / 'structure_analyzer_pretrained.pt')\n        print(f\"  üíæ New best model saved (F1: {val_f1:.4f})\")\n    else:\n        patience_counter += 1\n    \n    # Print progress\n    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%, F1: {train_f1:.4f}\")\n    print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%, F1: {val_f1:.4f}\")\n    print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    # Early stopping\n    if patience_counter >= patience:\n        print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n        break\n\nprint(f\"\\n‚úÖ Pre-training complete! Best validation F1: {best_val_f1:.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n\n# Training setup\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n\n# Training metrics\nhistory = {\n    'train_loss': [], 'train_acc': [], 'train_f1': [],\n    'val_loss': [], 'val_acc': [], 'val_f1': []\n}\n\nprint(f\"üìö Training setup complete:\")\nprint(f\"   Optimizer: AdamW (lr={config['learning_rate']}, wd={config['weight_decay']})\")\nprint(f\"   Scheduler: CosineAnnealingLR\")\nprint(f\"   Batch size: {config['batch_size']}\")\nprint(f\"   Train batches: {len(train_loader)}\")\nprint(f\"   Val batches: {len(val_loader)}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Supervised Pre-training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load configuration\nconfig = {\n    'window': 48,  # 48 30-minute bars = 24 hours\n    'input_features': 8,  # OHLCV + EMA21 + EMA50 + ATR\n    'hidden_dim': 256,\n    'n_heads': 8,\n    'n_layers': 4,\n    'dropout': 0.1,\n    'learning_rate': 1e-3,\n    'weight_decay': 1e-5,\n    'batch_size': 32,\n    'n_structure_classes': 4,  # Number of structure types\n}\n\n# Initialize Structure Analyzer\nprint(\"üèóÔ∏è Initializing Structure Analyzer model...\")\nmodel = StructureAnalyzer(config).to(device)\n\n# Add structure classification head for supervised training\nclass StructureClassificationHead(nn.Module):\n    \"\"\"Classification head for structure type prediction.\"\"\"\n    \n    def __init__(self, input_dim=256, hidden_dim=128, n_classes=4):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim // 2, n_classes)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\n# Attach classification head\nmodel.classification_head = StructureClassificationHead(\n    input_dim=256,\n    n_classes=config['n_structure_classes']\n).to(device)\n\n# Model summary\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\n‚úÖ Model initialized successfully:\")\nprint(f\"   Total parameters: {total_params:,}\")\nprint(f\"   Trainable parameters: {trainable_params:,}\")\nprint(f\"   Device: {device}\")\nprint(f\"\\nüìä Model Architecture:\")\nprint(f\"   - Window size: {config['window']} bars\")\nprint(f\"   - Input features: {config['input_features']}\")\nprint(f\"   - Hidden dimension: {config['hidden_dim']}\")\nprint(f\"   - Attention heads: {config['n_heads']}\")\nprint(f\"   - Transformer layers: {config['n_layers']}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Initialize Structure Analyzer Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze label distribution\nstructure_names = ['Strong Trend', 'Range Bound', 'Breakout', 'Reversal']\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor idx, (dataset, name) in enumerate([(train_dataset, 'Train'), \n                                        (val_dataset, 'Validation'), \n                                        (test_dataset, 'Test')]):\n    # Count labels\n    labels = dataset.labels\n    counts = np.bincount(labels, minlength=4)\n    \n    # Plot\n    ax = axes[idx]\n    bars = ax.bar(range(4), counts, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'])\n    ax.set_title(f'{name} Set Distribution', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Structure Type')\n    ax.set_ylabel('Count')\n    ax.set_xticks(range(4))\n    ax.set_xticklabels(structure_names, rotation=45, ha='right')\n    \n    # Add count labels on bars\n    for bar, count in zip(bars, counts):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{count}\\n({count/len(labels)*100:.1f}%)',\n                ha='center', va='bottom')\n\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'structure_label_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Analyze feature distributions\nprint(\"\\nüìä Feature Statistics:\")\nfor split_name, dataset in [('Train', train_dataset), ('Val', val_dataset), ('Test', test_dataset)]:\n    features = dataset.structure_features\n    print(f\"\\n{split_name} Set:\")\n    feature_names = ['Price Change', 'Trend Consistency', 'Volatility', 'Volume Mean', \n                     'Volume Trend', 'Price Position', 'Volume Imbalance', 'Avg Bar Range', 'Close Location']\n    \n    for i, name in enumerate(feature_names):\n        print(f\"  {name}: Œº={features[:, i].mean():.4f}, œÉ={features[:, i].std():.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Visualize Data Distribution",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create custom dataset for structure analysis\nclass StructureDataset(Dataset):\n    \"\"\"Custom dataset for structure analyzer training.\"\"\"\n    \n    def __init__(self, data_file, split='train', transform=None):\n        \"\"\"Initialize dataset.\n        \n        Args:\n            data_file: Path to HDF5 data file\n            split: Dataset split ('train', 'val', 'test')\n            transform: Optional data transformations\n        \"\"\"\n        self.data_file = Path(data_file)\n        self.split = split\n        self.transform = transform\n        \n        # Load data\n        self._load_data()\n        \n        # Generate structure labels\n        self._generate_labels()\n        \n        logger.info(f\"Loaded {split} dataset with {len(self)} samples\")\n    \n    def _load_data(self):\n        \"\"\"Load data from HDF5 file.\"\"\"\n        if self.data_file.exists():\n            with h5py.File(self.data_file, 'r') as f:\n                # Load 30m data for structure analyzer\n                if f'structure/{self.split}/data' in f:\n                    self.data = f[f'structure/{self.split}/data'][:]\n                    self.dates = [d.decode('utf-8') for d in f[f'structure/{self.split}/dates'][:]]\n                else:\n                    # Fallback to generic data structure\n                    self.data = f[f'{self.split}/30m'][:]\n                    self.dates = None\n                \n                # Load prices for label generation\n                if f'prices/{self.split}' in f:\n                    self.prices = f[f'prices/{self.split}'][:]\n                else:\n                    # Extract from data (assuming OHLC format)\n                    self.prices = self.data[:, :, 3]  # Close prices\n                \n                # Load metadata\n                self.metadata = {}\n                if 'metadata' in f:\n                    for key in f['metadata'].attrs:\n                        self.metadata[key] = f['metadata'].attrs[key]\n        else:\n            # Generate synthetic data for demonstration\n            print(\"Generating synthetic data...\")\n            self._generate_synthetic_data()\n    \n    def _generate_synthetic_data(self):\n        \"\"\"Generate synthetic market data for demonstration.\"\"\"\n        n_samples = {'train': 10000, 'val': 2000, 'test': 2000}[self.split]\n        \n        # Generate 30m data (48 bars √ó 8 features)\n        # Features: open, high, low, close, volume, ema_21, ema_50, atr\n        self.data = np.zeros((n_samples, 48, 8))\n        \n        for i in range(n_samples):\n            # Generate price series with trend\n            base_price = 1.0 + np.random.randn() * 0.1\n            trend = np.random.choice([-1, 0, 1]) * 0.001\n            volatility = 0.01 + np.random.rand() * 0.02\n            \n            prices = [base_price]\n            for t in range(1, 48):\n                change = trend + np.random.randn() * volatility\n                prices.append(prices[-1] * (1 + change))\n            \n            prices = np.array(prices)\n            \n            # Generate OHLC\n            for t in range(48):\n                self.data[i, t, 3] = prices[t]  # Close\n                self.data[i, t, 0] = prices[t] * (1 - np.random.rand() * 0.002)  # Open\n                self.data[i, t, 1] = prices[t] * (1 + np.random.rand() * 0.003)  # High\n                self.data[i, t, 2] = prices[t] * (1 - np.random.rand() * 0.003)  # Low\n                self.data[i, t, 4] = 1000 + np.random.rand() * 1000  # Volume\n                \n                # Technical indicators\n                if t >= 21:\n                    self.data[i, t, 5] = prices[t-21:t+1].mean()  # EMA 21\n                if t >= 50:\n                    self.data[i, t, 6] = prices[t-50:t+1].mean()  # EMA 50\n                self.data[i, t, 7] = volatility * prices[t]  # ATR proxy\n        \n        self.prices = self.data[:, :, 3]  # Close prices\n        self.dates = None\n        self.metadata = {'synthetic': True}\n    \n    def _generate_labels(self):\n        \"\"\"Generate structure labels from market data.\"\"\"\n        self.labels = []\n        self.structure_features = []\n        \n        for i in range(len(self.data)):\n            # Extract features for structure analysis\n            features = self._extract_structure_features(self.data[i])\n            self.structure_features.append(features)\n            \n            # Generate label based on structure type\n            label = self._classify_structure(features)\n            self.labels.append(label)\n        \n        self.labels = np.array(self.labels)\n        self.structure_features = np.array(self.structure_features)\n    \n    def _extract_structure_features(self, window):\n        \"\"\"Extract structure-relevant features from data window.\"\"\"\n        # Price features\n        close_prices = window[:, 3]\n        highs = window[:, 1]\n        lows = window[:, 2]\n        volumes = window[:, 4]\n        \n        # Trend features\n        price_change = (close_prices[-1] - close_prices[0]) / close_prices[0]\n        trend_consistency = np.sum(np.diff(close_prices) > 0) / len(close_prices)\n        \n        # Volatility features\n        returns = np.diff(close_prices) / close_prices[:-1]\n        volatility = np.std(returns) if len(returns) > 0 else 0\n        \n        # Volume profile\n        volume_mean = np.mean(volumes)\n        volume_std = np.std(volumes)\n        volume_trend = np.polyfit(range(len(volumes)), volumes, 1)[0] if len(volumes) > 1 else 0\n        \n        # Support/Resistance features\n        pivot_high = np.max(highs)\n        pivot_low = np.min(lows)\n        price_position = (close_prices[-1] - pivot_low) / (pivot_high - pivot_low + 1e-8)\n        \n        # Market depth proxy (using volume distribution)\n        volume_imbalance = np.sum(volumes[len(volumes)//2:]) / (np.sum(volumes[:len(volumes)//2]) + 1e-8)\n        \n        # Microstructure features\n        avg_bar_range = np.mean(highs - lows)\n        close_location = np.mean((close_prices - lows) / (highs - lows + 1e-8))\n        \n        features = {\n            'price_change': price_change,\n            'trend_consistency': trend_consistency,\n            'volatility': volatility,\n            'volume_mean': volume_mean,\n            'volume_trend': volume_trend,\n            'price_position': price_position,\n            'volume_imbalance': volume_imbalance,\n            'avg_bar_range': avg_bar_range,\n            'close_location': close_location\n        }\n        \n        return np.array(list(features.values()))\n    \n    def _classify_structure(self, features):\n        \"\"\"Classify market structure type.\n        \n        Structure types:\n        0: Strong Trend (Clear directional movement)\n        1: Range Bound (Consolidation between support/resistance)\n        2: Breakout (High volatility with directional bias)\n        3: Reversal (Trend exhaustion patterns)\n        \"\"\"\n        price_change = features[0]\n        trend_consistency = features[1]\n        volatility = features[2]\n        price_position = features[5]\n        \n        # Classification logic\n        if abs(price_change) > 0.02 and trend_consistency > 0.7:\n            return 0  # Strong Trend\n        elif volatility < 0.01 and abs(price_change) < 0.005:\n            return 1  # Range Bound\n        elif volatility > 0.02 and abs(price_change) > 0.01:\n            return 2  # Breakout\n        elif (price_position > 0.9 or price_position < 0.1) and trend_consistency < 0.5:\n            return 3  # Reversal\n        else:\n            # Default based on dominant characteristic\n            if trend_consistency > 0.6:\n                return 0\n            else:\n                return 1\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \"\"\"Get a single sample.\"\"\"\n        # Get data\n        market_data = torch.FloatTensor(self.data[idx])\n        structure_features = torch.FloatTensor(self.structure_features[idx])\n        label = torch.LongTensor([self.labels[idx]])\n        \n        # Apply transform if provided\n        if self.transform:\n            market_data = self.transform(market_data)\n        \n        return {\n            'market_data': market_data,\n            'structure_features': structure_features,\n            'label': label.squeeze(),\n            'prices': torch.FloatTensor(self.prices[idx])\n        }\n\n# Create datasets\nprint(\"\\nüìä Creating datasets...\")\n\n# Use existing data file or create synthetic data\ndata_file = DATA_PATH / \"structure_training_data.h5\"\n\ntrain_dataset = StructureDataset(data_file, split='train')\nval_dataset = StructureDataset(data_file, split='val')\ntest_dataset = StructureDataset(data_file, split='test')\n\nprint(f\"\\n‚úÖ Datasets created:\")\nprint(f\"   Train: {len(train_dataset)} samples\")\nprint(f\"   Val: {len(val_dataset)} samples\")\nprint(f\"   Test: {len(test_dataset)} samples\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Data loading configuration\nDATA_PATH = BASE_PATH / \"data\" / \"processed\"\n\n# Check available data files\nprint(\"üìÇ Checking available data files...\")\nif DATA_PATH.exists():\n    data_files = list(DATA_PATH.glob(\"*.h5\"))\n    print(f\"Found {len(data_files)} HDF5 files:\")\n    for f in data_files:\n        print(f\"  - {f.name}\")\nelse:\n    print(f\"‚ùå Data path does not exist: {DATA_PATH}\")\n    print(\"Creating mock data for demonstration...\")\n    DATA_PATH.mkdir(parents=True, exist_ok=True)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Load Training Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import dependencies\ntry:\n    # Core ML libraries\n    import torch.nn as nn\n    import torch.optim as optim\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, TensorDataset, Dataset\n    \n    # Scientific computing\n    from scipy import stats\n    from sklearn.metrics import confusion_matrix, classification_report\n    from sklearn.preprocessing import StandardScaler\n    \n    # Visualization\n    import matplotlib.patches as mpatches\n    from matplotlib.gridspec import GridSpec\n    \n    # Local imports\n    from agents.marl.agents.structure_analyzer import StructureAnalyzer\n    from agents.synergy.detector import SynergyDetector\n    from training.data_prep import MarketDataPipeline, DataLoader as MARLDataLoader\n    from training.rewards.reward_functions import StructureReward\n    \n    print(\"‚úÖ All dependencies loaded successfully\")\n    \nexcept ImportError as e:\n    print(f\"‚ùå Import error: {e}\")\n    print(\"Installing missing dependencies...\")\n    \n    # Install missing packages\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scipy\", \"scikit-learn\"])\n    \n    # Retry imports\n    from scipy import stats\n    from sklearn.metrics import confusion_matrix, classification_report\n    from sklearn.preprocessing import StandardScaler",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Environment setup and imports\nimport torch\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport json\nfrom tqdm import tqdm\nimport structlog\nfrom pathlib import Path\n\n# Configure structured logging\nstructlog.configure(\n    processors=[\n        structlog.stdlib.filter_by_level,\n        structlog.stdlib.add_logger_name,\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.PositionalArgumentsFormatter(),\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.format_exc_info,\n        structlog.dev.ConsoleRenderer()\n    ],\n    context_class=dict,\n    logger_factory=structlog.stdlib.LoggerFactory(),\n    cache_logger_on_first_use=True,\n)\n\nlogger = structlog.get_logger()\n\n# GPU check\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device.type == 'cuda':\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"‚ö†Ô∏è No GPU available, using CPU\")\n\n# Set paths\nBASE_PATH = Path(\"/home/QuantNova/AlgoSpace\")\nsys.path.insert(0, str(BASE_PATH))\nsys.path.insert(0, str(BASE_PATH / \"src\"))\n\n# Create necessary directories\nMODELS_PATH = BASE_PATH / \"models\" / \"agents\"\nRESULTS_PATH = BASE_PATH / \"results\" / \"structure_agent\"\nMODELS_PATH.mkdir(parents=True, exist_ok=True)\nRESULTS_PATH.mkdir(parents=True, exist_ok=True)\n\nprint(f\"‚úÖ Base path: {BASE_PATH}\")\nprint(f\"‚úÖ Models path: {MODELS_PATH}\")\nprint(f\"‚úÖ Results path: {RESULTS_PATH}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Environment Setup",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Structure Analyzer Agent Training\n\nThis notebook trains the Structure Analyzer agent individually before multi-agent training.\n\n## Agent Overview:\nThe Structure Analyzer identifies major market trends, support/resistance levels, and overall market structure using:\n- Volume profiles and market depth analysis\n- Microstructure feature extraction\n- Support/resistance level detection\n- Trend structure evaluation\n\n## Training Strategy:\n- Pre-train on historical structure patterns\n- Fine-tune with reinforcement learning\n- Optimize for structure pattern recognition and trend analysis\n\n## Key Features:\n- 30-minute timeframe analysis (48√ó8 matrix)\n- Integration with synergy detection patterns\n- Focus on market microstructure and volume analysis",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}