{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tactical Trader Agent Training\n",
    "\n",
    "This notebook trains the Short-term Tactician agent for immediate execution and tactical trading decisions.\n",
    "\n",
    "## Agent Overview:\n",
    "The Short-term Tactician specializes in:\n",
    "- Immediate price action analysis\n",
    "- Entry/exit timing optimization\n",
    "- Execution quality assessment\n",
    "- Technical indicator-based trading (RSI, MACD, Bollinger Bands, etc.)\n",
    "\n",
    "## Training Strategy:\n",
    "- Supervised pre-training on profitable trade labels\n",
    "- Reinforcement learning fine-tuning for risk-adjusted returns\n",
    "- Position sizing and risk management integration\n",
    "\n",
    "## Key Features:\n",
    "- 5-minute timeframe analysis (60×7 matrix)\n",
    "- Integration with synergy detection patterns\n",
    "- Focus on technical indicators and timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and imports\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import structlog\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure structured logging\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.stdlib.filter_by_level,\n",
    "        structlog.stdlib.add_logger_name,\n",
    "        structlog.stdlib.add_log_level,\n",
    "        structlog.stdlib.PositionalArgumentsFormatter(),\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.processors.StackInfoRenderer(),\n",
    "        structlog.processors.format_exc_info,\n",
    "        structlog.dev.ConsoleRenderer()\n",
    "    ],\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "    cache_logger_on_first_use=True,\n",
    ")\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "# GPU check and memory optimization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    # Enable memory optimization for Colab Pro\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"⚠️ No GPU available, using CPU\")\n",
    "\n",
    "# Set paths\n",
    "BASE_PATH = Path(\"/home/QuantNova/AlgoSpace\")\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "sys.path.insert(0, str(BASE_PATH / \"src\"))\n",
    "\n",
    "# Create necessary directories\n",
    "MODELS_PATH = BASE_PATH / \"models\" / \"agents\"\n",
    "RESULTS_PATH = BASE_PATH / \"results\" / \"tactical_agent\"\n",
    "CHECKPOINT_PATH = BASE_PATH / \"checkpoints\" / \"tactical_agent\"\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Base path: {BASE_PATH}\")\n",
    "print(f\"✅ Models path: {MODELS_PATH}\")\n",
    "print(f\"✅ Results path: {RESULTS_PATH}\")\n",
    "print(f\"✅ Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "try:\n",
    "    # Core ML libraries\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "    \n",
    "    # Scientific computing\n",
    "    from scipy import stats\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Technical indicators\n",
    "    try:\n",
    "        import talib\n",
    "    except ImportError:\n",
    "        print(\"Installing TA-Lib...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"TA-Lib\"])\n",
    "        import talib\n",
    "    \n",
    "    # Visualization\n",
    "    import matplotlib.patches as mpatches\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Local imports\n",
    "    from agents.marl.agents.short_term_tactician import ShortTermTactician\n",
    "    from agents.synergy.detector import SynergyDetector\n",
    "    from training.data_prep import MarketDataPipeline\n",
    "    from training.rewards.reward_functions import TacticalReward\n",
    "    from training.environments.trading_env import TradingEnvironment\n",
    "    \n",
    "    print(\"✅ All dependencies loaded successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Installing missing dependencies...\")\n",
    "    \n",
    "    # Install missing packages\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scipy\", \"scikit-learn\", \"plotly\"])\n",
    "    \n",
    "    # Retry imports\n",
    "    from scipy import stats\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading configuration\n",
    "DATA_PATH = BASE_PATH / \"data\" / \"historical\"\n",
    "\n",
    "# Load 5-minute data\n",
    "print(\"📂 Loading 5-minute market data...\")\n",
    "\n",
    "data_file = DATA_PATH / \"ES - 5 min.csv\"\n",
    "if data_file.exists():\n",
    "    df_5min = pd.read_csv(data_file)\n",
    "    print(f\"✅ Loaded {len(df_5min)} 5-minute bars\")\n",
    "    print(f\"   Date range: {df_5min.iloc[0]['Date']} to {df_5min.iloc[-1]['Date']}\")\n",
    "else:\n",
    "    print(f\"❌ Data file not found: {data_file}\")\n",
    "    print(\"Generating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Generate synthetic 5-minute data\n",
    "    dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='5min')\n",
    "    # Filter for market hours only (9:30 AM - 4:00 PM EST)\n",
    "    dates = dates[(dates.hour >= 9) & ((dates.hour < 16) | ((dates.hour == 16) & (dates.minute == 0)))]\n",
    "    dates = dates[(dates.hour > 9) | ((dates.hour == 9) & (dates.minute >= 30))]\n",
    "    \n",
    "    n_bars = len(dates)\n",
    "    base_price = 4000\n",
    "    \n",
    "    # Generate realistic price movements\n",
    "    returns = np.random.normal(0, 0.0005, n_bars)\n",
    "    prices = base_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    df_5min = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Open': prices * (1 + np.random.uniform(-0.001, 0.001, n_bars)),\n",
    "        'High': prices * (1 + np.random.uniform(0, 0.002, n_bars)),\n",
    "        'Low': prices * (1 - np.random.uniform(0, 0.002, n_bars)),\n",
    "        'Close': prices,\n",
    "        'Volume': np.random.lognormal(10, 0.5, n_bars)\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ Generated {len(df_5min)} synthetic 5-minute bars\")\n",
    "\n",
    "# Display data info\n",
    "print(\"\\n📊 Data Overview:\")\n",
    "print(df_5min.head())\n",
    "print(f\"\\nShape: {df_5min.shape}\")\n",
    "print(f\"\\nData types:\\n{df_5min.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate technical indicators\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"Calculate comprehensive technical indicators for tactical trading.\"\"\"\n",
    "    \n",
    "    print(\"📈 Calculating technical indicators...\")\n",
    "    \n",
    "    # Price data\n",
    "    close = df['Close'].values\n",
    "    high = df['High'].values\n",
    "    low = df['Low'].values\n",
    "    volume = df['Volume'].values\n",
    "    \n",
    "    # Trend Indicators\n",
    "    df['EMA_9'] = talib.EMA(close, timeperiod=9)\n",
    "    df['EMA_21'] = talib.EMA(close, timeperiod=21)\n",
    "    df['EMA_50'] = talib.EMA(close, timeperiod=50)\n",
    "    df['SMA_20'] = talib.SMA(close, timeperiod=20)\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'], df['MACD_signal'], df['MACD_hist'] = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = talib.RSI(close, timeperiod=14)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['BB_upper'], df['BB_middle'], df['BB_lower'] = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)\n",
    "    df['BB_width'] = df['BB_upper'] - df['BB_lower']\n",
    "    df['BB_percent'] = (close - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])\n",
    "    \n",
    "    # Stochastic\n",
    "    df['STOCH_K'], df['STOCH_D'] = talib.STOCH(high, low, close, fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "    \n",
    "    # ATR (Average True Range)\n",
    "    df['ATR'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "    \n",
    "    # ADX (Average Directional Index)\n",
    "    df['ADX'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    \n",
    "    # Volume indicators\n",
    "    df['OBV'] = talib.OBV(close, volume)\n",
    "    df['AD'] = talib.AD(high, low, close, volume)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df['MOM'] = talib.MOM(close, timeperiod=10)\n",
    "    df['ROC'] = talib.ROC(close, timeperiod=10)\n",
    "    \n",
    "    # Pattern recognition helpers\n",
    "    df['CDLHAMMER'] = talib.CDLHAMMER(df['Open'], high, low, close)\n",
    "    df['CDLDOJI'] = talib.CDLDOJI(df['Open'], high, low, close)\n",
    "    df['CDLENGULFING'] = talib.CDLENGULFING(df['Open'], high, low, close)\n",
    "    \n",
    "    # Support/Resistance levels (simplified)\n",
    "    df['PIVOT'] = (high + low + close) / 3\n",
    "    df['R1'] = 2 * df['PIVOT'] - low\n",
    "    df['S1'] = 2 * df['PIVOT'] - high\n",
    "    \n",
    "    # Price position relative to moving averages\n",
    "    df['Price_to_EMA9'] = (close - df['EMA_9']) / df['EMA_9']\n",
    "    df['Price_to_EMA21'] = (close - df['EMA_21']) / df['EMA_21']\n",
    "    df['Price_to_SMA20'] = (close - df['SMA_20']) / df['SMA_20']\n",
    "    \n",
    "    # Volatility measures\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Volatility'] = df['Returns'].rolling(window=20).std() * np.sqrt(252 * 78)  # Annualized\n",
    "    \n",
    "    # Volume analysis\n",
    "    df['Volume_SMA'] = df['Volume'].rolling(window=20).mean()\n",
    "    df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"✅ Calculated {len(df.columns) - 6} technical indicators\")\n",
    "    print(f\"   Final dataset: {len(df)} bars\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate indicators\n",
    "df_5min = calculate_technical_indicators(df_5min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trading_labels(df, profit_threshold=0.002, stop_loss=0.001, holding_periods=[12, 24, 36]):\n",
    "    \"\"\"\n",
    "    Create trading labels based on future price movements.\n",
    "    \n",
    "    Labels:\n",
    "    - 0: No trade (choppy/uncertain)\n",
    "    - 1: Long opportunity\n",
    "    - 2: Short opportunity\n",
    "    \"\"\"\n",
    "    print(\"🏷️ Creating trading labels...\")\n",
    "    \n",
    "    labels = []\n",
    "    label_metadata = []\n",
    "    \n",
    "    for i in range(len(df) - max(holding_periods)):\n",
    "        current_price = df.iloc[i]['Close']\n",
    "        \n",
    "        # Check multiple holding periods\n",
    "        long_profitable = False\n",
    "        short_profitable = False\n",
    "        best_return = 0\n",
    "        best_period = 0\n",
    "        \n",
    "        for period in holding_periods:\n",
    "            future_prices = df.iloc[i+1:i+period+1]['Close'].values\n",
    "            \n",
    "            if len(future_prices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check for profitable long\n",
    "            max_price = np.max(future_prices)\n",
    "            min_price = np.min(future_prices)\n",
    "            \n",
    "            long_return = (max_price - current_price) / current_price\n",
    "            long_risk = (current_price - min_price) / current_price\n",
    "            \n",
    "            short_return = (current_price - min_price) / current_price\n",
    "            short_risk = (max_price - current_price) / current_price\n",
    "            \n",
    "            # Check if trade would be profitable with risk management\n",
    "            if long_return > profit_threshold and long_risk < stop_loss * 2:\n",
    "                long_profitable = True\n",
    "                if long_return > best_return:\n",
    "                    best_return = long_return\n",
    "                    best_period = period\n",
    "            \n",
    "            if short_return > profit_threshold and short_risk < stop_loss * 2:\n",
    "                short_profitable = True\n",
    "                if short_return > best_return:\n",
    "                    best_return = short_return\n",
    "                    best_period = period\n",
    "        \n",
    "        # Determine label based on technical confirmation\n",
    "        rsi = df.iloc[i]['RSI']\n",
    "        macd_hist = df.iloc[i]['MACD_hist']\n",
    "        bb_percent = df.iloc[i]['BB_percent']\n",
    "        \n",
    "        if long_profitable and rsi < 70 and macd_hist > 0:\n",
    "            label = 1  # Long\n",
    "        elif short_profitable and rsi > 30 and macd_hist < 0:\n",
    "            label = 2  # Short\n",
    "        else:\n",
    "            label = 0  # No trade\n",
    "        \n",
    "        labels.append(label)\n",
    "        label_metadata.append({\n",
    "            'best_return': best_return,\n",
    "            'best_period': best_period,\n",
    "            'rsi': rsi,\n",
    "            'macd_hist': macd_hist,\n",
    "            'bb_percent': bb_percent\n",
    "        })\n",
    "    \n",
    "    # Pad the end\n",
    "    labels.extend([0] * max(holding_periods))\n",
    "    \n",
    "    df['Label'] = labels\n",
    "    \n",
    "    # Print label distribution\n",
    "    label_counts = pd.Series(labels).value_counts()\n",
    "    print(f\"\\n📊 Label Distribution:\")\n",
    "    print(f\"   No Trade: {label_counts.get(0, 0)} ({label_counts.get(0, 0)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"   Long: {label_counts.get(1, 0)} ({label_counts.get(1, 0)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"   Short: {label_counts.get(2, 0)} ({label_counts.get(2, 0)/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return df, label_metadata\n",
    "\n",
    "# Create labels\n",
    "df_5min, label_metadata = create_trading_labels(df_5min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Tactical Trading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacticalTradingDataset(Dataset):\n",
    "    \"\"\"Custom dataset for tactical trading with technical indicators.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, window_size=60, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with OHLCV and technical indicators\n",
    "            window_size: Number of 5-minute bars (60 = 5 hours)\n",
    "            transform: Optional data transformations\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.window_size = window_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Select key features for the 60×7 matrix\n",
    "        self.feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "            'EMA_21', 'ATR'\n",
    "        ]\n",
    "        \n",
    "        # Additional technical features for enhanced analysis\n",
    "        self.tech_feature_columns = [\n",
    "            'RSI', 'MACD', 'MACD_signal', 'BB_percent',\n",
    "            'STOCH_K', 'ADX', 'Volume_Ratio',\n",
    "            'Price_to_EMA9', 'Price_to_EMA21'\n",
    "        ]\n",
    "        \n",
    "        # Normalize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.price_scaler = MinMaxScaler()\n",
    "        \n",
    "        # Fit scalers\n",
    "        self.scaler.fit(df[self.tech_feature_columns].dropna())\n",
    "        self.price_scaler.fit(df[['Close']].dropna())\n",
    "        \n",
    "        # Valid indices (ensure we have enough history)\n",
    "        self.valid_indices = list(range(window_size, len(df)))\n",
    "        \n",
    "        logger.info(f\"Created dataset with {len(self.valid_indices)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sample.\"\"\"\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        \n",
    "        # Get window of data\n",
    "        window_data = self.df.iloc[real_idx - self.window_size:real_idx]\n",
    "        \n",
    "        # Create market matrix (60×7)\n",
    "        market_matrix = window_data[self.feature_columns].values\n",
    "        \n",
    "        # Normalize OHLCV\n",
    "        market_matrix[:, :4] = (market_matrix[:, :4] - market_matrix[0, 3]) / market_matrix[0, 3]  # Relative to first close\n",
    "        market_matrix[:, 4] = market_matrix[:, 4] / market_matrix[:, 4].mean()  # Volume relative to mean\n",
    "        market_matrix[:, 5] = (market_matrix[:, 5] - market_matrix[0, 3]) / market_matrix[0, 3]  # EMA relative\n",
    "        market_matrix[:, 6] = market_matrix[:, 6] / market_matrix[0, 3]  # ATR relative\n",
    "        \n",
    "        # Get technical indicators\n",
    "        tech_features = window_data[self.tech_feature_columns].iloc[-1].values\n",
    "        tech_features = self.scaler.transform(tech_features.reshape(1, -1)).flatten()\n",
    "        \n",
    "        # Get label\n",
    "        label = self.df.iloc[real_idx]['Label']\n",
    "        \n",
    "        # Create sample\n",
    "        sample = {\n",
    "            'market_matrix': torch.FloatTensor(market_matrix),\n",
    "            'tech_features': torch.FloatTensor(tech_features),\n",
    "            'label': torch.LongTensor([label]).squeeze(),\n",
    "            'timestamp': real_idx,\n",
    "            'current_price': self.df.iloc[real_idx]['Close']\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Split data into train/val/test\n",
    "print(\"\\n📊 Creating train/validation/test splits...\")\n",
    "\n",
    "# Use time-based split to avoid look-ahead bias\n",
    "n_samples = len(df_5min)\n",
    "train_end = int(n_samples * 0.7)\n",
    "val_end = int(n_samples * 0.85)\n",
    "\n",
    "train_df = df_5min.iloc[:train_end]\n",
    "val_df = df_5min.iloc[train_end:val_end]\n",
    "test_df = df_5min.iloc[val_end:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TacticalTradingDataset(train_df)\n",
    "val_dataset = TacticalTradingDataset(val_df)\n",
    "test_dataset = TacticalTradingDataset(test_df)\n",
    "\n",
    "print(f\"\\n✅ Datasets created:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples\")\n",
    "print(f\"   Val: {len(val_dataset)} samples\")\n",
    "print(f\"   Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Tactical Trader Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = {\n",
    "    'window': 60,  # 60 5-minute bars = 5 hours\n",
    "    'input_features': 7,  # OHLCV + EMA21 + ATR\n",
    "    'hidden_dim': 256,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 4,\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation': 4,  # For memory optimization\n",
    "}\n",
    "\n",
    "# Initialize Short-term Tactician\n",
    "print(\"🏗️ Initializing Short-term Tactician model...\")\n",
    "model = ShortTermTactician(config).to(device)\n",
    "\n",
    "# Add tactical classification head for supervised training\n",
    "class TacticalClassificationHead(nn.Module):\n",
    "    \"\"\"Classification head for tactical trading decisions.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=256, tech_features_dim=9, hidden_dim=128, n_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Combine embedded features with technical indicators\n",
    "        combined_dim = input_dim + tech_features_dim\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Position sizing head\n",
    "        self.position_size_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, embedded_features, tech_features):\n",
    "        # Combine features\n",
    "        combined = torch.cat([embedded_features, tech_features], dim=-1)\n",
    "        \n",
    "        # Get action logits\n",
    "        action_logits = self.layers(combined)\n",
    "        \n",
    "        # Get position size\n",
    "        position_size = self.position_size_head(combined)\n",
    "        \n",
    "        return action_logits, position_size\n",
    "\n",
    "# Attach classification head\n",
    "model.tactical_classification_head = TacticalClassificationHead(\n",
    "    input_dim=256,\n",
    "    tech_features_dim=9,\n",
    "    n_classes=3\n",
    ").to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✅ Model initialized successfully:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"\\n📊 Model Architecture:\")\n",
    "print(f\"   - Window size: {config['window']} bars (5 hours)\")\n",
    "print(f\"   - Input features: {config['input_features']}\")\n",
    "print(f\"   - Hidden dimension: {config['hidden_dim']}\")\n",
    "print(f\"   - Attention heads: {config['n_heads']}\")\n",
    "print(f\"   - Transformer layers: {config['n_layers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Supervised Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with memory optimization\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "# Training setup with class weights for imbalanced data\n",
    "train_labels = [train_dataset[i]['label'].item() for i in range(len(train_dataset))]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / (class_counts + 1e-5)\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=config['learning_rate'],\n",
    "    epochs=50,\n",
    "    steps_per_epoch=len(train_loader) // config['gradient_accumulation']\n",
    ")\n",
    "\n",
    "# Training metrics\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    'position_sizes': []\n",
    "}\n",
    "\n",
    "print(f\"📚 Training setup complete:\")\n",
    "print(f\"   Optimizer: AdamW (lr={config['learning_rate']}, wd={config['weight_decay']})\")\n",
    "print(f\"   Scheduler: OneCycleLR\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {config['gradient_accumulation']}\")\n",
    "print(f\"   Effective batch size: {config['batch_size'] * config['gradient_accumulation']}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"\\n   Class weights: {class_weights.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions with gradient accumulation\n",
    "def train_epoch(model, loader, criterion, optimizer, device, accumulation_steps=4):\n",
    "    \"\"\"Train for one epoch with gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_position_sizes = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move data to device\n",
    "        market_data = batch['market_matrix'].to(device)\n",
    "        tech_features = batch['tech_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass through embedder\n",
    "        x = market_data.transpose(1, 2)  # Shape: (batch, features, sequence)\n",
    "        embedded = model.embedder(x)  # Shape: (batch, sequence, hidden_dim)\n",
    "        \n",
    "        # Global pooling to get representation\n",
    "        representation = embedded.mean(dim=1)  # Shape: (batch, hidden_dim)\n",
    "        \n",
    "        # Classification with technical features\n",
    "        logits, position_size = model.tactical_classification_head(representation, tech_features)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_position_sizes.extend(position_size.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item() * accumulation_steps:.4f}'})\n",
    "        \n",
    "        # Clear cache periodically for memory management\n",
    "        if batch_idx % 50 == 0 and device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Average position size\n",
    "    avg_position_size = np.mean(all_position_sizes)\n",
    "    \n",
    "    return avg_loss, accuracy, f1, avg_position_size\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_position_sizes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            market_data = batch['market_matrix'].to(device)\n",
    "            tech_features = batch['tech_features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            x = market_data.transpose(1, 2)\n",
    "            embedded = model.embedder(x)\n",
    "            representation = embedded.mean(dim=1)\n",
    "            logits, position_size = model.tactical_classification_head(representation, tech_features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_position_sizes.extend(position_size.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    avg_position_size = np.mean(all_position_sizes)\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels, all_probs, avg_position_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with checkpointing\n",
    "n_epochs = 50\n",
    "best_val_f1 = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n🚀 Starting supervised pre-training...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1, train_pos_size = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, \n",
    "        accumulation_steps=config['gradient_accumulation']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _, _, val_pos_size = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['position_sizes'].append({'train': train_pos_size, 'val': val_pos_size})\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }\n",
    "        torch.save(checkpoint, CHECKPOINT_PATH / f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'val_acc': val_acc,\n",
    "            'config': config\n",
    "        }, MODELS_PATH / 'tactical_trader_pretrained.pt')\n",
    "        print(f\"  💾 New best model saved (F1: {val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%, F1: {train_f1:.4f}, Pos Size: {train_pos_size:.3f}\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%, F1: {val_f1:.4f}, Pos Size: {val_pos_size:.3f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n⚠️ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Clear cache\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n✅ Pre-training complete! Best validation F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(MODELS_PATH / 'tactical_trader_pretrained.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"📊 Evaluating best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"   Validation F1: {checkpoint['val_f1']:.4f}\")\n",
    "print(f\"   Validation Accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "# Test set evaluation\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels, test_probs, test_pos_size = validate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\n📈 Test Set Performance:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   F1 Score: {test_f1:.4f}\")\n",
    "print(f\"   Avg Position Size: {test_pos_size:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "action_names = ['No Trade', 'Long', 'Short']\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=action_names, yticklabels=action_names)\n",
    "plt.title('Tactical Trading Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'tactical_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n📋 Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=action_names))\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(\"\\n📊 Per-Class Performance:\")\n",
    "for i, name in enumerate(action_names):\n",
    "    class_mask = np.array(test_labels) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = (np.array(test_preds)[class_mask] == i).mean() * 100\n",
    "        class_samples = class_mask.sum()\n",
    "        print(f\"   {name}: {class_acc:.1f}% accuracy ({class_samples} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['train_loss'], label='Train', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['train_acc'], label='Train', linewidth=2)\n",
    "ax2.plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Trading Decision Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['train_f1'], label='Train', linewidth=2)\n",
    "ax3.plot(history['val_f1'], label='Validation', linewidth=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('F1 Score')\n",
    "ax3.set_title('F1 Score (Weighted)', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Position Sizes\n",
    "ax4 = axes[1, 1]\n",
    "train_pos_sizes = [h['train'] for h in history['position_sizes']]\n",
    "val_pos_sizes = [h['val'] for h in history['position_sizes']]\n",
    "ax4.plot(train_pos_sizes, label='Train', linewidth=2)\n",
    "ax4.plot(val_pos_sizes, label='Validation', linewidth=2)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Average Position Size')\n",
    "ax4.set_title('Position Sizing Evolution', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'tactical_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reinforcement Learning Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Environment for Tactical Trading\n",
    "class TacticalTradingEnv:\n",
    "    \"\"\"Trading environment for tactical short-term trading.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, initial_capital=100000, transaction_cost=0.0005, max_position_size=0.2):\n",
    "        self.data = data\n",
    "        self.initial_capital = initial_capital\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.max_position_size = max_position_size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.capital = self.initial_capital\n",
    "        self.position = 0  # Current position in shares\n",
    "        self.position_value = 0\n",
    "        self.current_idx = 60  # Start after first window\n",
    "        self.episode_rewards = []\n",
    "        self.trades = []\n",
    "        self.portfolio_values = [self.initial_capital]\n",
    "        \n",
    "        # Risk management\n",
    "        self.max_drawdown = 0\n",
    "        self.peak_value = self.initial_capital\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current market observation.\"\"\"\n",
    "        # Get 60-bar window\n",
    "        window = self.data[self.current_idx - 60:self.current_idx]\n",
    "        return window\n",
    "    \n",
    "    def step(self, action, position_size=0.1):\n",
    "        \"\"\"Execute action and return next state.\"\"\"\n",
    "        # Actions: 0=hold, 1=buy/long, 2=sell/short\n",
    "        prev_value = self.capital + self.position_value\n",
    "        \n",
    "        # Get current and next price\n",
    "        current_price = self.data[self.current_idx, 3]  # Close price\n",
    "        next_price = self.data[self.current_idx + 1, 3] if self.current_idx + 1 < len(self.data) else current_price\n",
    "        \n",
    "        # Risk-adjusted position sizing\n",
    "        atr = self.data[self.current_idx, 6]  # ATR for volatility\n",
    "        risk_adjusted_size = min(position_size, self.max_position_size) * (1 - atr / current_price * 10)\n",
    "        risk_adjusted_size = max(0.05, risk_adjusted_size)  # Minimum 5% position\n",
    "        \n",
    "        # Execute trade\n",
    "        if action == 1 and self.position <= 0:  # Buy\n",
    "            # Close short if exists\n",
    "            if self.position < 0:\n",
    "                buy_cost = abs(self.position) * next_price * (1 + self.transaction_cost)\n",
    "                self.capital -= buy_cost\n",
    "                profit = self.position_value - buy_cost\n",
    "                self.capital += self.position_value\n",
    "                self.position = 0\n",
    "                self.position_value = 0\n",
    "            \n",
    "            # Open long position\n",
    "            position_capital = self.capital * risk_adjusted_size\n",
    "            shares = position_capital / (current_price * (1 + self.transaction_cost))\n",
    "            self.position = shares\n",
    "            self.capital -= shares * current_price * (1 + self.transaction_cost)\n",
    "            self.position_value = shares * current_price\n",
    "            self.trades.append({\n",
    "                'type': 'BUY',\n",
    "                'idx': self.current_idx,\n",
    "                'price': current_price,\n",
    "                'shares': shares,\n",
    "                'size': risk_adjusted_size\n",
    "            })\n",
    "            \n",
    "        elif action == 2 and self.position >= 0:  # Sell/Short\n",
    "            # Close long if exists\n",
    "            if self.position > 0:\n",
    "                sell_value = self.position * next_price * (1 - self.transaction_cost)\n",
    "                self.capital += sell_value\n",
    "                self.position = 0\n",
    "                self.position_value = 0\n",
    "            \n",
    "            # Open short position\n",
    "            position_capital = self.capital * risk_adjusted_size\n",
    "            shares = position_capital / (current_price * (1 + self.transaction_cost))\n",
    "            self.position = -shares\n",
    "            self.capital += shares * current_price * (1 - self.transaction_cost)\n",
    "            self.position_value = -shares * current_price\n",
    "            self.trades.append({\n",
    "                'type': 'SELL',\n",
    "                'idx': self.current_idx,\n",
    "                'price': current_price,\n",
    "                'shares': shares,\n",
    "                'size': risk_adjusted_size\n",
    "            })\n",
    "        \n",
    "        # Update position value\n",
    "        if self.position > 0:  # Long\n",
    "            self.position_value = self.position * next_price\n",
    "        elif self.position < 0:  # Short\n",
    "            self.position_value = self.position * next_price\n",
    "        \n",
    "        # Calculate portfolio value and reward\n",
    "        current_value = self.capital + self.position_value\n",
    "        reward = (current_value - prev_value) / prev_value\n",
    "        \n",
    "        # Risk-adjusted reward (Sharpe-like)\n",
    "        if len(self.episode_rewards) > 0:\n",
    "            recent_returns = self.episode_rewards[-20:] if len(self.episode_rewards) >= 20 else self.episode_rewards\n",
    "            volatility = np.std(recent_returns) if len(recent_returns) > 1 else 0.01\n",
    "            sharpe_reward = reward / (volatility + 1e-6)\n",
    "            reward = 0.7 * reward + 0.3 * sharpe_reward * 0.01  # Blend raw and risk-adjusted\n",
    "        \n",
    "        self.episode_rewards.append(reward)\n",
    "        self.portfolio_values.append(current_value)\n",
    "        \n",
    "        # Update drawdown\n",
    "        if current_value > self.peak_value:\n",
    "            self.peak_value = current_value\n",
    "        drawdown = (self.peak_value - current_value) / self.peak_value\n",
    "        self.max_drawdown = max(self.max_drawdown, drawdown)\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_idx += 1\n",
    "        done = self.current_idx >= len(self.data) - 1\n",
    "        \n",
    "        # Get next observation\n",
    "        next_obs = self._get_observation() if not done else None\n",
    "        \n",
    "        info = {\n",
    "            'capital': self.capital,\n",
    "            'position': self.position,\n",
    "            'portfolio_value': current_value,\n",
    "            'total_return': (current_value - self.initial_capital) / self.initial_capital,\n",
    "            'max_drawdown': self.max_drawdown,\n",
    "            'n_trades': len(self.trades)\n",
    "        }\n",
    "        \n",
    "        return next_obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO implementation for tactical trading\n",
    "class TacticalPPO:\n",
    "    \"\"\"PPO algorithm for tactical trading with position sizing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.entropy_coef = 0.01\n",
    "        \n",
    "        # Add value head for RL\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(256 + 9, 128),  # embedded + tech features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.value_optimizer = optim.Adam(self.value_head.parameters(), lr=lr)\n",
    "    \n",
    "    def get_action(self, state, tech_features, deterministic=False):\n",
    "        \"\"\"Get action and position size from policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            x = state.transpose(1, 2)\n",
    "            embedded = self.model.embedder(x)\n",
    "            representation = embedded.mean(dim=1)\n",
    "            \n",
    "            # Get action logits and position size\n",
    "            action_logits, position_size = self.model.tactical_classification_head(\n",
    "                representation, tech_features\n",
    "            )\n",
    "            \n",
    "            # Sample action\n",
    "            if deterministic:\n",
    "                action = torch.argmax(action_logits, dim=1)\n",
    "            else:\n",
    "                dist = torch.distributions.Categorical(logits=action_logits)\n",
    "                action = dist.sample()\n",
    "            \n",
    "            # Get value\n",
    "            combined = torch.cat([representation, tech_features], dim=-1)\n",
    "            value = self.value_head(combined)\n",
    "            \n",
    "            return action.item(), position_size.item(), value.item()\n",
    "    \n",
    "    def compute_returns(self, rewards, values, dones):\n",
    "        \"\"\"Compute discounted returns.\"\"\"\n",
    "        returns = []\n",
    "        R = 0\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R = rewards[step] + self.gamma * R * (1 - dones[step])\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \"\"\"Update policy using collected trajectories.\"\"\"\n",
    "        # Prepare data\n",
    "        states = torch.cat([t['states'] for t in trajectories])\n",
    "        tech_features = torch.cat([t['tech_features'] for t in trajectories])\n",
    "        actions = torch.cat([t['actions'] for t in trajectories])\n",
    "        rewards = torch.cat([t['rewards'] for t in trajectories])\n",
    "        old_log_probs = torch.cat([t['log_probs'] for t in trajectories])\n",
    "        values = torch.cat([t['values'] for t in trajectories])\n",
    "        position_sizes = torch.cat([t['position_sizes'] for t in trajectories])\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = self.compute_returns(rewards, values, torch.zeros_like(rewards))\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        for _ in range(4):  # PPO epochs\n",
    "            # Forward pass\n",
    "            x = states.transpose(1, 2)\n",
    "            embedded = self.model.embedder(x)\n",
    "            representation = embedded.mean(dim=1)\n",
    "            \n",
    "            # Policy output\n",
    "            action_logits, new_position_sizes = self.model.tactical_classification_head(\n",
    "                representation, tech_features\n",
    "            )\n",
    "            \n",
    "            dist = torch.distributions.Categorical(logits=action_logits)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Value output\n",
    "            combined = torch.cat([representation, tech_features], dim=-1)\n",
    "            new_values = self.value_head(combined).squeeze()\n",
    "            \n",
    "            # PPO loss\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            \n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(new_values, returns)\n",
    "            position_loss = F.mse_loss(new_position_sizes.squeeze(), position_sizes)\n",
    "            \n",
    "            total_loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy + 0.1 * position_loss\n",
    "            \n",
    "            # Update\n",
    "            self.optimizer.zero_grad()\n",
    "            self.value_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item(), position_loss.item()\n",
    "\n",
    "# Initialize PPO\n",
    "ppo = TacticalPPO(model, lr=1e-4)\n",
    "\n",
    "print(\"✅ RL fine-tuning setup complete\")\n",
    "print(\"   Algorithm: PPO with position sizing\")\n",
    "print(\"   Learning rate: 1e-4\")\n",
    "print(\"   Gamma: 0.99\")\n",
    "print(\"   Epsilon clip: 0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL training loop\n",
    "n_episodes = 100\n",
    "episode_returns = []\n",
    "episode_lengths = []\n",
    "win_rates = []\n",
    "sharpe_ratios = []\n",
    "max_drawdowns = []\n",
    "\n",
    "# Use test data for RL fine-tuning\n",
    "env_data = test_dataset.df.values\n",
    "env = TacticalTradingEnv(env_data)\n",
    "\n",
    "print(\"\\n🚀 Starting RL fine-tuning...\\n\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    trajectory = {\n",
    "        'states': [],\n",
    "        'tech_features': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'log_probs': [],\n",
    "        'values': [],\n",
    "        'position_sizes': []\n",
    "    }\n",
    "    \n",
    "    # Collect trajectory\n",
    "    while not done:\n",
    "        # Convert observation to tensor\n",
    "        state_tensor = torch.FloatTensor(obs[:, :7]).unsqueeze(0).to(device)  # Use first 7 features\n",
    "        \n",
    "        # Get technical features for current state\n",
    "        current_idx = env.current_idx\n",
    "        tech_features = test_dataset.scaler.transform(\n",
    "            env_data[current_idx, 7:16].reshape(1, -1)  # Get tech indicators\n",
    "        )\n",
    "        tech_tensor = torch.FloatTensor(tech_features).to(device)\n",
    "        \n",
    "        # Get action and position size\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            x = state_tensor.transpose(1, 2)\n",
    "            embedded = model.embedder(x)\n",
    "            representation = embedded.mean(dim=1)\n",
    "            \n",
    "            # Get action and position size\n",
    "            action_logits, position_size = model.tactical_classification_head(\n",
    "                representation, tech_tensor\n",
    "            )\n",
    "            \n",
    "            dist = torch.distributions.Categorical(logits=action_logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # Get value\n",
    "            combined = torch.cat([representation, tech_tensor], dim=-1)\n",
    "            value = ppo.value_head(combined)\n",
    "        \n",
    "        # Step environment\n",
    "        next_obs, reward, done, info = env.step(action.item(), position_size.item())\n",
    "        \n",
    "        # Store trajectory\n",
    "        trajectory['states'].append(state_tensor)\n",
    "        trajectory['tech_features'].append(tech_tensor)\n",
    "        trajectory['actions'].append(action)\n",
    "        trajectory['rewards'].append(torch.tensor([reward]))\n",
    "        trajectory['log_probs'].append(log_prob)\n",
    "        trajectory['values'].append(value.squeeze())\n",
    "        trajectory['position_sizes'].append(torch.tensor([position_size.item()]))\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    # Process trajectory\n",
    "    for key in trajectory:\n",
    "        if trajectory[key]:\n",
    "            trajectory[key] = torch.stack(trajectory[key]).to(device)\n",
    "    \n",
    "    # Update policy\n",
    "    if len(trajectory['states']) > 0:\n",
    "        policy_loss, value_loss, position_loss = ppo.update([trajectory])\n",
    "    \n",
    "    # Record metrics\n",
    "    episode_return = info['total_return']\n",
    "    episode_returns.append(episode_return)\n",
    "    episode_lengths.append(info['n_trades'])\n",
    "    max_drawdowns.append(info['max_drawdown'])\n",
    "    \n",
    "    # Calculate win rate\n",
    "    winning_trades = sum(1 for r in env.episode_rewards if r > 0)\n",
    "    total_trades = len([r for r in env.episode_rewards if r != 0])\n",
    "    win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "    win_rates.append(win_rate)\n",
    "    \n",
    "    # Calculate Sharpe ratio\n",
    "    if len(env.episode_rewards) > 1:\n",
    "        returns_array = np.array(env.episode_rewards)\n",
    "        sharpe = np.mean(returns_array) / (np.std(returns_array) + 1e-6) * np.sqrt(252 * 78)  # Annualized\n",
    "        sharpe_ratios.append(sharpe)\n",
    "    else:\n",
    "        sharpe_ratios.append(0)\n",
    "    \n",
    "    # Log progress\n",
    "    if episode % 10 == 0:\n",
    "        avg_return = np.mean(episode_returns[-10:])\n",
    "        avg_win_rate = np.mean(win_rates[-10:])\n",
    "        avg_sharpe = np.mean(sharpe_ratios[-10:])\n",
    "        avg_drawdown = np.mean(max_drawdowns[-10:])\n",
    "        print(f\"Episode {episode}:\")\n",
    "        print(f\"  Avg Return: {avg_return*100:.2f}%\")\n",
    "        print(f\"  Avg Win Rate: {avg_win_rate*100:.1f}%\")\n",
    "        print(f\"  Avg Sharpe: {avg_sharpe:.2f}\")\n",
    "        print(f\"  Avg Max Drawdown: {avg_drawdown*100:.1f}%\")\n",
    "        print(f\"  Trades: {info['n_trades']}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % 25 == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'value_head_state_dict': ppo.value_head.state_dict(),\n",
    "            'episode': episode,\n",
    "            'metrics': {\n",
    "                'avg_return': np.mean(episode_returns[-10:]),\n",
    "                'avg_sharpe': np.mean(sharpe_ratios[-10:]),\n",
    "                'avg_win_rate': np.mean(win_rates[-10:])\n",
    "            },\n",
    "            'config': config\n",
    "        }, CHECKPOINT_PATH / f'rl_checkpoint_episode_{episode+1}.pt')\n",
    "\n",
    "# Save fine-tuned model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'value_head_state_dict': ppo.value_head.state_dict(),\n",
    "    'episode': episode,\n",
    "    'avg_return': np.mean(episode_returns[-10:]),\n",
    "    'avg_sharpe': np.mean(sharpe_ratios[-10:]),\n",
    "    'avg_win_rate': np.mean(win_rates[-10:]),\n",
    "    'config': config\n",
    "}, MODELS_PATH / 'tactical_trader_finetuned.pt')\n",
    "\n",
    "print(f\"\\n✅ RL fine-tuning complete!\")\n",
    "print(f\"   Final average return: {np.mean(episode_returns[-10:])*100:.2f}%\")\n",
    "print(f\"   Final Sharpe ratio: {np.mean(sharpe_ratios[-10:]):.2f}\")\n",
    "print(f\"   Final win rate: {np.mean(win_rates[-10:])*100:.1f}%\")\n",
    "print(f\"   Final max drawdown: {np.mean(max_drawdowns[-10:])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RL training results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode returns\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(episode_returns, alpha=0.3, label='Episode Return')\n",
    "ax1.plot(pd.Series(episode_returns).rolling(10).mean(), linewidth=2, label='10-Episode MA')\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Return (%)')\n",
    "ax1.set_title('Episode Returns', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sharpe ratios\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(sharpe_ratios, alpha=0.3, label='Sharpe Ratio')\n",
    "ax2.plot(pd.Series(sharpe_ratios).rolling(10).mean(), linewidth=2, label='10-Episode MA')\n",
    "ax2.axhline(y=1.0, color='g', linestyle='--', alpha=0.5, label='Good (1.0)')\n",
    "ax2.axhline(y=2.0, color='b', linestyle='--', alpha=0.5, label='Excellent (2.0)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Sharpe Ratio')\n",
    "ax2.set_title('Risk-Adjusted Returns', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Win rates\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(win_rates, alpha=0.3, label='Win Rate')\n",
    "ax3.plot(pd.Series(win_rates).rolling(10).mean(), linewidth=2, label='10-Episode MA')\n",
    "ax3.axhline(y=0.5, color='k', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Win Rate')\n",
    "ax3.set_title('Trading Win Rate', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Max drawdowns\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(np.array(max_drawdowns) * 100, alpha=0.3, label='Max Drawdown')\n",
    "ax4.plot(pd.Series(max_drawdowns).rolling(10).mean() * 100, linewidth=2, label='10-Episode MA')\n",
    "ax4.axhline(y=10, color='y', linestyle='--', alpha=0.5, label='Target (10%)')\n",
    "ax4.axhline(y=20, color='r', linestyle='--', alpha=0.5, label='Max Acceptable (20%)')\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Max Drawdown (%)')\n",
    "ax4.set_title('Risk Management', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'rl_training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Analysis and Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learned features and technical indicator importance\n",
    "def analyze_tactical_features(model, dataset, n_samples=100):\n",
    "    \"\"\"Analyze what features the model has learned for tactical trading.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    feature_importance = {\n",
    "        'No Trade': {'tech_features': [], 'embeddings': []},\n",
    "        'Long': {'tech_features': [], 'embeddings': []},\n",
    "        'Short': {'tech_features': [], 'embeddings': []}\n",
    "    }\n",
    "    \n",
    "    action_names = ['No Trade', 'Long', 'Short']\n",
    "    tech_feature_names = ['RSI', 'MACD', 'MACD_signal', 'BB_percent', \n",
    "                         'STOCH_K', 'ADX', 'Volume_Ratio', \n",
    "                         'Price_to_EMA9', 'Price_to_EMA21']\n",
    "    \n",
    "    attention_patterns = {i: [] for i in range(3)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(min(n_samples, len(dataset))):\n",
    "            sample = dataset[idx]\n",
    "            market_data = sample['market_matrix'].unsqueeze(0).to(device)\n",
    "            tech_features = sample['tech_features'].unsqueeze(0).to(device)\n",
    "            label = sample['label'].item()\n",
    "            \n",
    "            # Forward pass\n",
    "            x = market_data.transpose(1, 2)\n",
    "            embedded = model.embedder(x)\n",
    "            \n",
    "            # Extract attention weights if available\n",
    "            if hasattr(model.embedder, 'attention_weights'):\n",
    "                attention = model.embedder.attention_weights\n",
    "                if attention is not None:\n",
    "                    attention_patterns[label].append(attention.cpu().numpy())\n",
    "            \n",
    "            # Get representation\n",
    "            representation = embedded.mean(dim=1)\n",
    "            \n",
    "            # Store features by action type\n",
    "            action_name = action_names[label]\n",
    "            feature_importance[action_name]['tech_features'].append(tech_features.cpu().numpy())\n",
    "            feature_importance[action_name]['embeddings'].append(representation.cpu().numpy())\n",
    "    \n",
    "    # Analyze technical indicator importance\n",
    "    tech_importance_by_action = {}\n",
    "    \n",
    "    for action in action_names:\n",
    "        if feature_importance[action]['tech_features']:\n",
    "            tech_array = np.vstack(feature_importance[action]['tech_features'])\n",
    "            # Calculate feature importance as absolute mean\n",
    "            importance = np.abs(tech_array).mean(axis=0)\n",
    "            tech_importance_by_action[action] = dict(zip(tech_feature_names, importance))\n",
    "    \n",
    "    return feature_importance, attention_patterns, tech_importance_by_action\n",
    "\n",
    "# Run analysis\n",
    "print(\"🔍 Analyzing learned features...\")\n",
    "feature_importance, attention_patterns, tech_importance = analyze_tactical_features(model, test_dataset)\n",
    "\n",
    "# Visualize technical indicator importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "actions = list(tech_importance.keys())\n",
    "indicators = list(next(iter(tech_importance.values())).keys())\n",
    "n_indicators = len(indicators)\n",
    "n_actions = len(actions)\n",
    "\n",
    "x = np.arange(n_indicators)\n",
    "width = 0.25\n",
    "\n",
    "for i, action in enumerate(actions):\n",
    "    values = [tech_importance[action][ind] for ind in indicators]\n",
    "    ax.bar(x + i * width, values, width, label=action)\n",
    "\n",
    "ax.set_xlabel('Technical Indicators')\n",
    "ax.set_ylabel('Feature Importance')\n",
    "ax.set_title('Technical Indicator Importance by Action', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(indicators, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'technical_indicator_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print top indicators for each action\n",
    "print(\"\\n📊 Top Technical Indicators by Action:\")\n",
    "for action in actions:\n",
    "    sorted_indicators = sorted(tech_importance[action].items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\n{action}:\")\n",
    "    for ind, importance in sorted_indicators[:3]:\n",
    "        print(f\"  - {ind}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Summary and Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive training summary\n",
    "summary = f\"\"\"\n",
    "# Tactical Trader Agent Training Summary\n",
    "\n",
    "## Training Configuration\n",
    "- Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- Device: {device}\n",
    "- Model Parameters: {total_params:,}\n",
    "- Window Size: {config['window']} bars (5-minute)\n",
    "- Input Features: {config['input_features']}\n",
    "\n",
    "## Supervised Pre-training Results\n",
    "- Epochs: {len(history['train_loss'])}\n",
    "- Best Validation F1: {best_val_f1:.4f}\n",
    "- Test Set Performance:\n",
    "  - Accuracy: {test_acc:.2f}%\n",
    "  - F1 Score: {test_f1:.4f}\n",
    "  - Average Position Size: {test_pos_size:.3f}\n",
    "\n",
    "## Trading Action Performance\n",
    "\"\"\"\n",
    "\n",
    "# Add per-class performance\n",
    "for i, name in enumerate(action_names):\n",
    "    class_mask = np.array(test_labels) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = (np.array(test_preds)[class_mask] == i).mean() * 100\n",
    "        summary += f\"- {name}: {class_acc:.1f}% accuracy\\n\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "## Reinforcement Learning Fine-tuning\n",
    "- Episodes: {n_episodes}\n",
    "- Final Average Return: {np.mean(episode_returns[-10:])*100:.2f}%\n",
    "- Final Sharpe Ratio: {np.mean(sharpe_ratios[-10:]):.2f}\n",
    "- Final Win Rate: {np.mean(win_rates[-10:])*100:.1f}%\n",
    "- Final Max Drawdown: {np.mean(max_drawdowns[-10:])*100:.1f}%\n",
    "- Average Trades per Episode: {np.mean(episode_lengths[-10:]):.1f}\n",
    "\n",
    "## Model Architecture\n",
    "- Embedder: Transformer-based (7 features → 256 dim)\n",
    "- Attention: {config['n_heads']} heads, {config['n_layers']} layers\n",
    "- Policy Head: 3 actions (hold, long, short)\n",
    "- Position Sizing: Adaptive (5% - 20%)\n",
    "- Timing Head: 0-5 bar delay capability\n",
    "\n",
    "## Technical Indicators Used\n",
    "- Trend: EMA(9,21,50), MACD\n",
    "- Momentum: RSI, Stochastic, ROC\n",
    "- Volatility: Bollinger Bands, ATR\n",
    "- Volume: OBV, AD, Volume Ratio\n",
    "- Market Structure: ADX, Pivot Points\n",
    "\n",
    "## Risk Management Features\n",
    "- Position sizing based on ATR\n",
    "- Maximum position size: 20%\n",
    "- Sharpe-adjusted rewards\n",
    "- Drawdown monitoring\n",
    "\n",
    "## Output Files\n",
    "- Pre-trained Model: {MODELS_PATH}/tactical_trader_pretrained.pt\n",
    "- Fine-tuned Model: {MODELS_PATH}/tactical_trader_finetuned.pt\n",
    "- Training History: {RESULTS_PATH}/tactical_training_history.png\n",
    "- Confusion Matrix: {RESULTS_PATH}/tactical_confusion_matrix.png\n",
    "- RL Metrics: {RESULTS_PATH}/rl_training_metrics.png\n",
    "- Feature Analysis: {RESULTS_PATH}/technical_indicator_importance.png\n",
    "\n",
    "## Integration Notes\n",
    "- Compatible with MatrixAssembler5m output\n",
    "- Integrates with SynergyDetector patterns\n",
    "- Provides timing signals for execution\n",
    "- Weights 30% in multi-agent decision making\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(RESULTS_PATH / 'training_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "# Export model configuration\n",
    "model_config = {\n",
    "    'architecture': 'ShortTermTactician',\n",
    "    'config': config,\n",
    "    'input_shape': (60, 7),  # 5m bars × features\n",
    "    'tech_features': 9,  # Number of technical indicators\n",
    "    'output_heads': {\n",
    "        'action': 3,\n",
    "        'confidence': 1,\n",
    "        'timing': 5,\n",
    "        'reasoning': 48,\n",
    "        'position_size': 1\n",
    "    },\n",
    "    'training_metrics': {\n",
    "        'pretrain_f1': best_val_f1,\n",
    "        'test_accuracy': test_acc,\n",
    "        'rl_return': np.mean(episode_returns[-10:]),\n",
    "        'rl_sharpe': np.mean(sharpe_ratios[-10:]),\n",
    "        'rl_win_rate': np.mean(win_rates[-10:]),\n",
    "        'rl_max_drawdown': np.mean(max_drawdowns[-10:])\n",
    "    },\n",
    "    'synergy_integration': True,\n",
    "    'agent_weight': 0.3\n",
    "}\n",
    "\n",
    "with open(MODELS_PATH / 'tactical_trader_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Training complete! All models and results saved.\")\n",
    "print(f\"📁 Models directory: {MODELS_PATH}\")\n",
    "print(f\"📊 Results directory: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Deployment Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration code\n",
    "integration_example = \"\"\"\n",
    "# Integration with AlgoSpace MARL System\n",
    "\n",
    "## 1. Load the trained model in your trading system:\n",
    "\n",
    "```python\n",
    "from agents.marl.agents.short_term_tactician import ShortTermTactician\n",
    "import torch\n",
    "\n",
    "# Initialize tactical trader\n",
    "config = {\n",
    "    'window': 60,\n",
    "    'input_features': 7,\n",
    "    'hidden_dim': 256,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 4,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "tactical_agent = ShortTermTactician(config)\n",
    "\n",
    "# Load pre-trained weights\n",
    "checkpoint = torch.load('models/agents/tactical_trader_finetuned.pt')\n",
    "tactical_agent.load_state_dict(checkpoint['model_state_dict'])\n",
    "tactical_agent.eval()\n",
    "```\n",
    "\n",
    "## 2. Use in MARL consensus:\n",
    "\n",
    "```python\n",
    "from training.marl_trainer import MARLTrainer\n",
    "\n",
    "# Configure MARL system\n",
    "marl_config = {\n",
    "    'agents': {\n",
    "        'structure_analyzer': structure_agent,\n",
    "        'regime_detector': regime_agent,\n",
    "        'tactical_trader': tactical_agent\n",
    "    },\n",
    "    'consensus_weights': {\n",
    "        'structure_analyzer': 0.4,\n",
    "        'regime_detector': 0.3,\n",
    "        'tactical_trader': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize MARL trainer\n",
    "trainer = MARLTrainer(marl_config)\n",
    "```\n",
    "\n",
    "## 3. Real-time trading integration:\n",
    "\n",
    "```python\n",
    "# Process incoming market data\n",
    "market_data_5m = matrix_assembler.process_5m_data(raw_data)\n",
    "\n",
    "# Calculate technical indicators\n",
    "tech_indicators = calculate_technical_indicators(market_data_5m)\n",
    "\n",
    "# Get tactical trading decision\n",
    "with torch.no_grad():\n",
    "    tactical_output = tactical_agent({\n",
    "        'market_matrix': market_data_5m,\n",
    "        'regime_embedding': regime_embedding,\n",
    "        'synergy_context': synergy_context\n",
    "    })\n",
    "\n",
    "# Extract trading signals\n",
    "action = tactical_output['action']  # [pass, long, short]\n",
    "confidence = tactical_output['confidence']\n",
    "timing = tactical_output['timing_recommendation']  # Bars to wait\n",
    "position_size = tactical_output.get('position_size', 0.1)\n",
    "\n",
    "# Execute with timing\n",
    "if confidence > 0.7 and timing == 0:  # Execute now\n",
    "    execute_trade(\n",
    "        action=action,\n",
    "        position_size=calculate_risk_adjusted_size(position_size, current_atr)\n",
    "    )\n",
    "elif timing > 0:\n",
    "    schedule_trade(action, timing, position_size)\n",
    "```\n",
    "\n",
    "## 4. Risk management integration:\n",
    "\n",
    "```python\n",
    "# Calculate position size with risk limits\n",
    "def calculate_risk_adjusted_size(base_size, atr, max_risk=0.02):\n",
    "    # Risk per trade\n",
    "    account_value = get_account_value()\n",
    "    risk_amount = account_value * max_risk\n",
    "    \n",
    "    # Position size based on ATR\n",
    "    stop_distance = 2 * atr\n",
    "    shares = risk_amount / stop_distance\n",
    "    \n",
    "    # Apply limits\n",
    "    max_position = account_value * 0.2  # 20% max\n",
    "    position_value = min(shares * current_price, max_position)\n",
    "    \n",
    "    return position_value / current_price\n",
    "```\n",
    "\n",
    "## 5. Performance monitoring:\n",
    "\n",
    "```python\n",
    "# Track tactical agent performance\n",
    "metrics_tracker.log({\n",
    "    'agent': 'tactical_trader',\n",
    "    'action': action,\n",
    "    'confidence': confidence,\n",
    "    'position_size': position_size,\n",
    "    'timing': timing,\n",
    "    'technical_signals': {\n",
    "        'rsi': tech_indicators['RSI'],\n",
    "        'macd': tech_indicators['MACD'],\n",
    "        'bb_percent': tech_indicators['BB_percent']\n",
    "    },\n",
    "    'synergy_alignment': synergy_context['synergy_type']\n",
    "})\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(integration_example)\n",
    "\n",
    "# Save integration guide\n",
    "with open(RESULTS_PATH / 'integration_guide.md', 'w') as f:\n",
    "    f.write(integration_example)\n",
    "\n",
    "print(\"\\n✅ Tactical Trader Agent Training notebook complete!\")\n",
    "print(\"\\n📚 Next Steps:\")\n",
    "print(\"1. Review the training summary and metrics\")\n",
    "print(\"2. Test the model with live market data\")\n",
    "print(\"3. Integrate with MARL consensus mechanism\")\n",
    "print(\"4. Monitor real-time performance\")\n",
    "print(\"5. Fine-tune based on live trading results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}