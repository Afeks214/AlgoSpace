{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_cell"
   },
   "source": [
    "# AlgoSpace Data Preparation for Google Colab\n",
    "\n",
    "This notebook handles data preparation and preprocessing for MARL training in Google Colab.\n",
    "\n",
    "## Key Features:\n",
    "- Download and process historical market data\n",
    "- Generate training matrices for each agent\n",
    "- Create train/validation/test splits\n",
    "- Optimize data format for Colab training\n",
    "- Upload processed data to Google Drive\n",
    "\n",
    "## Data Sources:\n",
    "- Yahoo Finance (for demonstration)\n",
    "- Alpha Vantage API (optional)\n",
    "- Your own CSV/Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_environment"
   },
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è Not running in Google Colab\")\n",
    "\n",
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/AlgoSpace\"\n",
    "    !mkdir -p {DRIVE_BASE}/data/{raw,processed,compressed}\n",
    "else:\n",
    "    DRIVE_BASE = \"./drive_simulation\"\n",
    "    import os\n",
    "    os.makedirs(f\"{DRIVE_BASE}/data/raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE}/data/processed\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q yfinance pandas numpy h5py\n",
    "!pip install -q ta pandas-ta\n",
    "!pip install -q scikit-learn tqdm\n",
    "!pip install -q pyarrow  # For parquet support\n",
    "\n",
    "print(\"‚úÖ Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import h5py\n",
    "from datetime import datetime, timedelta\n",
    "import ta\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_config_section"
   },
   "source": [
    "## 2. Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_configuration"
   },
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "DATA_CONFIG = {\n",
    "    # Market symbols to download\n",
    "    'symbols': [\n",
    "        'SPY', 'QQQ', 'IWM', 'DIA',  # Major indices\n",
    "        'GLD', 'SLV', 'USO', 'UNG',  # Commodities\n",
    "        'TLT', 'IEF', 'SHY', 'AGG',  # Bonds\n",
    "        'VIX', 'VXX', 'UVXY',         # Volatility\n",
    "        'AAPL', 'MSFT', 'GOOGL', 'AMZN',  # Tech stocks\n",
    "        'JPM', 'BAC', 'GS', 'MS'      # Financial stocks\n",
    "    ],\n",
    "    \n",
    "    # Date range\n",
    "    'start_date': '2018-01-01',\n",
    "    'end_date': '2023-12-31',\n",
    "    \n",
    "    # Data splits\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # Feature engineering\n",
    "    'lookback_periods': [5, 10, 20, 50, 100, 200],\n",
    "    'technical_indicators': True,\n",
    "    'market_microstructure': True,\n",
    "    \n",
    "    # Matrix generation\n",
    "    'correlation_window': 20,\n",
    "    'volume_profile_bins': 10,\n",
    "    'orderbook_levels': 5,\n",
    "    \n",
    "    # Normalization\n",
    "    'normalization_method': 'robust',  # 'standard' or 'robust'\n",
    "    'clip_outliers': True,\n",
    "    'outlier_threshold': 5\n",
    "}\n",
    "\n",
    "print(\"üìã Data Configuration:\")\n",
    "print(f\"- Symbols: {len(DATA_CONFIG['symbols'])} assets\")\n",
    "print(f\"- Date Range: {DATA_CONFIG['start_date']} to {DATA_CONFIG['end_date']}\")\n",
    "print(f\"- Train/Val/Test Split: {DATA_CONFIG['train_ratio']}/{DATA_CONFIG['val_ratio']}/{DATA_CONFIG['test_ratio']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## 3. Download Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_data"
   },
   "outputs": [],
   "source": [
    "# Download market data\n",
    "def download_market_data(symbols, start_date, end_date):\n",
    "    \"\"\"Download historical market data from Yahoo Finance.\"\"\"\n",
    "    \n",
    "    all_data = {}\n",
    "    failed_symbols = []\n",
    "    \n",
    "    print(f\"üì• Downloading data for {len(symbols)} symbols...\")\n",
    "    \n",
    "    for symbol in tqdm(symbols, desc=\"Downloading\"):\n",
    "        try:\n",
    "            # Download data\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            data = ticker.history(start=start_date, end=end_date)\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                # Add symbol column\n",
    "                data['Symbol'] = symbol\n",
    "                all_data[symbol] = data\n",
    "            else:\n",
    "                failed_symbols.append(symbol)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Failed to download {symbol}: {e}\")\n",
    "            failed_symbols.append(symbol)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Downloaded data for {len(all_data)} symbols\")\n",
    "    if failed_symbols:\n",
    "        print(f\"‚ùå Failed symbols: {failed_symbols}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Download the data\n",
    "market_data = download_market_data(\n",
    "    DATA_CONFIG['symbols'],\n",
    "    DATA_CONFIG['start_date'],\n",
    "    DATA_CONFIG['end_date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_overview"
   },
   "outputs": [],
   "source": [
    "# Data overview\n",
    "print(\"üìä Data Overview:\")\n",
    "for symbol, data in list(market_data.items())[:5]:  # Show first 5\n",
    "    print(f\"\\n{symbol}:\")\n",
    "    print(f\"  - Shape: {data.shape}\")\n",
    "    print(f\"  - Date Range: {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "    print(f\"  - Columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_section"
   },
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "technical_indicators"
   },
   "outputs": [],
   "source": [
    "# Add technical indicators\n",
    "def add_technical_indicators(df, lookback_periods):\n",
    "    \"\"\"Add technical indicators to the dataframe.\"\"\"\n",
    "    \n",
    "    # Price-based features\n",
    "    df['returns'] = df['Close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df['high_low_ratio'] = df['High'] / df['Low']\n",
    "    df['close_open_ratio'] = df['Close'] / df['Open']\n",
    "    \n",
    "    # Volume features\n",
    "    df['volume_sma'] = df['Volume'].rolling(window=20).mean()\n",
    "    df['volume_ratio'] = df['Volume'] / df['volume_sma']\n",
    "    df['dollar_volume'] = df['Close'] * df['Volume']\n",
    "    \n",
    "    # Moving averages\n",
    "    for period in lookback_periods:\n",
    "        df[f'sma_{period}'] = df['Close'].rolling(window=period).mean()\n",
    "        df[f'ema_{period}'] = df['Close'].ewm(span=period).mean()\n",
    "        df[f'close_sma_{period}_ratio'] = df['Close'] / df[f'sma_{period}']\n",
    "    \n",
    "    # Volatility indicators\n",
    "    df['atr'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n",
    "    df['bb_high'], df['bb_mid'], df['bb_low'] = ta.volatility.bollinger_hband(df['Close']), \\\n",
    "                                                 ta.volatility.bollinger_mavg(df['Close']), \\\n",
    "                                                 ta.volatility.bollinger_lband(df['Close'])\n",
    "    df['bb_width'] = (df['bb_high'] - df['bb_low']) / df['bb_mid']\n",
    "    df['bb_position'] = (df['Close'] - df['bb_low']) / (df['bb_high'] - df['bb_low'])\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df['rsi'] = ta.momentum.rsi(df['Close'])\n",
    "    df['macd'] = ta.trend.macd(df['Close'])\n",
    "    df['macd_signal'] = ta.trend.macd_signal(df['Close'])\n",
    "    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
    "    df['stoch'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n",
    "    \n",
    "    # Trend indicators\n",
    "    df['adx'] = ta.trend.adx(df['High'], df['Low'], df['Close'])\n",
    "    df['cci'] = ta.trend.cci(df['High'], df['Low'], df['Close'])\n",
    "    \n",
    "    # Support/Resistance levels\n",
    "    for period in [20, 50]:\n",
    "        df[f'resistance_{period}'] = df['High'].rolling(window=period).max()\n",
    "        df[f'support_{period}'] = df['Low'].rolling(window=period).min()\n",
    "        df[f'sr_ratio_{period}'] = (df['Close'] - df[f'support_{period}']) / \\\n",
    "                                   (df[f'resistance_{period}'] - df[f'support_{period}'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply technical indicators\n",
    "print(\"üîß Adding technical indicators...\")\n",
    "for symbol in tqdm(market_data.keys(), desc=\"Processing\"):\n",
    "    market_data[symbol] = add_technical_indicators(\n",
    "        market_data[symbol], \n",
    "        DATA_CONFIG['lookback_periods']\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Technical indicators added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "market_microstructure"
   },
   "outputs": [],
   "source": [
    "# Add market microstructure features\n",
    "def add_microstructure_features(df):\n",
    "    \"\"\"Add market microstructure features.\"\"\"\n",
    "    \n",
    "    # Spread estimation (using high-low as proxy)\n",
    "    df['spread_pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    df['spread_ma'] = df['spread_pct'].rolling(window=20).mean()\n",
    "    \n",
    "    # Volume profile\n",
    "    df['volume_profile'] = df['Volume'].rolling(window=20).apply(\n",
    "        lambda x: np.percentile(x, 75) / np.percentile(x, 25) if np.percentile(x, 25) > 0 else 1\n",
    "    )\n",
    "    \n",
    "    # Price efficiency\n",
    "    df['price_efficiency'] = abs(df['returns']) / df['spread_pct']\n",
    "    \n",
    "    # Amihud illiquidity\n",
    "    df['amihud_illiquidity'] = abs(df['returns']) / (df['dollar_volume'] + 1e-10)\n",
    "    df['amihud_ma'] = df['amihud_illiquidity'].rolling(window=20).mean()\n",
    "    \n",
    "    # Roll measure (proxy for effective spread)\n",
    "    df['roll_measure'] = df['returns'].rolling(window=2).cov()\n",
    "    \n",
    "    # Order flow imbalance (using volume and price direction)\n",
    "    df['price_direction'] = np.sign(df['returns'])\n",
    "    df['signed_volume'] = df['Volume'] * df['price_direction']\n",
    "    df['order_imbalance'] = df['signed_volume'].rolling(window=20).sum() / \\\n",
    "                           df['Volume'].rolling(window=20).sum()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply microstructure features\n",
    "if DATA_CONFIG['market_microstructure']:\n",
    "    print(\"üî¨ Adding market microstructure features...\")\n",
    "    for symbol in tqdm(market_data.keys(), desc=\"Processing\"):\n",
    "        market_data[symbol] = add_microstructure_features(market_data[symbol])\n",
    "    print(\"‚úÖ Market microstructure features added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "matrix_section"
   },
   "source": [
    "## 5. Generate Agent-Specific Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlation_matrix"
   },
   "outputs": [],
   "source": [
    "# Generate correlation matrices for Regime Detector\n",
    "def generate_correlation_matrices(data_dict, window=20):\n",
    "    \"\"\"Generate rolling correlation matrices.\"\"\"\n",
    "    \n",
    "    # Combine returns data\n",
    "    returns_data = pd.DataFrame()\n",
    "    for symbol, data in data_dict.items():\n",
    "        if 'returns' in data.columns:\n",
    "            returns_data[symbol] = data['returns']\n",
    "    \n",
    "    # Align indices\n",
    "    returns_data = returns_data.dropna()\n",
    "    \n",
    "    # Calculate rolling correlations\n",
    "    correlation_matrices = []\n",
    "    dates = []\n",
    "    \n",
    "    print(f\"üìä Generating correlation matrices (window={window})...\")\n",
    "    \n",
    "    for i in tqdm(range(window, len(returns_data)), desc=\"Processing\"):\n",
    "        window_data = returns_data.iloc[i-window:i]\n",
    "        corr_matrix = window_data.corr().values\n",
    "        \n",
    "        # Replace NaN with 0\n",
    "        corr_matrix = np.nan_to_num(corr_matrix, 0)\n",
    "        \n",
    "        correlation_matrices.append(corr_matrix)\n",
    "        dates.append(returns_data.index[i])\n",
    "    \n",
    "    return np.array(correlation_matrices), dates, list(returns_data.columns)\n",
    "\n",
    "# Generate correlation matrices\n",
    "corr_matrices, corr_dates, corr_symbols = generate_correlation_matrices(\n",
    "    market_data, \n",
    "    DATA_CONFIG['correlation_window']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {corr_matrices.shape[0]} correlation matrices\")\n",
    "print(f\"   Shape: {corr_matrices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "volume_profile_matrix"
   },
   "outputs": [],
   "source": [
    "# Generate volume profile matrices for Structure Analyzer\n",
    "def generate_volume_profile_matrices(data_dict, n_bins=10):\n",
    "    \"\"\"Generate volume profile matrices.\"\"\"\n",
    "    \n",
    "    volume_profiles = []\n",
    "    dates = []\n",
    "    \n",
    "    # Get common dates\n",
    "    common_dates = None\n",
    "    for symbol, data in data_dict.items():\n",
    "        if common_dates is None:\n",
    "            common_dates = set(data.index)\n",
    "        else:\n",
    "            common_dates = common_dates.intersection(set(data.index))\n",
    "    common_dates = sorted(list(common_dates))\n",
    "    \n",
    "    print(f\"üìä Generating volume profile matrices (bins={n_bins})...\")\n",
    "    \n",
    "    for date in tqdm(common_dates[100:], desc=\"Processing\"):  # Skip first 100 days for history\n",
    "        daily_profiles = []\n",
    "        \n",
    "        for symbol, data in data_dict.items():\n",
    "            if date in data.index:\n",
    "                # Get last 20 days of data\n",
    "                idx = data.index.get_loc(date)\n",
    "                if idx >= 20:\n",
    "                    window_data = data.iloc[idx-20:idx+1]\n",
    "                    \n",
    "                    # Create volume profile\n",
    "                    price_range = window_data['High'].max() - window_data['Low'].min()\n",
    "                    if price_range > 0:\n",
    "                        bins = np.linspace(\n",
    "                            window_data['Low'].min(), \n",
    "                            window_data['High'].max(), \n",
    "                            n_bins + 1\n",
    "                        )\n",
    "                        \n",
    "                        profile = np.zeros(n_bins)\n",
    "                        for _, row in window_data.iterrows():\n",
    "                            # Distribute volume across price range\n",
    "                            bin_idx = np.digitize(\n",
    "                                (row['High'] + row['Low']) / 2, \n",
    "                                bins\n",
    "                            ) - 1\n",
    "                            if 0 <= bin_idx < n_bins:\n",
    "                                profile[bin_idx] += row['Volume']\n",
    "                        \n",
    "                        # Normalize\n",
    "                        if profile.sum() > 0:\n",
    "                            profile = profile / profile.sum()\n",
    "                        \n",
    "                        daily_profiles.append(profile)\n",
    "                    else:\n",
    "                        daily_profiles.append(np.zeros(n_bins))\n",
    "                else:\n",
    "                    daily_profiles.append(np.zeros(n_bins))\n",
    "            else:\n",
    "                daily_profiles.append(np.zeros(n_bins))\n",
    "        \n",
    "        if daily_profiles:\n",
    "            volume_profiles.append(np.array(daily_profiles))\n",
    "            dates.append(date)\n",
    "    \n",
    "    return np.array(volume_profiles), dates\n",
    "\n",
    "# Generate volume profile matrices\n",
    "volume_matrices, volume_dates = generate_volume_profile_matrices(\n",
    "    market_data,\n",
    "    DATA_CONFIG['volume_profile_bins']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {volume_matrices.shape[0]} volume profile matrices\")\n",
    "print(f\"   Shape: {volume_matrices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_matrices"
   },
   "outputs": [],
   "source": [
    "# Generate feature matrices for all agents\n",
    "def generate_feature_matrices(data_dict):\n",
    "    \"\"\"Generate feature matrices for all agents.\"\"\"\n",
    "    \n",
    "    # Select features for each agent\n",
    "    regime_features = ['returns', 'volume_ratio', 'atr', 'adx', 'vix_proxy']\n",
    "    structure_features = ['spread_pct', 'volume_profile', 'order_imbalance', 'amihud_illiquidity']\n",
    "    tactical_features = ['rsi', 'macd_diff', 'bb_position', 'stoch', 'close_sma_20_ratio']\n",
    "    risk_features = ['atr', 'bb_width', 'spread_pct', 'amihud_illiquidity', 'max_drawdown']\n",
    "    \n",
    "    # Combine all data\n",
    "    all_features = {}\n",
    "    \n",
    "    print(\"üî® Generating feature matrices for all agents...\")\n",
    "    \n",
    "    # Get common dates\n",
    "    common_dates = None\n",
    "    for symbol, data in data_dict.items():\n",
    "        if common_dates is None:\n",
    "            common_dates = set(data.index)\n",
    "        else:\n",
    "            common_dates = common_dates.intersection(set(data.index))\n",
    "    common_dates = sorted(list(common_dates))\n",
    "    \n",
    "    # Extract features\n",
    "    for agent_name, feature_list in [\n",
    "        ('regime_detector', regime_features),\n",
    "        ('structure_analyzer', structure_features),\n",
    "        ('tactical_trader', tactical_features),\n",
    "        ('risk_manager', risk_features)\n",
    "    ]:\n",
    "        agent_data = []\n",
    "        \n",
    "        for date in tqdm(common_dates[200:], desc=f\"Processing {agent_name}\"):\n",
    "            daily_features = []\n",
    "            \n",
    "            for symbol, data in data_dict.items():\n",
    "                if date in data.index:\n",
    "                    row_features = []\n",
    "                    for feat in feature_list:\n",
    "                        if feat in data.columns:\n",
    "                            row_features.append(data.loc[date, feat])\n",
    "                        elif feat == 'vix_proxy':  # Calculate VIX proxy\n",
    "                            idx = data.index.get_loc(date)\n",
    "                            if idx >= 20:\n",
    "                                returns = data['returns'].iloc[idx-20:idx]\n",
    "                                vix_proxy = returns.std() * np.sqrt(252) * 100\n",
    "                                row_features.append(vix_proxy)\n",
    "                            else:\n",
    "                                row_features.append(0)\n",
    "                        elif feat == 'max_drawdown':  # Calculate max drawdown\n",
    "                            idx = data.index.get_loc(date)\n",
    "                            if idx >= 50:\n",
    "                                prices = data['Close'].iloc[idx-50:idx+1]\n",
    "                                cum_returns = (1 + prices.pct_change()).cumprod()\n",
    "                                running_max = cum_returns.expanding().max()\n",
    "                                drawdown = (cum_returns - running_max) / running_max\n",
    "                                row_features.append(drawdown.min())\n",
    "                            else:\n",
    "                                row_features.append(0)\n",
    "                        else:\n",
    "                            row_features.append(0)\n",
    "                    \n",
    "                    daily_features.append(row_features)\n",
    "            \n",
    "            if daily_features:\n",
    "                agent_data.append(daily_features)\n",
    "        \n",
    "        all_features[agent_name] = np.array(agent_data)\n",
    "    \n",
    "    return all_features, common_dates[200:]\n",
    "\n",
    "# Generate feature matrices\n",
    "feature_matrices, feature_dates = generate_feature_matrices(market_data)\n",
    "\n",
    "print(\"\\n‚úÖ Feature matrices generated:\")\n",
    "for agent, matrix in feature_matrices.items():\n",
    "    print(f\"   {agent}: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "normalization_section"
   },
   "source": [
    "## 6. Data Normalization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalize_data"
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalize_data(data, method='robust', clip_threshold=5):\n",
    "    \"\"\"Normalize data using specified method.\"\"\"\n",
    "    \n",
    "    # Reshape data for normalization\n",
    "    original_shape = data.shape\n",
    "    if len(data.shape) == 3:\n",
    "        # Reshape from (samples, assets, features) to (samples*assets, features)\n",
    "        n_samples, n_assets, n_features = data.shape\n",
    "        data_reshaped = data.reshape(-1, n_features)\n",
    "    else:\n",
    "        data_reshaped = data\n",
    "    \n",
    "    # Replace inf and -inf with NaN\n",
    "    data_reshaped = np.where(np.isinf(data_reshaped), np.nan, data_reshaped)\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    data_reshaped = np.nan_to_num(data_reshaped, 0)\n",
    "    \n",
    "    # Normalize\n",
    "    if method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    data_normalized = scaler.fit_transform(data_reshaped)\n",
    "    \n",
    "    # Clip outliers\n",
    "    if clip_threshold:\n",
    "        data_normalized = np.clip(data_normalized, -clip_threshold, clip_threshold)\n",
    "    \n",
    "    # Reshape back\n",
    "    if len(original_shape) == 3:\n",
    "        data_normalized = data_normalized.reshape(original_shape)\n",
    "    \n",
    "    return data_normalized, scaler\n",
    "\n",
    "# Normalize all data\n",
    "print(\"üîß Normalizing data...\")\n",
    "\n",
    "normalized_features = {}\n",
    "scalers = {}\n",
    "\n",
    "# Normalize feature matrices\n",
    "for agent, matrix in feature_matrices.items():\n",
    "    normalized_features[agent], scalers[agent] = normalize_data(\n",
    "        matrix,\n",
    "        method=DATA_CONFIG['normalization_method'],\n",
    "        clip_threshold=DATA_CONFIG['outlier_threshold']\n",
    "    )\n",
    "    print(f\"   {agent}: normalized\")\n",
    "\n",
    "# Normalize correlation matrices (already in [-1, 1] range)\n",
    "normalized_features['correlation_matrices'] = corr_matrices\n",
    "\n",
    "# Normalize volume profiles (already normalized to sum to 1)\n",
    "normalized_features['volume_profiles'] = volume_matrices\n",
    "\n",
    "print(\"‚úÖ Data normalization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "split_section"
   },
   "source": [
    "## 7. Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_splits"
   },
   "outputs": [],
   "source": [
    "# Create data splits\n",
    "def create_data_splits(data, dates, train_ratio, val_ratio, test_ratio):\n",
    "    \"\"\"Create train/validation/test splits.\"\"\"\n",
    "    \n",
    "    n_samples = len(data)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = int(n_samples * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "    \n",
    "    # Split dates\n",
    "    train_dates = dates[:train_end]\n",
    "    val_dates = dates[train_end:val_end]\n",
    "    test_dates = dates[val_end:]\n",
    "    \n",
    "    return {\n",
    "        'train': (train_data, train_dates),\n",
    "        'val': (val_data, val_dates),\n",
    "        'test': (test_data, test_dates)\n",
    "    }\n",
    "\n",
    "# Create splits for all data\n",
    "print(\"‚úÇÔ∏è Creating data splits...\")\n",
    "\n",
    "data_splits = {}\n",
    "\n",
    "# Split feature matrices\n",
    "for agent, matrix in normalized_features.items():\n",
    "    if agent in ['correlation_matrices', 'volume_profiles']:\n",
    "        # Use appropriate dates\n",
    "        dates = corr_dates if agent == 'correlation_matrices' else volume_dates\n",
    "    else:\n",
    "        dates = feature_dates\n",
    "    \n",
    "    data_splits[agent] = create_data_splits(\n",
    "        matrix, dates,\n",
    "        DATA_CONFIG['train_ratio'],\n",
    "        DATA_CONFIG['val_ratio'],\n",
    "        DATA_CONFIG['test_ratio']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{agent}:\")\n",
    "    print(f\"  Train: {data_splits[agent]['train'][0].shape} samples\")\n",
    "    print(f\"  Val: {data_splits[agent]['val'][0].shape} samples\")\n",
    "    print(f\"  Test: {data_splits[agent]['test'][0].shape} samples\")\n",
    "\n",
    "print(\"\\n‚úÖ Data splits created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_hdf5"
   },
   "outputs": [],
   "source": [
    "# Save data to HDF5 format\n",
    "def save_to_hdf5(filename, data_splits, scalers, metadata):\n",
    "    \"\"\"Save processed data to HDF5 file.\"\"\"\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Save metadata\n",
    "        meta_group = f.create_group('metadata')\n",
    "        for key, value in metadata.items():\n",
    "            if isinstance(value, (list, dict)):\n",
    "                meta_group.attrs[key] = json.dumps(value)\n",
    "            else:\n",
    "                meta_group.attrs[key] = value\n",
    "        \n",
    "        # Save data splits\n",
    "        for agent, splits in data_splits.items():\n",
    "            agent_group = f.create_group(agent)\n",
    "            \n",
    "            for split_name, (data, dates) in splits.items():\n",
    "                split_group = agent_group.create_group(split_name)\n",
    "                \n",
    "                # Save data\n",
    "                split_group.create_dataset('data', data=data, compression='gzip')\n",
    "                \n",
    "                # Save dates as strings\n",
    "                date_strings = [str(d) for d in dates]\n",
    "                split_group.create_dataset(\n",
    "    'dates', \n",
    "    data=np.array(date_strings, dtype='S')\n",
    "                )\n",
    "        \n",
    "        # Save scalers parameters\n",
    "        if scalers:\n",
    "            scaler_group = f.create_group('scalers')\n",
    "            for agent, scaler in scalers.items():\n",
    "                agent_scaler = scaler_group.create_group(agent)\n",
    "                if hasattr(scaler, 'center_'):\n",
    "                    agent_scaler.create_dataset('center', data=scaler.center_)\n",
    "                    agent_scaler.create_dataset('scale', data=scaler.scale_)\n",
    "                elif hasattr(scaler, 'mean_'):\n",
    "                    agent_scaler.create_dataset('mean', data=scaler.mean_)\n",
    "                    agent_scaler.create_dataset('scale', data=scaler.scale_)\n",
    "    \n",
    "    print(f\"‚úÖ Data saved to {filename}\")\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'data_config': DATA_CONFIG,\n",
    "    'symbols': list(market_data.keys()),\n",
    "    'n_symbols': len(market_data),\n",
    "    'date_range': {\n",
    "        'start': DATA_CONFIG['start_date'],\n",
    "        'end': DATA_CONFIG['end_date']\n",
    "    },\n",
    "    'normalization_method': DATA_CONFIG['normalization_method'],\n",
    "    'features_per_agent': {\n",
    "        agent: matrix.shape[-1] if len(matrix.shape) > 2 else matrix.shape[1]\n",
    "        for agent, matrix in normalized_features.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to HDF5\n",
    "output_file = f\"{DRIVE_BASE}/data/processed/marl_training_data_{datetime.now().strftime('%Y%m%d')}.h5\"\n",
    "save_to_hdf5(output_file, data_splits, scalers, metadata)\n",
    "\n",
    "# Also save a compressed version\n",
    "print(\"\\nüì¶ Creating compressed version...\")\n",
    "import zipfile\n",
    "compressed_file = output_file.replace('.h5', '.zip')\n",
    "with zipfile.ZipFile(compressed_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(output_file, os.path.basename(output_file))\n",
    "print(f\"‚úÖ Compressed file saved: {compressed_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_saved_data"
   },
   "outputs": [],
   "source": [
    "# Verify saved data\n",
    "print(\"üîç Verifying saved data...\")\n",
    "\n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    print(\"\\nFile structure:\")\n",
    "    \n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  Dataset: {name} - Shape: {obj.shape}\")\n",
    "        elif isinstance(obj, h5py.Group):\n",
    "            print(f\"  Group: {name}\")\n",
    "    \n",
    "    f.visititems(print_structure)\n",
    "    \n",
    "    # Print metadata\n",
    "    print(\"\\nMetadata:\")\n",
    "    for key, value in f['metadata'].attrs.items():\n",
    "        if key == 'data_config':\n",
    "            print(f\"  {key}: <config dictionary>\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# File size\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "compressed_size_mb = os.path.getsize(compressed_file) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìä File sizes:\")\n",
    "print(f\"  Original: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Compressed: {compressed_size_mb:.2f} MB\")\n",
    "print(f\"  Compression ratio: {file_size_mb/compressed_size_mb:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_section"
   },
   "source": [
    "## 9. Data Upload Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_summary"
   },
   "outputs": [],
   "source": [
    "# Create data preparation summary\n",
    "summary = f\"\"\"\n",
    "# Data Preparation Summary\n",
    "\n",
    "## Dataset Information\n",
    "- **Created**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Symbols**: {len(market_data)} assets\n",
    "- **Date Range**: {DATA_CONFIG['start_date']} to {DATA_CONFIG['end_date']}\n",
    "- **Total Days**: {len(feature_dates)}\n",
    "\n",
    "## Data Splits\n",
    "- **Train**: {DATA_CONFIG['train_ratio']*100:.0f}% ({len(data_splits['regime_detector']['train'][0])} samples)\n",
    "- **Validation**: {DATA_CONFIG['val_ratio']*100:.0f}% ({len(data_splits['regime_detector']['val'][0])} samples)\n",
    "- **Test**: {DATA_CONFIG['test_ratio']*100:.0f}% ({len(data_splits['regime_detector']['test'][0])} samples)\n",
    "\n",
    "## Feature Matrices\n",
    "\"\"\"\n",
    "\n",
    "for agent, splits in data_splits.items():\n",
    "    train_shape = splits['train'][0].shape\n",
    "    summary += f\"\\n### {agent}\"\n",
    "    summary += f\"\\n- Shape: {train_shape}\"\n",
    "    if len(train_shape) == 3:\n",
    "        summary += f\" (samples √ó assets √ó features)\"\n",
    "    elif len(train_shape) == 2:\n",
    "        summary += f\" (samples √ó features)\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "\n",
    "## Technical Indicators Added\n",
    "- Price-based: returns, log returns, ratios\n",
    "- Moving averages: SMA, EMA (multiple periods)\n",
    "- Volatility: ATR, Bollinger Bands\n",
    "- Momentum: RSI, MACD, Stochastic\n",
    "- Trend: ADX, CCI\n",
    "- Microstructure: spread, order imbalance, illiquidity\n",
    "\n",
    "## File Information\n",
    "- **Output File**: `{os.path.basename(output_file)}`\n",
    "- **File Size**: {file_size_mb:.2f} MB\n",
    "- **Compressed Size**: {compressed_size_mb:.2f} MB\n",
    "- **Location**: `{DRIVE_BASE}/data/processed/`\n",
    "\n",
    "## Usage in Training\n",
    "```python\n",
    "# Load data in training notebook\n",
    "import h5py\n",
    "\n",
    "with h5py.File('{output_file}', 'r') as f:\n",
    "    # Load regime detector training data\n",
    "    regime_train = f['regime_detector/train/data'][:]\n",
    "    regime_train_dates = f['regime_detector/train/dates'][:]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "summary_file = f\"{DRIVE_BASE}/data/processed/data_preparation_summary_{datetime.now().strftime('%Y%m%d')}.md\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary)\n",
    "print(f\"\\n‚úÖ Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_message"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüéâ Data preparation complete!\")\n",
    "print(\"\\nüìã Next steps:\")\n",
    "print(\"1. The processed data is now saved in your Google Drive\")\n",
    "print(\"2. Use the Master Training Notebook to load this data\")\n",
    "print(\"3. The data is optimized for efficient loading in Colab\")\n",
    "print(\"\\nData file path for training:\")\n",
    "print(f\"'{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}