{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_cell"
   },
   "source": "# AlgoSpace Advanced Data Preparation Pipeline\n\nThis notebook implements a comprehensive feature engineering pipeline for the AlgoSpace MARL trading system.\n\n## Key Features:\n- ES futures data processing with Heiken Ashi transformation\n- Advanced LVN (Low Volume Node) strength scoring\n- MMD (Maximum Mean Discrepancy) feature vector calculation for the Regime Detection Engine\n- Interaction feature engineering\n- Separate output files for main MARL and RDE training\n\n## Architecture Context:\nThis data preparation supports a hierarchical system with three main models:\n1. **Regime Detection Engine (RDE):** Uses MMD feature vectors to learn market regimes\n2. **Risk Management Sub-system (M-RMS):** Uses trade plan features\n3. **Main MARL Core:** Uses all features for final trading decisions",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_environment"
   },
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âš ï¸ Not running in Google Colab\")\n",
    "\n",
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/AlgoSpace\"\n",
    "    !mkdir -p {DRIVE_BASE}/data/{raw,processed,compressed}\n",
    "else:\n",
    "    DRIVE_BASE = \"./drive_simulation\"\n",
    "    import os\n",
    "    os.makedirs(f\"{DRIVE_BASE}/data/raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_BASE}/data/processed\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": "# Install required packages\n!pip install -q yfinance pandas numpy h5py pyarrow\n!pip install -q ta pandas-ta\n!pip install -q scikit-learn tqdm\n!pip install -q signatory  # For path signatures\n!pip install -q pot  # For MMD calculations\n!pip install -q scipy matplotlib seaborn\n\nprint(\"âœ… Packages installed\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": "# Import libraries\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport h5py\nfrom datetime import datetime, timedelta\nimport ta\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Advanced feature engineering imports\nfrom scipy import stats\nfrom scipy.spatial.distance import cdist\nimport ot  # Optimal transport for MMD\nfrom collections import deque\n\nprint(\"âœ… Libraries imported\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_config_section"
   },
   "source": [
    "## 2. Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "data_configuration"
   },
   "outputs": [],
   "source": "# Data configuration for ES futures\nDATA_CONFIG = {\n    # ES futures symbol (using SPY as proxy for demonstration)\n    'symbol': 'ES=F',  # E-mini S&P 500 futures\n    'proxy_symbol': 'SPY',  # Use SPY if ES futures data not available\n    \n    # Date range (5 years)\n    'start_date': '2019-01-01',\n    'end_date': '2023-12-31',\n    \n    # Data splits\n    'train_ratio': 0.7,\n    'val_ratio': 0.15,\n    'test_ratio': 0.15,\n    \n    # Heiken Ashi parameters\n    'ha_timeframe': '30min',\n    \n    # LVN parameters\n    'volume_profile_bins': 50,\n    'lvn_lookback_days': 20,\n    'lvn_threshold_percentile': 30,  # Bottom 30% of volume nodes\n    \n    # MMD parameters\n    'mmd_window_size': 96,  # 96 * 30min = 48 hours\n    'n_market_regimes': 7,\n    'path_signature_depth': 3,\n    \n    # Feature engineering\n    'lookback_periods': [5, 10, 20, 50, 100, 200],\n    'technical_indicators': True,\n    'market_microstructure': True,\n    \n    # Normalization\n    'normalization_method': 'robust',\n    'clip_outliers': True,\n    'outlier_threshold': 5\n}\n\nprint(\"ðŸ“‹ Data Configuration:\")\nprint(f\"- Symbol: {DATA_CONFIG['symbol']} (or {DATA_CONFIG['proxy_symbol']} as proxy)\")\nprint(f\"- Date Range: {DATA_CONFIG['start_date']} to {DATA_CONFIG['end_date']}\")\nprint(f\"- Heiken Ashi Timeframe: {DATA_CONFIG['ha_timeframe']}\")\nprint(f\"- Market Regimes: {DATA_CONFIG['n_market_regimes']}\")\nprint(f\"- MMD Window Size: {DATA_CONFIG['mmd_window_size']} bars\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## 3. Download Market Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "download_data"
   },
   "outputs": [],
   "source": "# Download ES futures data\ndef download_es_futures_data(config):\n    \"\"\"Download ES futures data or use proxy if not available.\"\"\"\n    \n    print(f\"ðŸ“¥ Downloading ES futures data...\")\n    \n    try:\n        # Try to download ES futures data\n        ticker = yf.Ticker(config['symbol'])\n        data = ticker.history(start=config['start_date'], end=config['end_date'], interval='30m')\n        \n        if len(data) > 0:\n            print(f\"âœ… Downloaded {config['symbol']} data: {len(data)} rows\")\n            data_symbol = config['symbol']\n        else:\n            raise ValueError(\"No ES futures data available\")\n            \n    except Exception as e:\n        print(f\"âš ï¸ ES futures data not available: {e}\")\n        print(f\"ðŸ“¥ Using {config['proxy_symbol']} as proxy...\")\n        \n        # Use proxy data\n        ticker = yf.Ticker(config['proxy_symbol'])\n        data = ticker.history(start=config['start_date'], end=config['end_date'], interval='30m')\n        data_symbol = config['proxy_symbol']\n        \n        if len(data) == 0:\n            raise ValueError(\"No data available for proxy symbol either\")\n        \n        print(f\"âœ… Downloaded {config['proxy_symbol']} proxy data: {len(data)} rows\")\n    \n    # Clean data\n    data = data.dropna()\n    data = data[data['Volume'] > 0]\n    \n    # Forward fill any remaining gaps\n    data = data.fillna(method='ffill')\n    \n    return data, data_symbol\n\n# Download the data\nes_data, used_symbol = download_es_futures_data(DATA_CONFIG)\n\nprint(f\"\\nðŸ“Š Data Overview:\")\nprint(f\"  - Symbol Used: {used_symbol}\")\nprint(f\"  - Shape: {es_data.shape}\")\nprint(f\"  - Date Range: {es_data.index[0]} to {es_data.index[-1]}\")\nprint(f\"  - Columns: {list(es_data.columns)}\")"
  },
  {
   "cell_type": "code",
   "source": "# Calculate Heiken Ashi data\ndef calculate_heiken_ashi(df):\n    \"\"\"Calculate Heiken Ashi candles from OHLC data.\"\"\"\n    \n    ha_df = pd.DataFrame(index=df.index)\n    \n    # Calculate Heiken Ashi values\n    ha_df['HA_Close'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4\n    \n    # Initialize first HA_Open\n    ha_df['HA_Open'] = 0.0\n    ha_df.iloc[0, ha_df.columns.get_loc('HA_Open')] = (df['Open'].iloc[0] + df['Close'].iloc[0]) / 2\n    \n    # Calculate subsequent HA_Open values\n    for i in range(1, len(ha_df)):\n        ha_df.iloc[i, ha_df.columns.get_loc('HA_Open')] = (\n            ha_df.iloc[i-1, ha_df.columns.get_loc('HA_Open')] + \n            ha_df.iloc[i-1, ha_df.columns.get_loc('HA_Close')]\n        ) / 2\n    \n    # Calculate HA High and Low\n    ha_df['HA_High'] = pd.concat([df['High'], ha_df['HA_Open'], ha_df['HA_Close']], axis=1).max(axis=1)\n    ha_df['HA_Low'] = pd.concat([df['Low'], ha_df['HA_Open'], ha_df['HA_Close']], axis=1).min(axis=1)\n    \n    # Add volume (same as original)\n    ha_df['HA_Volume'] = df['Volume']\n    \n    # Add derived features\n    ha_df['HA_Body'] = ha_df['HA_Close'] - ha_df['HA_Open']\n    ha_df['HA_UpperShadow'] = ha_df['HA_High'] - pd.concat([ha_df['HA_Open'], ha_df['HA_Close']], axis=1).max(axis=1)\n    ha_df['HA_LowerShadow'] = pd.concat([ha_df['HA_Open'], ha_df['HA_Close']], axis=1).min(axis=1) - ha_df['HA_Low']\n    ha_df['HA_Direction'] = np.sign(ha_df['HA_Body'])\n    \n    return ha_df\n\n# Calculate Heiken Ashi\nha_data = calculate_heiken_ashi(es_data)\n\n# Combine original and HA data\ncombined_data = pd.concat([es_data, ha_data], axis=1)\n\nprint(\"âœ… Heiken Ashi data calculated\")\nprint(f\"   Combined data shape: {combined_data.shape}\")\nprint(f\"   HA columns: {[col for col in ha_data.columns]}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Advanced LVN (Low Volume Node) calculation and strength scoring\ndef identify_lvns_with_strength(data, config):\n    \"\"\"Identify LVNs from volume profile and calculate strength scores.\"\"\"\n    \n    print(\"ðŸ” Identifying LVNs and calculating strength scores...\")\n    \n    lvn_data = []\n    lookback = config['lvn_lookback_days'] * 48  # Convert days to 30-min bars\n    \n    for i in tqdm(range(lookback, len(data)), desc=\"Processing LVNs\"):\n        # Get window data\n        window = data.iloc[i-lookback:i]\n        \n        # Create volume profile\n        price_min = window['HA_Low'].min()\n        price_max = window['HA_High'].max()\n        bins = np.linspace(price_min, price_max, config['volume_profile_bins'] + 1)\n        \n        # Calculate volume at each price level\n        volume_profile = np.zeros(config['volume_profile_bins'])\n        \n        for idx, row in window.iterrows():\n            # Distribute volume across the bar's range\n            bar_low_bin = np.searchsorted(bins, row['HA_Low'], side='left')\n            bar_high_bin = np.searchsorted(bins, row['HA_High'], side='right')\n            \n            if bar_high_bin > bar_low_bin:\n                volume_per_bin = row['HA_Volume'] / (bar_high_bin - bar_low_bin)\n                for bin_idx in range(max(0, bar_low_bin), min(config['volume_profile_bins'], bar_high_bin)):\n                    volume_profile[bin_idx] += volume_per_bin\n        \n        # Identify LVNs (low volume nodes)\n        threshold = np.percentile(volume_profile, config['lvn_threshold_percentile'])\n        lvn_indices = np.where(volume_profile < threshold)[0]\n        \n        # Calculate LVN levels and strength scores\n        current_lvns = []\n        for lvn_idx in lvn_indices:\n            lvn_price = (bins[lvn_idx] + bins[lvn_idx + 1]) / 2\n            strength_score = calculate_lvn_strength(\n                lvn_price, \n                window, \n                data.iloc[i:min(i+48, len(data))],  # Look ahead 1 day for strength\n                volume_profile[lvn_idx],\n                np.mean(volume_profile)\n            )\n            current_lvns.append({\n                'price': lvn_price,\n                'strength': strength_score,\n                'volume_ratio': volume_profile[lvn_idx] / (np.mean(volume_profile) + 1e-10)\n            })\n        \n        # Sort by strength and keep top LVNs\n        current_lvns.sort(key=lambda x: x['strength'], reverse=True)\n        \n        lvn_data.append({\n            'timestamp': data.index[i],\n            'lvns': current_lvns[:5],  # Keep top 5 LVNs\n            'strongest_lvn_price': current_lvns[0]['price'] if current_lvns else np.nan,\n            'strongest_lvn_strength': current_lvns[0]['strength'] if current_lvns else 0,\n            'n_lvns': len(current_lvns)\n        })\n    \n    return pd.DataFrame(lvn_data).set_index('timestamp')\n\n\ndef calculate_lvn_strength(level, historical_window, future_window, lvn_volume, avg_volume):\n    \"\"\"\n    Calculate LVN strength score (0-100) based on:\n    - Number of tests\n    - Magnitude of bounces\n    - Recency\n    - Volume characteristics\n    \"\"\"\n    \n    strength_components = []\n    \n    # 1. Test frequency score (how many times price tested this level)\n    tolerance = 0.001  # 0.1% tolerance\n    tests = 0\n    bounces = []\n    \n    for idx, row in historical_window.iterrows():\n        if abs(row['HA_Low'] - level) / level < tolerance or abs(row['HA_High'] - level) / level < tolerance:\n            tests += 1\n            # Calculate bounce magnitude\n            next_bars = historical_window.loc[idx:].iloc[1:6]  # Next 5 bars\n            if len(next_bars) > 0:\n                bounce_magnitude = abs(next_bars['HA_Close'].iloc[-1] - row['HA_Close']) / row['HA_Close']\n                bounces.append(bounce_magnitude)\n    \n    test_score = min(tests * 10, 30)  # Max 30 points for tests\n    \n    # 2. Bounce magnitude score\n    if bounces:\n        avg_bounce = np.mean(bounces) * 100  # Convert to percentage\n        bounce_score = min(avg_bounce * 5, 25)  # Max 25 points for bounces\n    else:\n        bounce_score = 0\n    \n    # 3. Recency score (more recent tests score higher)\n    recency_weights = np.linspace(0.5, 1.0, len(historical_window))\n    recent_tests = 0\n    \n    for i, (idx, row) in enumerate(historical_window.iterrows()):\n        if abs(row['HA_Low'] - level) / level < tolerance or abs(row['HA_High'] - level) / level < tolerance:\n            recent_tests += recency_weights[i]\n    \n    recency_score = min(recent_tests * 5, 20)  # Max 20 points for recency\n    \n    # 4. Volume void score (lower volume = stronger LVN)\n    volume_void_score = (1 - lvn_volume / avg_volume) * 15  # Max 15 points\n    \n    # 5. Future validation score (did the level hold in the near future?)\n    future_score = 0\n    if len(future_window) > 0:\n        future_tests = 0\n        for idx, row in future_window.iterrows():\n            if abs(row['HA_Low'] - level) / level < tolerance:\n                future_tests += 1\n                if row['HA_Close'] > level:  # Bounce up from level\n                    future_score += 2\n        \n        future_score = min(future_score, 10)  # Max 10 points\n    \n    # Calculate total strength score\n    total_score = test_score + bounce_score + recency_score + volume_void_score + future_score\n    \n    # Normalize to 0-100\n    return min(total_score, 100)\n\n\n# Calculate LVNs with strength scores\nlvn_df = identify_lvns_with_strength(combined_data, DATA_CONFIG)\n\n# Merge with main data\ncombined_data = combined_data.join(lvn_df[['strongest_lvn_price', 'strongest_lvn_strength', 'n_lvns']], how='left')\ncombined_data.fillna(method='ffill', inplace=True)\n\nprint(\"âœ… LVN analysis complete\")\nprint(f\"   LVN columns added: {['strongest_lvn_price', 'strongest_lvn_strength', 'n_lvns']}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# MMD Feature Vector Calculation for RDE\ndef identify_market_regimes(data, n_regimes=7):\n    \"\"\"Identify archetypal market regimes using unsupervised clustering.\"\"\"\n    \n    print(\"ðŸŽ¯ Identifying market regimes...\")\n    \n    # Extract features for clustering\n    features = []\n    window_size = 48  # 1 day of 30-min bars\n    \n    for i in range(window_size, len(data)):\n        window = data.iloc[i-window_size:i]\n        \n        # Calculate regime features\n        regime_features = {\n            'volatility': window['HA_Close'].pct_change().std() * np.sqrt(48 * 252),  # Annualized\n            'momentum': (window['HA_Close'].iloc[-1] / window['HA_Close'].iloc[0] - 1),\n            'volume_trend': window['HA_Volume'].mean() / window['HA_Volume'].iloc[:24].mean(),\n            'range_ratio': (window['HA_High'].max() - window['HA_Low'].min()) / window['HA_Close'].mean(),\n            'direction_consistency': window['HA_Direction'].mean(),\n            'shadow_ratio': (window['HA_UpperShadow'] + window['HA_LowerShadow']).mean() / window['HA_Body'].abs().mean()\n        }\n        \n        features.append(list(regime_features.values()))\n    \n    features = np.array(features)\n    \n    # Normalize features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features)\n    \n    # Fit Gaussian Mixture Model\n    gmm = GaussianMixture(n_components=n_regimes, covariance_type='full', random_state=42)\n    gmm.fit(features_scaled)\n    \n    # Get regime assignments\n    regime_labels = gmm.predict(features_scaled)\n    \n    # Calculate regime centers (reference signatures)\n    regime_centers = gmm.means_\n    \n    print(f\"âœ… Identified {n_regimes} market regimes\")\n    \n    # Show regime distribution\n    unique, counts = np.unique(regime_labels, return_counts=True)\n    for regime, count in zip(unique, counts):\n        print(f\"   Regime {regime}: {count} samples ({count/len(regime_labels)*100:.1f}%)\")\n    \n    return gmm, scaler, regime_centers, regime_labels\n\n\ndef calculate_path_signature(window_data, depth=3):\n    \"\"\"Calculate path signature features for a data window.\"\"\"\n    \n    # Extract relevant price/volume paths\n    paths = np.column_stack([\n        window_data['HA_Close'].values,\n        window_data['HA_Volume'].values,\n        window_data['HA_Body'].values\n    ])\n    \n    # Normalize paths\n    paths_normalized = (paths - paths.mean(axis=0)) / (paths.std(axis=0) + 1e-10)\n    \n    # Simple signature approximation (without signatory library)\n    signature_features = []\n    \n    # Level 1: increments\n    increments = np.diff(paths_normalized, axis=0)\n    signature_features.extend(increments.mean(axis=0))\n    signature_features.extend(increments.std(axis=0))\n    \n    # Level 2: areas (simplified)\n    if depth >= 2:\n        for i in range(paths_normalized.shape[1]):\n            for j in range(i, paths_normalized.shape[1]):\n                area = np.sum(paths_normalized[:-1, i] * np.diff(paths_normalized[:, j]))\n                signature_features.append(area)\n    \n    # Level 3: volumes (simplified)\n    if depth >= 3:\n        for i in range(min(3, paths_normalized.shape[1])):\n            volume = np.sum(paths_normalized[:-2, i] * np.diff(paths_normalized[:-1, i]) * np.diff(paths_normalized[1:, i]))\n            signature_features.append(volume)\n    \n    return np.array(signature_features)\n\n\ndef calculate_mmd_features(data, regime_centers, scaler, config):\n    \"\"\"Calculate MMD feature vectors for each time window.\"\"\"\n    \n    print(\"ðŸ“Š Calculating MMD feature vectors...\")\n    \n    mmd_features_list = []\n    window_size = config['mmd_window_size']\n    \n    # Identify regimes and calculate reference signatures\n    gmm, regime_scaler, regime_centers, _ = identify_market_regimes(data, config['n_market_regimes'])\n    \n    # Calculate reference path signatures for each regime\n    reference_signatures = {}\n    regime_windows = {i: [] for i in range(config['n_market_regimes'])}\n    \n    # Group windows by regime\n    for i in tqdm(range(window_size, len(data)), desc=\"Grouping by regime\"):\n        window = data.iloc[i-window_size:i]\n        \n        # Extract features for regime classification\n        regime_features = np.array([\n            window['HA_Close'].pct_change().std() * np.sqrt(48 * 252),\n            (window['HA_Close'].iloc[-1] / window['HA_Close'].iloc[0] - 1),\n            window['HA_Volume'].mean() / window['HA_Volume'].iloc[:48].mean(),\n            (window['HA_High'].max() - window['HA_Low'].min()) / window['HA_Close'].mean(),\n            window['HA_Direction'].mean(),\n            (window['HA_UpperShadow'] + window['HA_LowerShadow']).mean() / (window['HA_Body'].abs().mean() + 1e-10)\n        ]).reshape(1, -1)\n        \n        regime = gmm.predict(regime_scaler.transform(regime_features))[0]\n        regime_windows[regime].append(window)\n    \n    # Calculate reference signatures\n    for regime, windows in regime_windows.items():\n        if windows:\n            # Use median window as reference\n            signatures = [calculate_path_signature(w, config['path_signature_depth']) for w in windows[:50]]\n            reference_signatures[regime] = np.median(signatures, axis=0)\n        else:\n            reference_signatures[regime] = np.zeros(10)  # Default signature\n    \n    # Calculate MMD features for each window\n    for i in tqdm(range(window_size, len(data)), desc=\"Calculating MMD\"):\n        window = data.iloc[i-window_size:i]\n        \n        # Calculate window signature\n        window_signature = calculate_path_signature(window, config['path_signature_depth'])\n        \n        # Calculate MMD scores against each reference signature\n        mmd_scores = []\n        for regime in range(config['n_market_regimes']):\n            ref_sig = reference_signatures[regime]\n            \n            # Simple MMD approximation using L2 distance\n            mmd = np.linalg.norm(window_signature - ref_sig)\n            mmd_scores.append(mmd)\n        \n        # Additional statistical features\n        stat_features = [\n            window['HA_Close'].pct_change().std() * np.sqrt(48 * 252),  # Realized volatility\n            stats.skew(window['HA_Close'].pct_change().dropna()),  # Skewness\n            stats.kurtosis(window['HA_Close'].pct_change().dropna()),  # Kurtosis\n            window['HA_Volume'].std() / window['HA_Volume'].mean(),  # Volume coefficient of variation\n            np.corrcoef(window['HA_Close'].values[:-1], window['HA_Close'].values[1:])[0, 1],  # Autocorrelation\n        ]\n        \n        # Combine all features into MMD feature vector\n        mmd_feature_vector = np.concatenate([mmd_scores, stat_features])\n        \n        mmd_features_list.append({\n            'timestamp': data.index[i],\n            'mmd_features': mmd_feature_vector,\n            'dominant_regime': np.argmin(mmd_scores)\n        })\n    \n    return pd.DataFrame(mmd_features_list).set_index('timestamp'), reference_signatures\n\n\n# Calculate MMD features\nmmd_df, reference_signatures = calculate_mmd_features(combined_data, None, None, DATA_CONFIG)\n\nprint(\"âœ… MMD feature calculation complete\")\nprint(f\"   MMD feature vector size: {len(mmd_df['mmd_features'].iloc[0])}\")\nprint(f\"   Features: {DATA_CONFIG['n_market_regimes']} MMD scores + 5 statistical features\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_section"
   },
   "source": "# Add technical indicators including MLMI and NWRQK\ndef add_technical_indicators(df, lookback_periods):\n    \"\"\"Add technical indicators to the dataframe.\"\"\"\n    \n    # Price-based features\n    df['returns'] = df['Close'].pct_change()\n    df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))\n    df['high_low_ratio'] = df['High'] / df['Low']\n    df['close_open_ratio'] = df['Close'] / df['Open']\n    \n    # Heiken Ashi specific features\n    df['ha_returns'] = df['HA_Close'].pct_change()\n    df['ha_body_ratio'] = df['HA_Body'] / df['HA_Close']\n    df['ha_shadow_imbalance'] = (df['HA_UpperShadow'] - df['HA_LowerShadow']) / (df['HA_UpperShadow'] + df['HA_LowerShadow'] + 1e-10)\n    \n    # Volume features\n    df['volume_sma'] = df['Volume'].rolling(window=20).mean()\n    df['volume_ratio'] = df['Volume'] / df['volume_sma']\n    df['dollar_volume'] = df['Close'] * df['Volume']\n    df['ha_volume_trend'] = df['HA_Volume'].rolling(window=10).mean() / df['HA_Volume'].rolling(window=50).mean()\n    \n    # Moving averages\n    for period in lookback_periods:\n        df[f'sma_{period}'] = df['Close'].rolling(window=period).mean()\n        df[f'ema_{period}'] = df['Close'].ewm(span=period).mean()\n        df[f'close_sma_{period}_ratio'] = df['Close'] / df[f'sma_{period}']\n        df[f'ha_sma_{period}'] = df['HA_Close'].rolling(window=period).mean()\n    \n    # Volatility indicators\n    df['atr'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n    df['ha_atr'] = ta.volatility.average_true_range(df['HA_High'], df['HA_Low'], df['HA_Close'])\n    df['bb_high'], df['bb_mid'], df['bb_low'] = ta.volatility.bollinger_hband(df['Close']), \\\n                                                 ta.volatility.bollinger_mavg(df['Close']), \\\n                                                 ta.volatility.bollinger_lband(df['Close'])\n    df['bb_width'] = (df['bb_high'] - df['bb_low']) / df['bb_mid']\n    df['bb_position'] = (df['Close'] - df['bb_low']) / (df['bb_high'] - df['bb_low'])\n    \n    # Momentum indicators\n    df['rsi'] = ta.momentum.rsi(df['Close'])\n    df['ha_rsi'] = ta.momentum.rsi(df['HA_Close'])\n    df['macd'] = ta.trend.macd(df['Close'])\n    df['macd_signal'] = ta.trend.macd_signal(df['Close'])\n    df['macd_diff'] = df['macd'] - df['macd_signal']\n    df['stoch'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n    \n    # Trend indicators\n    df['adx'] = ta.trend.adx(df['High'], df['Low'], df['Close'])\n    df['cci'] = ta.trend.cci(df['High'], df['Low'], df['Close'])\n    df['aroon_up'] = ta.trend.aroon_up(df['High'], df['Low'])\n    df['aroon_down'] = ta.trend.aroon_down(df['High'], df['Low'])\n    \n    # Support/Resistance levels\n    for period in [20, 50]:\n        df[f'resistance_{period}'] = df['High'].rolling(window=period).max()\n        df[f'support_{period}'] = df['Low'].rolling(window=period).min()\n        df[f'sr_ratio_{period}'] = (df['Close'] - df[f'support_{period}']) / \\\n                                   (df[f'resistance_{period}'] - df[f'support_{period}'])\n    \n    # MLMI (Market Level Momentum Indicator) - Custom implementation\n    def calculate_mlmi(data, period=14):\n        \"\"\"Calculate custom MLMI indicator.\"\"\"\n        # Momentum component\n        momentum = data['Close'].diff(period) / data['Close'].shift(period)\n        \n        # Level component (position relative to recent range)\n        high_range = data['High'].rolling(window=period).max()\n        low_range = data['Low'].rolling(window=period).min()\n        level = (data['Close'] - low_range) / (high_range - low_range + 1e-10)\n        \n        # Combine momentum and level\n        mlmi = momentum * 0.6 + level * 0.4\n        \n        return mlmi\n    \n    df['mlmi'] = calculate_mlmi(df)\n    \n    # NWRQK (Normalized Weighted Range Quality K) - Custom implementation\n    def calculate_nwrqk(data, k_period=10):\n        \"\"\"Calculate custom NWRQK indicator.\"\"\"\n        # Range quality\n        true_range = pd.DataFrame({\n            'hl': data['High'] - data['Low'],\n            'hc': abs(data['High'] - data['Close'].shift(1)),\n            'lc': abs(data['Low'] - data['Close'].shift(1))\n        }).max(axis=1)\n        \n        # Weighted by volume\n        volume_weight = data['Volume'] / data['Volume'].rolling(window=k_period).mean()\n        weighted_range = true_range * volume_weight\n        \n        # Normalize\n        nwrqk = weighted_range / weighted_range.rolling(window=k_period).mean()\n        \n        return nwrqk\n    \n    df['nwrqk'] = calculate_nwrqk(df)\n    \n    # Interaction features\n    df['mlmi_minus_nwrqk'] = df['mlmi'] - df['nwrqk']\n    df['mlmi_times_nwrqk'] = df['mlmi'] * df['nwrqk']\n    df['mlmi_nwrqk_ratio'] = df['mlmi'] / (df['nwrqk'] + 1e-10)\n    \n    # Additional interaction features\n    df['rsi_bb_interaction'] = df['rsi'] * df['bb_position']\n    df['macd_adx_interaction'] = df['macd_diff'] * df['adx']\n    df['volume_atr_interaction'] = df['volume_ratio'] * df['atr']\n    \n    return df\n\n# Apply technical indicators\nprint(\"ðŸ”§ Adding technical indicators...\")\ncombined_data = add_technical_indicators(combined_data, DATA_CONFIG['lookback_periods'])\n\nprint(\"âœ… Technical indicators added\")\nprint(f\"   Total features: {len(combined_data.columns)}\")\nprint(f\"   Key indicators: MLMI, NWRQK, RSI, MACD, ADX, ATR, BB\")\nprint(f\"   Interaction features: mlmi_minus_nwrqk, mlmi_times_nwrqk, etc.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "technical_indicators"
   },
   "outputs": [],
   "source": "# Add technical indicators including MLMI and NWRQK\ndef add_technical_indicators(df, lookback_periods):\n    \"\"\"Add technical indicators to the dataframe.\"\"\"\n    \n    # Price-based features\n    df['returns'] = df['Close'].pct_change()\n    df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))\n    df['high_low_ratio'] = df['High'] / df['Low']\n    df['close_open_ratio'] = df['Close'] / df['Open']\n    \n    # Heiken Ashi specific features\n    df['ha_returns'] = df['HA_Close'].pct_change()\n    df['ha_body_ratio'] = df['HA_Body'] / df['HA_Close']\n    df['ha_shadow_imbalance'] = (df['HA_UpperShadow'] - df['HA_LowerShadow']) / (df['HA_UpperShadow'] + df['HA_LowerShadow'] + 1e-10)\n    \n    # Volume features\n    df['volume_sma'] = df['Volume'].rolling(window=20).mean()\n    df['volume_ratio'] = df['Volume'] / df['volume_sma']\n    df['dollar_volume'] = df['Close'] * df['Volume']\n    df['ha_volume_trend'] = df['HA_Volume'].rolling(window=10).mean() / df['HA_Volume'].rolling(window=50).mean()\n    \n    # Moving averages\n    for period in lookback_periods:\n        df[f'sma_{period}'] = df['Close'].rolling(window=period).mean()\n        df[f'ema_{period}'] = df['Close'].ewm(span=period).mean()\n        df[f'close_sma_{period}_ratio'] = df['Close'] / df[f'sma_{period}']\n        df[f'ha_sma_{period}'] = df['HA_Close'].rolling(window=period).mean()\n    \n    # Volatility indicators\n    df['atr'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n    df['ha_atr'] = ta.volatility.average_true_range(df['HA_High'], df['HA_Low'], df['HA_Close'])\n    df['bb_high'], df['bb_mid'], df['bb_low'] = ta.volatility.bollinger_hband(df['Close']), \\\n                                                 ta.volatility.bollinger_mavg(df['Close']), \\\n                                                 ta.volatility.bollinger_lband(df['Close'])\n    df['bb_width'] = (df['bb_high'] - df['bb_low']) / df['bb_mid']\n    df['bb_position'] = (df['Close'] - df['bb_low']) / (df['bb_high'] - df['bb_low'])\n    \n    # Momentum indicators\n    df['rsi'] = ta.momentum.rsi(df['Close'])\n    df['ha_rsi'] = ta.momentum.rsi(df['HA_Close'])\n    df['macd'] = ta.trend.macd(df['Close'])\n    df['macd_signal'] = ta.trend.macd_signal(df['Close'])\n    df['macd_diff'] = df['macd'] - df['macd_signal']\n    df['stoch'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n    \n    # Trend indicators\n    df['adx'] = ta.trend.adx(df['High'], df['Low'], df['Close'])\n    df['cci'] = ta.trend.cci(df['High'], df['Low'], df['Close'])\n    df['aroon_up'] = ta.trend.aroon_up(df['High'], df['Low'])\n    df['aroon_down'] = ta.trend.aroon_down(df['High'], df['Low'])\n    \n    # Support/Resistance levels\n    for period in [20, 50]:\n        df[f'resistance_{period}'] = df['High'].rolling(window=period).max()\n        df[f'support_{period}'] = df['Low'].rolling(window=period).min()\n        df[f'sr_ratio_{period}'] = (df['Close'] - df[f'support_{period}']) / \\\n                                   (df[f'resistance_{period}'] - df[f'support_{period}'])\n    \n    # MLMI (Market Level Momentum Indicator) - Custom implementation\n    def calculate_mlmi(data, period=14):\n        \"\"\"Calculate custom MLMI indicator.\"\"\"\n        # Momentum component\n        momentum = data['Close'].diff(period) / data['Close'].shift(period)\n        \n        # Level component (position relative to recent range)\n        high_range = data['High'].rolling(window=period).max()\n        low_range = data['Low'].rolling(window=period).min()\n        level = (data['Close'] - low_range) / (high_range - low_range + 1e-10)\n        \n        # Combine momentum and level\n        mlmi = momentum * 0.6 + level * 0.4\n        \n        return mlmi\n    \n    df['mlmi'] = calculate_mlmi(df)\n    \n    # NWRQK (Normalized Weighted Range Quality K) - Custom implementation\n    def calculate_nwrqk(data, k_period=10):\n        \"\"\"Calculate custom NWRQK indicator.\"\"\"\n        # Range quality\n        true_range = pd.DataFrame({\n            'hl': data['High'] - data['Low'],\n            'hc': abs(data['High'] - data['Close'].shift(1)),\n            'lc': abs(data['Low'] - data['Close'].shift(1))\n        }).max(axis=1)\n        \n        # Weighted by volume\n        volume_weight = data['Volume'] / data['Volume'].rolling(window=k_period).mean()\n        weighted_range = true_range * volume_weight\n        \n        # Normalize\n        nwrqk = weighted_range / weighted_range.rolling(window=k_period).mean()\n        \n        return nwrqk\n    \n    df['nwrqk'] = calculate_nwrqk(df)\n    \n    # Interaction features\n    df['mlmi_minus_nwrqk'] = df['mlmi'] - df['nwrqk']\n    df['mlmi_times_nwrqk'] = df['mlmi'] * df['nwrqk']\n    df['mlmi_nwrqk_ratio'] = df['mlmi'] / (df['nwrqk'] + 1e-10)\n    \n    # Additional interaction features\n    df['rsi_bb_interaction'] = df['rsi'] * df['bb_position']\n    df['macd_adx_interaction'] = df['macd_diff'] * df['adx']\n    df['volume_atr_interaction'] = df['volume_ratio'] * df['atr']\n    \n    return df\n\n# Apply technical indicators\nprint(\"ðŸ”§ Adding technical indicators...\")\ncombined_data = add_technical_indicators(combined_data, DATA_CONFIG['lookback_periods'])\n\nprint(\"âœ… Technical indicators added\")\nprint(f\"   Total features: {len(combined_data.columns)}\")\nprint(f\"   Key indicators: MLMI, NWRQK, RSI, MACD, ADX, ATR, BB\")\nprint(f\"   Interaction features: mlmi_minus_nwrqk, mlmi_times_nwrqk, etc.\")"
  },
  {
   "cell_type": "code",
   "source": "# Final Data Preparation and Output\ndef prepare_final_datasets(combined_data, mmd_df, config):\n    \"\"\"Prepare final datasets for main MARL and RDE training.\"\"\"\n    \n    print(\"ðŸ“¦ Preparing final datasets...\")\n    \n    # Align all data by timestamp\n    min_timestamp = max(combined_data.index[0], mmd_df.index[0])\n    max_timestamp = min(combined_data.index[-1], mmd_df.index[-1])\n    \n    # Filter to common timerange\n    combined_data_aligned = combined_data.loc[min_timestamp:max_timestamp]\n    mmd_df_aligned = mmd_df.loc[min_timestamp:max_timestamp]\n    \n    # Drop any remaining NaN values\n    combined_data_aligned = combined_data_aligned.fillna(method='ffill').fillna(0)\n    \n    # Prepare main MARL training data\n    main_features = [\n        # Price features\n        'Open', 'High', 'Low', 'Close', 'Volume',\n        'HA_Open', 'HA_High', 'HA_Low', 'HA_Close', 'HA_Volume',\n        'HA_Body', 'HA_UpperShadow', 'HA_LowerShadow', 'HA_Direction',\n        \n        # Returns and ratios\n        'returns', 'log_returns', 'ha_returns', 'high_low_ratio', 'close_open_ratio',\n        'ha_body_ratio', 'ha_shadow_imbalance',\n        \n        # Volume features\n        'volume_ratio', 'dollar_volume', 'ha_volume_trend',\n        \n        # Technical indicators\n        'rsi', 'ha_rsi', 'macd', 'macd_signal', 'macd_diff', 'stoch',\n        'atr', 'ha_atr', 'bb_width', 'bb_position',\n        'adx', 'cci', 'aroon_up', 'aroon_down',\n        \n        # Custom indicators\n        'mlmi', 'nwrqk',\n        \n        # Interaction features\n        'mlmi_minus_nwrqk', 'mlmi_times_nwrqk', 'mlmi_nwrqk_ratio',\n        'rsi_bb_interaction', 'macd_adx_interaction', 'volume_atr_interaction',\n        \n        # LVN features\n        'strongest_lvn_price', 'strongest_lvn_strength', 'n_lvns',\n        \n        # Moving averages and ratios\n        'sma_20', 'ema_20', 'close_sma_20_ratio',\n        'sma_50', 'ema_50', 'close_sma_50_ratio',\n        \n        # Support/Resistance\n        'resistance_20', 'support_20', 'sr_ratio_20',\n        'resistance_50', 'support_50', 'sr_ratio_50'\n    ]\n    \n    # Select available features\n    available_features = [f for f in main_features if f in combined_data_aligned.columns]\n    main_df = combined_data_aligned[available_features].copy()\n    \n    # Add dominant regime from MMD analysis\n    main_df['dominant_regime'] = mmd_df_aligned['dominant_regime']\n    \n    print(f\"âœ… Main MARL dataset prepared: {main_df.shape}\")\n    \n    # Prepare RDE training data (MMD features)\n    rde_data_list = []\n    \n    for idx, row in mmd_df_aligned.iterrows():\n        rde_data_list.append({\n            'timestamp': idx,\n            **{f'mmd_feature_{i}': val for i, val in enumerate(row['mmd_features'])}\n        })\n    \n    rde_df = pd.DataFrame(rde_data_list).set_index('timestamp')\n    \n    print(f\"âœ… RDE dataset prepared: {rde_df.shape}\")\n    \n    return main_df, rde_df\n\n\n# Prepare final datasets\nmain_df, rde_df = prepare_final_datasets(combined_data, mmd_df, DATA_CONFIG)\n\n# Save to parquet files\noutput_dir = f\"{DRIVE_BASE}/data/processed\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save main MARL training data\nmain_output_path = f\"{output_dir}/training_data_main.parquet\"\nmain_df.to_parquet(main_output_path, engine='pyarrow', compression='snappy')\nprint(f\"âœ… Main training data saved to: {main_output_path}\")\nprint(f\"   Size: {os.path.getsize(main_output_path) / 1e6:.2f} MB\")\n\n# Save RDE training data\nrde_output_path = f\"{output_dir}/training_data_rde.parquet\"\nrde_df.to_parquet(rde_output_path, engine='pyarrow', compression='snappy')\nprint(f\"âœ… RDE training data saved to: {rde_output_path}\")\nprint(f\"   Size: {os.path.getsize(rde_output_path) / 1e6:.2f} MB\")\n\n# Save metadata\nmetadata = {\n    'created_date': datetime.now().isoformat(),\n    'data_config': DATA_CONFIG,\n    'data_symbol': used_symbol,\n    'date_range': {\n        'start': str(main_df.index[0]),\n        'end': str(main_df.index[-1])\n    },\n    'main_features': list(main_df.columns),\n    'rde_features': list(rde_df.columns),\n    'n_samples': len(main_df),\n    'reference_signatures': {str(k): v.tolist() for k, v in reference_signatures.items()}\n}\n\nmetadata_path = f\"{output_dir}/data_preparation_metadata.json\"\nwith open(metadata_path, 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(f\"âœ… Metadata saved to: {metadata_path}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create comprehensive data preparation summary\nsummary = f\"\"\"\n# Advanced Data Preparation Summary\n\n## Dataset Information\n- **Created**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n- **Symbol**: {used_symbol}\n- **Date Range**: {main_df.index[0]} to {main_df.index[-1]}\n- **Total Samples**: {len(main_df):,}\n- **Timeframe**: 30-minute bars with Heiken Ashi\n\n## Feature Engineering Completed\n### 1. Heiken Ashi Transformation\n- HA OHLC values calculated\n- HA body, shadows, and direction features\n\n### 2. LVN Analysis\n- Identified Low Volume Nodes from volume profile\n- Calculated strength scores (0-100) based on:\n  - Number of price tests\n  - Bounce magnitudes\n  - Recency of tests\n  - Volume characteristics\n- Features: strongest_lvn_price, strongest_lvn_strength, n_lvns\n\n### 3. MMD Feature Vectors (for RDE)\n- Identified {DATA_CONFIG['n_market_regimes']} archetypal market regimes\n- Calculated path signatures for each window\n- MMD scores against reference regime signatures\n- Additional statistical features (volatility, skew, kurtosis, etc.)\n- Total MMD features: {len(rde_df.columns)}\n\n### 4. Technical Indicators\n- Standard: RSI, MACD, ADX, ATR, Bollinger Bands, etc.\n- Custom: MLMI (Market Level Momentum Indicator)\n- Custom: NWRQK (Normalized Weighted Range Quality K)\n- Interaction features: mlmi_minus_nwrqk, mlmi_times_nwrqk, etc.\n\n## Output Files\n### 1. training_data_main.parquet\n- Path: `{main_output_path}`\n- Shape: {main_df.shape}\n- Features: {len(main_df.columns)} columns\n- Size: {os.path.getsize(main_output_path) / 1e6:.2f} MB\n- Use: Main MARL training and simulation\n\n### 2. training_data_rde.parquet\n- Path: `{rde_output_path}`\n- Shape: {rde_df.shape}\n- Features: {len(rde_df.columns)} MMD features\n- Size: {os.path.getsize(rde_output_path) / 1e6:.2f} MB\n- Use: Regime Detection Engine training\n\n## Usage in Training Notebooks\n```python\n# Load main MARL data\nimport pandas as pd\nmain_data = pd.read_parquet('{main_output_path}')\n\n# Load RDE data\nrde_data = pd.read_parquet('{rde_output_path}')\n```\n\n## Key Features for Each Model\n### Main MARL Core\n- All technical indicators and HA features\n- LVN strength scores\n- Interaction features (mlmi_minus_nwrqk, etc.)\n- Dominant regime labels\n\n### Regime Detection Engine (RDE)\n- MMD scores for {DATA_CONFIG['n_market_regimes']} regimes\n- Statistical features (volatility, skew, kurtosis)\n- Path signature components\n\n### Risk Management Sub-system\nWill use subset of main features focused on:\n- ATR and volatility measures\n- LVN levels for stop placement\n- Support/resistance ratios\n\"\"\"\n\nprint(summary)\n\n# Save summary\nsummary_file = f\"{output_dir}/data_preparation_summary_{datetime.now().strftime('%Y%m%d')}.md\"\nwith open(summary_file, 'w') as f:\n    f.write(summary)\nprint(f\"\\nâœ… Summary saved to: {summary_file}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"\\nðŸŽ‰ Advanced data preparation complete!\")\nprint(\"\\nðŸ“‹ Next steps:\")\nprint(\"1. Use training_data_rde.parquet in Regime_Agent_Training.ipynb\")\nprint(\"2. Use training_data_main.parquet in MARL_Training_Master_Colab.ipynb\")\nprint(\"3. Both files contain aligned timestamps for synchronized training\")\nprint(\"\\nData files ready for the hierarchical MARL system training!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalize_data"
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalize_data(data, method='robust', clip_threshold=5):\n",
    "    \"\"\"Normalize data using specified method.\"\"\"\n",
    "    \n",
    "    # Reshape data for normalization\n",
    "    original_shape = data.shape\n",
    "    if len(data.shape) == 3:\n",
    "        # Reshape from (samples, assets, features) to (samples*assets, features)\n",
    "        n_samples, n_assets, n_features = data.shape\n",
    "        data_reshaped = data.reshape(-1, n_features)\n",
    "    else:\n",
    "        data_reshaped = data\n",
    "    \n",
    "    # Replace inf and -inf with NaN\n",
    "    data_reshaped = np.where(np.isinf(data_reshaped), np.nan, data_reshaped)\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    data_reshaped = np.nan_to_num(data_reshaped, 0)\n",
    "    \n",
    "    # Normalize\n",
    "    if method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    data_normalized = scaler.fit_transform(data_reshaped)\n",
    "    \n",
    "    # Clip outliers\n",
    "    if clip_threshold:\n",
    "        data_normalized = np.clip(data_normalized, -clip_threshold, clip_threshold)\n",
    "    \n",
    "    # Reshape back\n",
    "    if len(original_shape) == 3:\n",
    "        data_normalized = data_normalized.reshape(original_shape)\n",
    "    \n",
    "    return data_normalized, scaler\n",
    "\n",
    "# Normalize all data\n",
    "print(\"ðŸ”§ Normalizing data...\")\n",
    "\n",
    "normalized_features = {}\n",
    "scalers = {}\n",
    "\n",
    "# Normalize feature matrices\n",
    "for agent, matrix in feature_matrices.items():\n",
    "    normalized_features[agent], scalers[agent] = normalize_data(\n",
    "        matrix,\n",
    "        method=DATA_CONFIG['normalization_method'],\n",
    "        clip_threshold=DATA_CONFIG['outlier_threshold']\n",
    "    )\n",
    "    print(f\"   {agent}: normalized\")\n",
    "\n",
    "# Normalize correlation matrices (already in [-1, 1] range)\n",
    "normalized_features['correlation_matrices'] = corr_matrices\n",
    "\n",
    "# Normalize volume profiles (already normalized to sum to 1)\n",
    "normalized_features['volume_profiles'] = volume_matrices\n",
    "\n",
    "print(\"âœ… Data normalization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_message"
   },
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ‰ Data preparation complete!\")\n",
    "print(\"\\nðŸ“‹ Next steps:\")\n",
    "print(\"1. The processed data is now saved in your Google Drive\")\n",
    "print(\"2. Use the Master Training Notebook to load this data\")\n",
    "print(\"3. The data is optimized for efficient loading in Colab\")\n",
    "print(\"\\nData file path for training:\")\n",
    "print(f\"'{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}