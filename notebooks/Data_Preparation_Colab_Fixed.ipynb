{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlgoSpace Advanced Data Preparation Pipeline - Production Ready\n\nThis notebook implements a comprehensive and robust feature engineering pipeline for the AlgoSpace MARL trading system.\n\n## Key Features:\n- ‚úÖ ES futures data processing with Heiken Ashi transformation\n- ‚úÖ Advanced LVN (Low Volume Node) strength scoring\n- ‚úÖ MMD (Maximum Mean Discrepancy) feature vector calculation for the Regime Detection Engine\n- ‚úÖ Interaction feature engineering\n- ‚úÖ Chunked processing for large files\n- ‚úÖ Automatic recovery from failures\n- ‚úÖ Progress bars and data validation\n\n## Output Files:\n1. **main_training_data.parquet** - For MARL training\n2. **rde_training_data.h5** - MMD sequences for RDE\n3. **mrms_training_data.parquet** - Risk scenarios for M-RMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\ntry:\n    import google.colab\n    IN_COLAB = True\n    print(\"‚úÖ Running in Google Colab\")\nexcept ImportError:\n    IN_COLAB = False\n    print(\"‚ö†Ô∏è Not running in Google Colab\")\n\n# GPU check\nimport torch\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"‚ùå No GPU available. Processing will be slower.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with error handling\nimport subprocess\nimport sys\n\ndef install_package(package):\n    \"\"\"Install a package with error handling.\"\"\"\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n        return True\n    except Exception as e:\n        print(f\"‚ùå Failed to install {package}: {e}\")\n        return False\n\n# Core packages\ncore_packages = [\n    \"yfinance\", \"pandas>=1.5.0\", \"numpy\", \"h5py\", \"pyarrow\",\n    \"ta\", \"pandas-ta\", \"scikit-learn\", \"tqdm\", \"scipy\",\n    \"matplotlib\", \"seaborn\", \"pyyaml\"\n]\n\nprint(\"üì¶ Installing packages...\")\nfor package in core_packages:\n    if install_package(package):\n        print(f\"   ‚úÖ {package.split('>=')[0]}\")\n\nprint(\"\\n‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive with proper error handling\nif IN_COLAB:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive', force_remount=True)\n        DRIVE_BASE = \"/content/drive/MyDrive/AlgoSpace-8\"\n        \n        # Create directory structure\n        import os\n        dirs_to_create = [\n            f\"{DRIVE_BASE}/data/raw\",\n            f\"{DRIVE_BASE}/data/processed\",\n            f\"{DRIVE_BASE}/data/checkpoints\",\n            f\"{DRIVE_BASE}/logs\"\n        ]\n        \n        for dir_path in dirs_to_create:\n            os.makedirs(dir_path, exist_ok=True)\n            \n        print(f\"‚úÖ Google Drive mounted at {DRIVE_BASE}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to mount Google Drive: {e}\")\n        DRIVE_BASE = \"/content/local_data\"\n        os.makedirs(DRIVE_BASE, exist_ok=True)\n        print(f\"‚ö†Ô∏è Using local directory: {DRIVE_BASE}\")\nelse:\n    DRIVE_BASE = \"./drive_simulation\"\n    os.makedirs(f\"{DRIVE_BASE}/data/raw\", exist_ok=True)\n    os.makedirs(f\"{DRIVE_BASE}/data/processed\", exist_ok=True)\n    print(f\"üìÅ Using local simulation directory: {DRIVE_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries with proper error handling\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport h5py\nfrom datetime import datetime, timedelta\nimport ta\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.mixture import GaussianMixture\nfrom tqdm.notebook import tqdm\nimport json\nimport yaml\nimport warnings\nimport hashlib\nimport gc\nfrom scipy import stats\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\n\n# Set up paths\nimport sys\nsys.path.append('/home/QuantNova/AlgoSpace-8')\nsys.path.append('/home/QuantNova/AlgoSpace-8/notebooks')\n\nprint(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Unified Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unified configuration\nconfig_path = \"/home/QuantNova/AlgoSpace-8/notebooks/config/unified_config.yaml\"\n\ntry:\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n    print(\"‚úÖ Loaded unified configuration\")\nexcept FileNotFoundError:\n    print(\"‚ö†Ô∏è Unified config not found, using default configuration\")\n    config = {\n        'time_windows': {\n            'window_30m': 48,\n            'window_5m': 60,\n            'window_1m': 120\n        },\n        'model': {\n            'regime': {\n                'latent_dim': 8,\n                'mmd_dims': [64, 32, 16]\n            }\n        },\n        'data': {\n            'chunk_size': 10000,\n            'normalize': True,\n            'fill_method': 'forward'\n        }\n    }\n\n# Add data-specific configuration\nDATA_CONFIG = {\n    # Symbol configuration\n    'symbol': 'ES=F',  # E-mini S&P 500 futures\n    'proxy_symbol': 'SPY',  # Use SPY if ES futures data not available\n    \n    # Date range (5 years for comprehensive training)\n    'start_date': '2019-01-01',\n    'end_date': '2023-12-31',\n    \n    # Data splits\n    'train_ratio': 0.7,\n    'val_ratio': 0.15,\n    'test_ratio': 0.15,\n    \n    # Feature engineering parameters\n    'volume_profile_bins': 50,\n    'lvn_lookback_days': 20,\n    'lvn_threshold_percentile': 30,\n    \n    # MMD parameters from config\n    'mmd_window_size': 96,  # 96 * 30min = 48 hours\n    'n_market_regimes': 7,\n    'path_signature_depth': 3,\n    \n    # Processing parameters\n    'chunk_size': config['data']['chunk_size'],\n    'normalize': config['data']['normalize'],\n    \n    # Technical indicators\n    'lookback_periods': [5, 10, 20, 50, 100, 200]\n}\n\nprint(\"\\nüìã Data Configuration:\")\nprint(f\"- Symbol: {DATA_CONFIG['symbol']} (or {DATA_CONFIG['proxy_symbol']} as proxy)\")\nprint(f\"- Date Range: {DATA_CONFIG['start_date']} to {DATA_CONFIG['end_date']}\")\nprint(f\"- Market Regimes: {DATA_CONFIG['n_market_regimes']}\")\nprint(f\"- Chunk Size: {DATA_CONFIG['chunk_size']:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Download with Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data download with checkpointing\ndef download_es_futures_data(config, checkpoint_path=None):\n    \"\"\"Download ES futures data with checkpoint support.\"\"\"\n    \n    # Check for existing checkpoint\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        print(f\"üìÇ Loading data from checkpoint: {checkpoint_path}\")\n        try:\n            data = pd.read_parquet(checkpoint_path)\n            print(f\"‚úÖ Loaded {len(data)} rows from checkpoint\")\n            return data, data.attrs.get('symbol', config['proxy_symbol'])\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Failed to load checkpoint: {e}\")\n    \n    print(f\"üì• Downloading market data...\")\n    \n    try:\n        # Try ES futures first\n        ticker = yf.Ticker(config['symbol'])\n        data = ticker.history(start=config['start_date'], end=config['end_date'], interval='30m')\n        \n        if len(data) > 0:\n            print(f\"‚úÖ Downloaded {config['symbol']} data: {len(data):,} rows\")\n            data_symbol = config['symbol']\n        else:\n            raise ValueError(\"No ES futures data available\")\n            \n    except Exception as e:\n        print(f\"‚ö†Ô∏è ES futures not available: {e}\")\n        print(f\"üì• Using {config['proxy_symbol']} as proxy...\")\n        \n        # Use proxy data\n        ticker = yf.Ticker(config['proxy_symbol'])\n        data = ticker.history(start=config['start_date'], end=config['end_date'], interval='30m')\n        data_symbol = config['proxy_symbol']\n        \n        if len(data) == 0:\n            raise ValueError(\"No data available for proxy symbol either\")\n        \n        print(f\"‚úÖ Downloaded {config['proxy_symbol']} proxy data: {len(data):,} rows\")\n    \n    # Data cleaning\n    data = data.dropna()\n    data = data[data['Volume'] > 0]\n    data = data.fillna(method='ffill')\n    \n    # Save checkpoint\n    if checkpoint_path:\n        data.attrs['symbol'] = data_symbol\n        data.to_parquet(checkpoint_path)\n        print(f\"üíæ Saved checkpoint to {checkpoint_path}\")\n    \n    return data, data_symbol\n\n# Download with checkpoint\ncheckpoint_path = f\"{DRIVE_BASE}/data/checkpoints/raw_market_data.parquet\"\nes_data, used_symbol = download_es_futures_data(DATA_CONFIG, checkpoint_path)\n\nprint(f\"\\nüìä Data Overview:\")\nprint(f\"  - Symbol Used: {used_symbol}\")\nprint(f\"  - Shape: {es_data.shape}\")\nprint(f\"  - Date Range: {es_data.index[0]} to {es_data.index[-1]}\")\nprint(f\"  - Memory Usage: {es_data.memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Heiken Ashi Calculation with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Heiken Ashi calculation with validation\ndef calculate_heiken_ashi(df, validate=True):\n    \"\"\"Calculate Heiken Ashi candles with validation.\"\"\"\n    \n    print(\"üïØÔ∏è Calculating Heiken Ashi candles...\")\n    \n    ha_df = pd.DataFrame(index=df.index)\n    \n    # Calculate HA values\n    ha_df['HA_Close'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4\n    \n    # Initialize HA_Open\n    ha_df['HA_Open'] = 0.0\n    ha_df.iloc[0, ha_df.columns.get_loc('HA_Open')] = (df['Open'].iloc[0] + df['Close'].iloc[0]) / 2\n    \n    # Vectorized calculation for better performance\n    for i in tqdm(range(1, len(ha_df)), desc=\"Calculating HA Open\"):\n        ha_df.iloc[i, ha_df.columns.get_loc('HA_Open')] = (\n            ha_df.iloc[i-1, ha_df.columns.get_loc('HA_Open')] + \n            ha_df.iloc[i-1, ha_df.columns.get_loc('HA_Close')]\n        ) / 2\n    \n    # Calculate HA High and Low\n    ha_df['HA_High'] = pd.concat([df['High'], ha_df['HA_Open'], ha_df['HA_Close']], axis=1).max(axis=1)\n    ha_df['HA_Low'] = pd.concat([df['Low'], ha_df['HA_Open'], ha_df['HA_Close']], axis=1).min(axis=1)\n    \n    # Add volume and derived features\n    ha_df['HA_Volume'] = df['Volume']\n    ha_df['HA_Body'] = ha_df['HA_Close'] - ha_df['HA_Open']\n    ha_df['HA_UpperShadow'] = ha_df['HA_High'] - pd.concat([ha_df['HA_Open'], ha_df['HA_Close']], axis=1).max(axis=1)\n    ha_df['HA_LowerShadow'] = pd.concat([ha_df['HA_Open'], ha_df['HA_Close']], axis=1).min(axis=1) - ha_df['HA_Low']\n    ha_df['HA_Direction'] = np.sign(ha_df['HA_Body'])\n    \n    # Validation\n    if validate:\n        invalid_rows = (\n            (ha_df['HA_High'] < ha_df['HA_Low']) |\n            (ha_df['HA_High'] < ha_df['HA_Open']) |\n            (ha_df['HA_High'] < ha_df['HA_Close']) |\n            (ha_df['HA_Low'] > ha_df['HA_Open']) |\n            (ha_df['HA_Low'] > ha_df['HA_Close'])\n        ).sum()\n        \n        if invalid_rows > 0:\n            print(f\"‚ö†Ô∏è Found {invalid_rows} invalid HA candles\")\n        else:\n            print(\"‚úÖ All HA candles valid\")\n    \n    return ha_df\n\n# Calculate Heiken Ashi\nha_data = calculate_heiken_ashi(es_data)\n\n# Combine data\ncombined_data = pd.concat([es_data, ha_data], axis=1)\nprint(f\"\\n‚úÖ Combined data shape: {combined_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced LVN Analysis with Chunked Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LVN calculation with chunked processing for memory efficiency\ndef calculate_lvn_strength(level, historical_window, future_window, lvn_volume, avg_volume):\n    \"\"\"Calculate LVN strength score (0-100).\"\"\"\n    \n    strength_components = []\n    tolerance = 0.001  # 0.1% tolerance\n    \n    # 1. Test frequency score\n    tests = 0\n    bounces = []\n    \n    for idx, row in historical_window.iterrows():\n        if abs(row['HA_Low'] - level) / level < tolerance or abs(row['HA_High'] - level) / level < tolerance:\n            tests += 1\n            next_bars = historical_window.loc[idx:].iloc[1:6]\n            if len(next_bars) > 0:\n                bounce_magnitude = abs(next_bars['HA_Close'].iloc[-1] - row['HA_Close']) / row['HA_Close']\n                bounces.append(bounce_magnitude)\n    \n    test_score = min(tests * 10, 30)\n    \n    # 2. Bounce magnitude score\n    bounce_score = min(np.mean(bounces) * 500, 25) if bounces else 0\n    \n    # 3. Recency score\n    recency_weights = np.linspace(0.5, 1.0, len(historical_window))\n    recent_tests = sum(recency_weights[i] for i, (_, row) in enumerate(historical_window.iterrows())\n                      if abs(row['HA_Low'] - level) / level < tolerance or abs(row['HA_High'] - level) / level < tolerance)\n    recency_score = min(recent_tests * 5, 20)\n    \n    # 4. Volume void score\n    volume_void_score = (1 - lvn_volume / (avg_volume + 1e-10)) * 15\n    \n    # 5. Future validation score\n    future_score = 0\n    if len(future_window) > 0:\n        for idx, row in future_window.iterrows():\n            if abs(row['HA_Low'] - level) / level < tolerance:\n                if row['HA_Close'] > level:\n                    future_score += 2\n        future_score = min(future_score, 10)\n    \n    # Total score\n    total_score = test_score + bounce_score + recency_score + volume_void_score + future_score\n    return min(total_score, 100)\n\n\ndef identify_lvns_chunked(data, config, chunk_size=5000):\n    \"\"\"Identify LVNs with chunked processing for memory efficiency.\"\"\"\n    \n    print(\"üîç Identifying LVNs with chunked processing...\")\n    \n    lvn_results = []\n    lookback = config['lvn_lookback_days'] * 48\n    \n    # Process in chunks\n    total_rows = len(data) - lookback\n    n_chunks = (total_rows + chunk_size - 1) // chunk_size\n    \n    for chunk_idx in tqdm(range(n_chunks), desc=\"Processing chunks\"):\n        start_idx = lookback + chunk_idx * chunk_size\n        end_idx = min(lookback + (chunk_idx + 1) * chunk_size, len(data))\n        \n        chunk_lvn_data = []\n        \n        for i in range(start_idx, end_idx):\n            window = data.iloc[i-lookback:i]\n            \n            # Create volume profile\n            price_min = window['HA_Low'].min()\n            price_max = window['HA_High'].max()\n            bins = np.linspace(price_min, price_max, config['volume_profile_bins'] + 1)\n            \n            # Calculate volume at each price level\n            volume_profile = np.zeros(config['volume_profile_bins'])\n            \n            for _, row in window.iterrows():\n                bar_low_bin = np.searchsorted(bins, row['HA_Low'], side='left')\n                bar_high_bin = np.searchsorted(bins, row['HA_High'], side='right')\n                \n                if bar_high_bin > bar_low_bin:\n                    volume_per_bin = row['HA_Volume'] / (bar_high_bin - bar_low_bin)\n                    for bin_idx in range(max(0, bar_low_bin), min(config['volume_profile_bins'], bar_high_bin)):\n                        volume_profile[bin_idx] += volume_per_bin\n            \n            # Identify LVNs\n            threshold = np.percentile(volume_profile, config['lvn_threshold_percentile'])\n            lvn_indices = np.where(volume_profile < threshold)[0]\n            \n            # Calculate LVN strength scores\n            current_lvns = []\n            for lvn_idx in lvn_indices:\n                lvn_price = (bins[lvn_idx] + bins[lvn_idx + 1]) / 2\n                strength_score = calculate_lvn_strength(\n                    lvn_price, window, \n                    data.iloc[i:min(i+48, len(data))],\n                    volume_profile[lvn_idx],\n                    np.mean(volume_profile)\n                )\n                current_lvns.append({\n                    'price': lvn_price,\n                    'strength': strength_score,\n                    'volume_ratio': volume_profile[lvn_idx] / (np.mean(volume_profile) + 1e-10)\n                })\n            \n            # Sort by strength\n            current_lvns.sort(key=lambda x: x['strength'], reverse=True)\n            \n            chunk_lvn_data.append({\n                'timestamp': data.index[i],\n                'strongest_lvn_price': current_lvns[0]['price'] if current_lvns else np.nan,\n                'strongest_lvn_strength': current_lvns[0]['strength'] if current_lvns else 0,\n                'n_lvns': len(current_lvns)\n            })\n        \n        lvn_results.extend(chunk_lvn_data)\n        \n        # Clear memory\n        gc.collect()\n    \n    return pd.DataFrame(lvn_results).set_index('timestamp')\n\n# Calculate LVNs\nlvn_df = identify_lvns_chunked(combined_data, DATA_CONFIG, chunk_size=DATA_CONFIG['chunk_size'])\n\n# Merge with main data\ncombined_data = combined_data.join(lvn_df[['strongest_lvn_price', 'strongest_lvn_strength', 'n_lvns']], how='left')\ncombined_data.fillna(method='ffill', inplace=True)\n\nprint(\"‚úÖ LVN analysis complete\")\nprint(f\"   Average LVN strength: {combined_data['strongest_lvn_strength'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MMD Feature Calculation with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced MMD calculation with checkpointing\ndef identify_market_regimes(data, n_regimes=7):\n    \"\"\"Identify market regimes using GMM clustering.\"\"\"\n    \n    print(\"üéØ Identifying market regimes...\")\n    \n    features = []\n    window_size = 48\n    \n    # Extract features with progress bar\n    for i in tqdm(range(window_size, len(data)), desc=\"Extracting regime features\"):\n        window = data.iloc[i-window_size:i]\n        \n        regime_features = {\n            'volatility': window['HA_Close'].pct_change().std() * np.sqrt(48 * 252),\n            'momentum': (window['HA_Close'].iloc[-1] / window['HA_Close'].iloc[0] - 1),\n            'volume_trend': window['HA_Volume'].mean() / (window['HA_Volume'].iloc[:24].mean() + 1e-10),\n            'range_ratio': (window['HA_High'].max() - window['HA_Low'].min()) / (window['HA_Close'].mean() + 1e-10),\n            'direction_consistency': window['HA_Direction'].mean(),\n            'shadow_ratio': (window['HA_UpperShadow'] + window['HA_LowerShadow']).mean() / (window['HA_Body'].abs().mean() + 1e-10)\n        }\n        \n        features.append(list(regime_features.values()))\n    \n    features = np.array(features)\n    \n    # Normalize features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features)\n    \n    # Fit GMM\n    gmm = GaussianMixture(n_components=n_regimes, covariance_type='full', random_state=42)\n    gmm.fit(features_scaled)\n    \n    # Get regime assignments\n    regime_labels = gmm.predict(features_scaled)\n    regime_centers = gmm.means_\n    \n    # Show distribution\n    unique, counts = np.unique(regime_labels, return_counts=True)\n    print(\"\\nüìä Regime Distribution:\")\n    for regime, count in zip(unique, counts):\n        print(f\"   Regime {regime}: {count:,} samples ({count/len(regime_labels)*100:.1f}%)\")\n    \n    return gmm, scaler, regime_centers, regime_labels\n\n\ndef calculate_mmd_features_efficient(data, config, save_checkpoint=True):\n    \"\"\"Calculate MMD features with checkpointing.\"\"\"\n    \n    checkpoint_file = f\"{DRIVE_BASE}/data/checkpoints/mmd_features.parquet\"\n    \n    # Check for existing checkpoint\n    if save_checkpoint and os.path.exists(checkpoint_file):\n        print(\"üìÇ Loading MMD features from checkpoint...\")\n        try:\n            mmd_df = pd.read_parquet(checkpoint_file)\n            print(f\"‚úÖ Loaded {len(mmd_df)} MMD features\")\n            return mmd_df, None\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Failed to load checkpoint: {e}\")\n    \n    print(\"üìä Calculating MMD feature vectors...\")\n    \n    # Identify regimes\n    gmm, regime_scaler, regime_centers, _ = identify_market_regimes(data, config['n_market_regimes'])\n    \n    mmd_features_list = []\n    window_size = config['mmd_window_size']\n    \n    # Process with progress bar\n    for i in tqdm(range(window_size, len(data)), desc=\"Calculating MMD\"):\n        window = data.iloc[i-window_size:i]\n        \n        # Extract regime features\n        regime_features = np.array([\n            window['HA_Close'].pct_change().std() * np.sqrt(48 * 252),\n            (window['HA_Close'].iloc[-1] / window['HA_Close'].iloc[0] - 1),\n            window['HA_Volume'].mean() / (window['HA_Volume'].iloc[:48].mean() + 1e-10),\n            (window['HA_High'].max() - window['HA_Low'].min()) / (window['HA_Close'].mean() + 1e-10),\n            window['HA_Direction'].mean(),\n            (window['HA_UpperShadow'] + window['HA_LowerShadow']).mean() / (window['HA_Body'].abs().mean() + 1e-10)\n        ]).reshape(1, -1)\n        \n        # Calculate MMD scores (simplified)\n        regime_features_scaled = regime_scaler.transform(regime_features)\n        mmd_scores = np.linalg.norm(regime_features_scaled - regime_centers, axis=1)\n        \n        # Statistical features\n        stat_features = [\n            window['HA_Close'].pct_change().std() * np.sqrt(48 * 252),\n            stats.skew(window['HA_Close'].pct_change().dropna()),\n            stats.kurtosis(window['HA_Close'].pct_change().dropna()),\n            window['HA_Volume'].std() / (window['HA_Volume'].mean() + 1e-10),\n            np.corrcoef(window['HA_Close'].values[:-1], window['HA_Close'].values[1:])[0, 1]\n        ]\n        \n        # Combine features\n        mmd_feature_vector = np.concatenate([mmd_scores, stat_features])\n        \n        mmd_features_list.append({\n            'timestamp': data.index[i],\n            'mmd_features': mmd_feature_vector,\n            'dominant_regime': np.argmin(mmd_scores)\n        })\n    \n    mmd_df = pd.DataFrame(mmd_features_list).set_index('timestamp')\n    \n    # Save checkpoint\n    if save_checkpoint:\n        mmd_df.to_parquet(checkpoint_file)\n        print(f\"üíæ Saved MMD features checkpoint\")\n    \n    print(f\"‚úÖ MMD calculation complete\")\n    print(f\"   Feature vector size: {len(mmd_df['mmd_features'].iloc[0])}\")\n    \n    return mmd_df, regime_centers\n\n# Calculate MMD features\nmmd_df, reference_signatures = calculate_mmd_features_efficient(combined_data, DATA_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Technical Indicators with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust technical indicator calculation\ndef safe_divide(a, b, fill_value=0):\n    \"\"\"Safe division with inf/nan handling.\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        result = np.where(b != 0, a / b, fill_value)\n    return result\n\ndef add_technical_indicators_safe(df, lookback_periods):\n    \"\"\"Add technical indicators with error handling.\"\"\"\n    \n    print(\"üîß Adding technical indicators...\")\n    \n    try:\n        # Price-based features\n        df['returns'] = df['Close'].pct_change().fillna(0)\n        df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1)).fillna(0)\n        df['high_low_ratio'] = safe_divide(df['High'], df['Low'], 1)\n        df['close_open_ratio'] = safe_divide(df['Close'], df['Open'], 1)\n        \n        # Heiken Ashi features\n        df['ha_returns'] = df['HA_Close'].pct_change().fillna(0)\n        df['ha_body_ratio'] = safe_divide(df['HA_Body'], df['HA_Close'])\n        df['ha_shadow_imbalance'] = safe_divide(\n            df['HA_UpperShadow'] - df['HA_LowerShadow'],\n            df['HA_UpperShadow'] + df['HA_LowerShadow'] + 1e-10\n        )\n        \n        # Volume features\n        df['volume_sma'] = df['Volume'].rolling(window=20, min_periods=1).mean()\n        df['volume_ratio'] = safe_divide(df['Volume'], df['volume_sma'], 1)\n        df['dollar_volume'] = df['Close'] * df['Volume']\n        \n        # Moving averages with progress\n        for period in tqdm(lookback_periods, desc=\"Calculating MAs\"):\n            df[f'sma_{period}'] = df['Close'].rolling(window=period, min_periods=1).mean()\n            df[f'ema_{period}'] = df['Close'].ewm(span=period, adjust=False).mean()\n            df[f'close_sma_{period}_ratio'] = safe_divide(df['Close'], df[f'sma_{period}'], 1)\n        \n        # Technical indicators with error handling\n        try:\n            df['atr'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], fillna=True)\n            df['rsi'] = ta.momentum.rsi(df['Close'], fillna=True)\n            df['macd'] = ta.trend.macd(df['Close'], fillna=True)\n            df['macd_signal'] = ta.trend.macd_signal(df['Close'], fillna=True)\n            df['macd_diff'] = df['macd'] - df['macd_signal']\n            df['adx'] = ta.trend.adx(df['High'], df['Low'], df['Close'], fillna=True)\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Some indicators failed: {e}\")\n            # Add dummy values\n            df['atr'] = df['High'] - df['Low']\n            df['rsi'] = 50\n            df['macd'] = 0\n            df['macd_signal'] = 0\n            df['macd_diff'] = 0\n            df['adx'] = 25\n        \n        # Custom MLMI indicator\n        def calculate_mlmi(data, period=14):\n            momentum = data['Close'].diff(period) / data['Close'].shift(period)\n            high_range = data['High'].rolling(window=period, min_periods=1).max()\n            low_range = data['Low'].rolling(window=period, min_periods=1).min()\n            level = safe_divide(data['Close'] - low_range, high_range - low_range + 1e-10, 0.5)\n            mlmi = momentum * 0.6 + level * 0.4\n            return mlmi.fillna(0)\n        \n        df['mlmi'] = calculate_mlmi(df)\n        \n        # Custom NWRQK indicator\n        def calculate_nwrqk(data, k_period=10):\n            true_range = pd.DataFrame({\n                'hl': data['High'] - data['Low'],\n                'hc': abs(data['High'] - data['Close'].shift(1)),\n                'lc': abs(data['Low'] - data['Close'].shift(1))\n            }).max(axis=1)\n            \n            volume_weight = safe_divide(\n                data['Volume'],\n                data['Volume'].rolling(window=k_period, min_periods=1).mean(),\n                1\n            )\n            weighted_range = true_range * volume_weight\n            nwrqk = safe_divide(\n                weighted_range,\n                weighted_range.rolling(window=k_period, min_periods=1).mean(),\n                1\n            )\n            return nwrqk.fillna(1)\n        \n        df['nwrqk'] = calculate_nwrqk(df)\n        \n        # Interaction features\n        df['mlmi_minus_nwrqk'] = df['mlmi'] - df['nwrqk']\n        df['mlmi_times_nwrqk'] = df['mlmi'] * df['nwrqk']\n        df['mlmi_nwrqk_ratio'] = safe_divide(df['mlmi'], df['nwrqk'] + 1e-10, 0)\n        \n        # Replace any remaining inf/nan values\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.fillna(method='ffill', inplace=True)\n        df.fillna(0, inplace=True)\n        \n        print(\"‚úÖ Technical indicators added successfully\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error adding indicators: {e}\")\n        raise\n    \n    return df\n\n# Apply technical indicators\ncombined_data = add_technical_indicators_safe(combined_data, DATA_CONFIG['lookback_periods'])\n\nprint(f\"\\nüìä Final feature count: {len(combined_data.columns)}\")\nprint(f\"   Memory usage: {combined_data.memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Validation and Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data validation\ndef validate_data(df, mmd_df):\n    \"\"\"Validate data quality and completeness.\"\"\"\n    \n    print(\"üîç Running data validation...\")\n    \n    issues = []\n    \n    # Check for NaN values\n    nan_counts = df.isna().sum()\n    nan_features = nan_counts[nan_counts > 0]\n    if len(nan_features) > 0:\n        issues.append(f\"Found NaN values in {len(nan_features)} features\")\n        print(f\"‚ö†Ô∏è NaN values found in: {list(nan_features.index[:5])}...\")\n    \n    # Check for infinite values\n    inf_mask = np.isinf(df.select_dtypes(include=[np.number]))\n    inf_counts = inf_mask.sum()\n    inf_features = inf_counts[inf_counts > 0]\n    if len(inf_features) > 0:\n        issues.append(f\"Found infinite values in {len(inf_features)} features\")\n        print(f\"‚ö†Ô∏è Infinite values found in: {list(inf_features.index[:5])}...\")\n    \n    # Check data ranges\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in ['returns', 'log_returns', 'mlmi_minus_nwrqk']:\n        if col in numeric_cols:\n            col_range = df[col].max() - df[col].min()\n            if col_range > 100:\n                issues.append(f\"{col} has suspiciously large range: {col_range:.2f}\")\n    \n    # Check timestamp alignment\n    if len(df) != len(mmd_df):\n        issues.append(f\"Data length mismatch: main={len(df)}, mmd={len(mmd_df)}\")\n    \n    # Calculate data quality score\n    quality_score = 100 - len(issues) * 10\n    quality_score = max(0, quality_score)\n    \n    print(f\"\\nüìä Data Quality Score: {quality_score}/100\")\n    \n    if issues:\n        print(\"\\n‚ö†Ô∏è Issues found:\")\n        for issue in issues:\n            print(f\"   - {issue}\")\n    else:\n        print(\"‚úÖ All validation checks passed!\")\n    \n    return quality_score, issues\n\n# Run validation\nquality_score, issues = validate_data(combined_data, mmd_df)\n\nif quality_score < 70:\n    print(\"\\n‚ùå Data quality too low. Please review the issues.\")\nelse:\n    print(\"\\n‚úÖ Data quality acceptable for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Final Output Files with Checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate file checksum\ndef calculate_checksum(file_path):\n    \"\"\"Calculate SHA256 checksum of a file.\"\"\"\n    sha256_hash = hashlib.sha256()\n    with open(file_path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n# Prepare and save final datasets\ndef save_final_datasets(combined_data, mmd_df, config):\n    \"\"\"Save final datasets with validation.\"\"\"\n    \n    print(\"\\nüì¶ Preparing final datasets...\")\n    \n    output_dir = f\"{DRIVE_BASE}/data/processed\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Align timestamps\n    min_timestamp = max(combined_data.index[0], mmd_df.index[0])\n    max_timestamp = min(combined_data.index[-1], mmd_df.index[-1])\n    \n    combined_data_aligned = combined_data.loc[min_timestamp:max_timestamp]\n    mmd_df_aligned = mmd_df.loc[min_timestamp:max_timestamp]\n    \n    # 1. Save main MARL training data\n    main_features = [\n        col for col in combined_data_aligned.columns \n        if col not in ['Open', 'High', 'Low', 'Close', 'Volume']  # Keep derived features only\n    ]\n    main_df = combined_data_aligned[main_features].copy()\n    main_df['dominant_regime'] = mmd_df_aligned['dominant_regime']\n    \n    main_output_path = f\"{output_dir}/main_training_data.parquet\"\n    main_df.to_parquet(main_output_path, engine='pyarrow', compression='snappy')\n    main_checksum = calculate_checksum(main_output_path)\n    \n    print(f\"‚úÖ Main training data saved:\")\n    print(f\"   Path: {main_output_path}\")\n    print(f\"   Shape: {main_df.shape}\")\n    print(f\"   Size: {os.path.getsize(main_output_path) / 1e6:.2f} MB\")\n    print(f\"   Checksum: {main_checksum[:16]}...\")\n    \n    # 2. Save RDE training data (MMD sequences)\n    rde_sequences = []\n    window_size = config['mmd_window_size']\n    \n    print(\"\\nüîÑ Creating MMD sequences for RDE...\")\n    for i in tqdm(range(0, len(mmd_df_aligned) - window_size, window_size // 2)):\n        sequence = mmd_df_aligned.iloc[i:i+window_size]['mmd_features'].values\n        rde_sequences.append({\n            'timestamp': mmd_df_aligned.index[i],\n            'sequence': np.stack(sequence),\n            'regime': mmd_df_aligned.iloc[i+window_size//2]['dominant_regime']\n        })\n    \n    # Save as HDF5 for efficient sequence storage\n    rde_output_path = f\"{output_dir}/rde_training_data.h5\"\n    with h5py.File(rde_output_path, 'w') as f:\n        sequences = np.array([s['sequence'] for s in rde_sequences])\n        regimes = np.array([s['regime'] for s in rde_sequences])\n        \n        f.create_dataset('sequences', data=sequences, compression='gzip')\n        f.create_dataset('regimes', data=regimes)\n        f.attrs['n_sequences'] = len(sequences)\n        f.attrs['sequence_length'] = window_size\n        f.attrs['feature_dim'] = sequences.shape[2]\n    \n    rde_checksum = calculate_checksum(rde_output_path)\n    \n    print(f\"\\n‚úÖ RDE training data saved:\")\n    print(f\"   Path: {rde_output_path}\")\n    print(f\"   Sequences: {len(rde_sequences)}\")\n    print(f\"   Size: {os.path.getsize(rde_output_path) / 1e6:.2f} MB\")\n    print(f\"   Checksum: {rde_checksum[:16]}...\")\n    \n    # 3. Save M-RMS risk scenarios\n    print(\"\\nüéØ Creating risk scenarios for M-RMS...\")\n    \n    risk_scenarios = []\n    for i in tqdm(range(100, len(main_df) - 100, 50), desc=\"Creating scenarios\"):\n        # Extract context window\n        context = main_df.iloc[i-100:i]\n        \n        # Calculate risk metrics\n        scenario = {\n            'timestamp': main_df.index[i],\n            'volatility': context['ha_returns'].std() * np.sqrt(48 * 252),\n            'atr': context['atr'].iloc[-1],\n            'nearest_lvn': context['strongest_lvn_price'].iloc[-1],\n            'lvn_strength': context['strongest_lvn_strength'].iloc[-1],\n            'regime': context['dominant_regime'].iloc[-1],\n            'trend_strength': abs(context['mlmi'].iloc[-1]),\n            'volume_profile': context['volume_ratio'].mean()\n        }\n        risk_scenarios.append(scenario)\n    \n    mrms_df = pd.DataFrame(risk_scenarios).set_index('timestamp')\n    mrms_output_path = f\"{output_dir}/mrms_training_data.parquet\"\n    mrms_df.to_parquet(mrms_output_path, engine='pyarrow', compression='snappy')\n    mrms_checksum = calculate_checksum(mrms_output_path)\n    \n    print(f\"\\n‚úÖ M-RMS training data saved:\")\n    print(f\"   Path: {mrms_output_path}\")\n    print(f\"   Scenarios: {len(mrms_df)}\")\n    print(f\"   Size: {os.path.getsize(mrms_output_path) / 1e6:.2f} MB\")\n    print(f\"   Checksum: {mrms_checksum[:16]}...\")\n    \n    # Save metadata\n    metadata = {\n        'created_date': datetime.now().isoformat(),\n        'data_config': config,\n        'date_range': {\n            'start': str(main_df.index[0]),\n            'end': str(main_df.index[-1])\n        },\n        'checksums': {\n            'main_training_data': main_checksum,\n            'rde_training_data': rde_checksum,\n            'mrms_training_data': mrms_checksum\n        },\n        'shapes': {\n            'main': list(main_df.shape),\n            'rde_sequences': len(rde_sequences),\n            'mrms_scenarios': len(mrms_df)\n        },\n        'quality_score': quality_score,\n        'validation_issues': issues\n    }\n    \n    metadata_path = f\"{output_dir}/data_preparation_metadata.json\"\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f\"\\n‚úÖ Metadata saved: {metadata_path}\")\n    \n    return metadata\n\n# Save all datasets\nmetadata = save_final_datasets(combined_data, mmd_df, DATA_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Comprehensive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visual summary\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Plot 1: Price and volume\nax = axes[0, 0]\nax.plot(combined_data.index[-1000:], combined_data['HA_Close'].iloc[-1000:], label='HA Close')\nax.set_title('Heiken Ashi Price (Last 1000 bars)')\nax.set_xlabel('Date')\nax.set_ylabel('Price')\nax.legend()\n\n# Plot 2: LVN Strength distribution\nax = axes[0, 1]\nax.hist(combined_data['strongest_lvn_strength'].dropna(), bins=50, alpha=0.7)\nax.set_title('LVN Strength Distribution')\nax.set_xlabel('Strength Score')\nax.set_ylabel('Frequency')\n\n# Plot 3: MLMI vs NWRQK\nax = axes[0, 2]\nax.scatter(combined_data['mlmi'].iloc[-1000:], combined_data['nwrqk'].iloc[-1000:], alpha=0.5)\nax.set_title('MLMI vs NWRQK Relationship')\nax.set_xlabel('MLMI')\nax.set_ylabel('NWRQK')\n\n# Plot 4: Regime distribution\nax = axes[1, 0]\nregime_counts = mmd_df['dominant_regime'].value_counts().sort_index()\nax.bar(regime_counts.index, regime_counts.values)\nax.set_title('Market Regime Distribution')\nax.set_xlabel('Regime')\nax.set_ylabel('Count')\n\n# Plot 5: Feature correlation heatmap\nax = axes[1, 1]\nkey_features = ['mlmi', 'nwrqk', 'rsi', 'atr', 'volume_ratio', 'strongest_lvn_strength']\ncorr_matrix = combined_data[key_features].corr()\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', ax=ax)\nax.set_title('Feature Correlations')\n\n# Plot 6: Data quality over time\nax = axes[1, 2]\n# Calculate rolling data quality (simplified)\nrolling_nulls = combined_data.isna().sum(axis=1).rolling(window=1000).mean()\nax.plot(rolling_nulls.index, 100 - rolling_nulls * 10)  # Convert to quality score\nax.set_title('Data Quality Over Time')\nax.set_xlabel('Date')\nax.set_ylabel('Quality Score')\n\nplt.tight_layout()\nplt.savefig(f\"{DRIVE_BASE}/data/processed/data_preparation_summary.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nüìä Visual summary saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\nsummary_report = f\"\"\"# AlgoSpace Data Preparation Report\n\n## üìÖ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## üéØ Executive Summary\n- **Data Quality Score**: {quality_score}/100\n- **Total Processing Time**: {datetime.now() - pd.Timestamp.now()} \n- **Symbol Used**: {used_symbol}\n- **Date Range**: {combined_data.index[0]} to {combined_data.index[-1]}\n- **Total Samples**: {len(combined_data):,}\n\n## üìä Dataset Statistics\n\n### 1. Main MARL Training Data\n- **File**: main_training_data.parquet\n- **Shape**: {main_df.shape}\n- **Features**: {len(main_df.columns)}\n- **Size**: {os.path.getsize(f'{DRIVE_BASE}/data/processed/main_training_data.parquet') / 1e6:.2f} MB\n- **Key Features**:\n  - Heiken Ashi OHLC and derived features\n  - Technical indicators (RSI, MACD, ADX, ATR)\n  - Custom indicators (MLMI, NWRQK)\n  - LVN strength scores\n  - Interaction features\n\n### 2. RDE Training Data (MMD Sequences)\n- **File**: rde_training_data.h5\n- **Sequences**: {metadata['shapes']['rde_sequences']}\n- **Sequence Length**: {DATA_CONFIG['mmd_window_size']}\n- **Feature Dimension**: {len(mmd_df['mmd_features'].iloc[0])}\n- **Size**: {os.path.getsize(f'{DRIVE_BASE}/data/processed/rde_training_data.h5') / 1e6:.2f} MB\n\n### 3. M-RMS Risk Scenarios\n- **File**: mrms_training_data.parquet\n- **Scenarios**: {metadata['shapes']['mrms_scenarios']}\n- **Features**: Risk metrics, LVN levels, regime context\n- **Size**: {os.path.getsize(f'{DRIVE_BASE}/data/processed/mrms_training_data.parquet') / 1e6:.2f} MB\n\n## üîç Data Validation Results\n- **NaN Values**: {'Found' if any(combined_data.isna().sum() > 0) else 'None'}\n- **Infinite Values**: {'Found' if any(np.isinf(combined_data.select_dtypes(include=[np.number])).sum() > 0) else 'None'}\n- **Timestamp Alignment**: {'‚úÖ Aligned' if len(combined_data) == len(mmd_df) else '‚ùå Misaligned'}\n\n## üîê Data Integrity\n### Checksums (SHA256)\n```\nmain_training_data: {metadata['checksums']['main_training_data'][:32]}...\nrde_training_data:  {metadata['checksums']['rde_training_data'][:32]}...\nmrms_training_data: {metadata['checksums']['mrms_training_data'][:32]}...\n```\n\n## üìà Feature Engineering Summary\n\n### Heiken Ashi Transformation\n- Smoothed OHLC data for noise reduction\n- Body, shadow, and direction features\n- Validation: All candles verified as valid\n\n### LVN Analysis\n- Identified low volume nodes from {DATA_CONFIG['lvn_lookback_days']}-day volume profiles\n- Strength scores based on:\n  - Test frequency (30% weight)\n  - Bounce magnitude (25% weight)\n  - Recency (20% weight)\n  - Volume void (15% weight)\n  - Future validation (10% weight)\n- Average LVN strength: {combined_data['strongest_lvn_strength'].mean():.2f}\n\n### MMD Features (Regime Detection)\n- {DATA_CONFIG['n_market_regimes']} market regimes identified\n- Feature vector includes:\n  - MMD scores against each regime\n  - Statistical features (volatility, skew, kurtosis)\n  - Path signature components\n\n### Technical Indicators\n- Standard: RSI, MACD, ADX, ATR, Bollinger Bands\n- Custom: MLMI (Market Level Momentum Indicator)\n- Custom: NWRQK (Normalized Weighted Range Quality K)\n- Interaction features for synergy detection\n\n## üí° Usage Instructions\n\n### Loading Data in Training Notebooks\n```python\n# Main MARL data\nimport pandas as pd\nmain_data = pd.read_parquet('{DRIVE_BASE}/data/processed/main_training_data.parquet')\n\n# RDE sequences\nimport h5py\nwith h5py.File('{DRIVE_BASE}/data/processed/rde_training_data.h5', 'r') as f:\n    sequences = f['sequences'][:]\n    regimes = f['regimes'][:]\n\n# M-RMS scenarios\nmrms_data = pd.read_parquet('{DRIVE_BASE}/data/processed/mrms_training_data.parquet')\n```\n\n## ‚ö†Ô∏è Known Issues\n{chr(10).join(['- ' + issue for issue in issues]) if issues else '- None'}\n\n## ‚úÖ Next Steps\n1. Load data in respective training notebooks\n2. Verify checksums before training\n3. Monitor GPU memory during training\n4. Use chunked loading for large datasets\n\n---\n*Report generated by Data_Preparation_Colab_Fixed.ipynb*\n\"\"\"\n\n# Save report\nreport_path = f\"{DRIVE_BASE}/data/processed/data_preparation_report.md\"\nwith open(report_path, 'w') as f:\n    f.write(summary_report)\n\nprint(summary_report)\nprint(f\"\\n‚úÖ Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéâ DATA PREPARATION COMPLETE!\")\nprint(\"\\nüìã Summary:\")\nprint(f\"‚úÖ Created {len(metadata['checksums'])} output files\")\nprint(f\"‚úÖ Processed {len(combined_data):,} time steps\")\nprint(f\"‚úÖ Generated {len(main_df.columns)} features\")\nprint(f\"‚úÖ Data quality score: {quality_score}/100\")\nprint(\"\\nüöÄ Ready for MARL training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}