{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 2: MLMI → NW-RQK → FVG Trading Strategy\n",
    "\n",
    "**Ultra-Fast Backtesting with VectorBT and Numba JIT Compilation**\n",
    "\n",
    "This notebook implements the second synergy pattern where:\n",
    "1. MLMI provides the primary trend signal\n",
    "2. NW-RQK confirms the trend direction\n",
    "3. FVG validates the final entry zone\n",
    "\n",
    "Key differences from Synergy 1:\n",
    "- NW-RQK confirmation comes before FVG\n",
    "- May capture different market dynamics\n",
    "- Expected to generate similar trade counts but with different timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from numba import njit, prange, typed, types\n",
    "from numba.typed import Dict\n",
    "import warnings\n",
    "import time\n",
    "from typing import Tuple, Dict as TypeDict, Optional\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Numba for maximum performance\n",
    "import numba\n",
    "numba.config.THREADING_LAYER = 'threadsafe'\n",
    "numba.config.NUMBA_NUM_THREADS = numba.config.NUMBA_DEFAULT_NUM_THREADS\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Synergy 2: MLMI → NW-RQK → FVG Strategy\")\n",
    "print(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\n",
    "print(f\"VectorBT version: {vbt.__version__}\")\n",
    "print(\"Environment ready for ultra-fast backtesting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading with Pre-compilation\n",
    "\n",
    "def load_data_optimized(file_path: str, timeframe: str = '5m') -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare data with optimizations\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read CSV with optimized settings\n",
    "    df = pd.read_csv(file_path, \n",
    "                     parse_dates=['Timestamp'],\n",
    "                     infer_datetime_format=True,\n",
    "                     date_parser=lambda x: pd.to_datetime(x, dayfirst=True),\n",
    "                     index_col='Timestamp')\n",
    "    \n",
    "    # Ensure numeric types for fast operations\n",
    "    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float64)\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
    "    \n",
    "    # Sort index for faster operations\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Pre-calculate commonly used features\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['LogReturns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df['HL_Range'] = df['High'] - df['Low']\n",
    "    df['OC_Range'] = abs(df['Open'] - df['Close'])\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"Loaded {len(df):,} rows in {load_time:.2f} seconds\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Pre-compile all Numba functions\n",
    "print(\"Pre-compiling Numba functions for maximum speed...\")\n",
    "\n",
    "@njit(cache=True)\n",
    "def dummy_compile():\n",
    "    \"\"\"Dummy function to trigger compilation\"\"\"\n",
    "    return np.array([1.0, 2.0, 3.0]).sum()\n",
    "\n",
    "_ = dummy_compile()  # Trigger compilation\n",
    "\n",
    "# Load data files\n",
    "print(\"\\nLoading data files...\")\n",
    "file_5m = \"/home/QuantNova/AlgoSpace-Strategy-1/@NQ - 5 min - ETH.csv\"\n",
    "file_30m = \"/home/QuantNova/AlgoSpace-Strategy-1/NQ - 30 min - ETH.csv\"\n",
    "\n",
    "df_5m = load_data_optimized(file_5m, '5m')\n",
    "df_30m = load_data_optimized(file_30m, '30m')\n",
    "\n",
    "print(f\"\\n5-minute data: {df_5m.index[0]} to {df_5m.index[-1]}\")\n",
    "print(f\"30-minute data: {df_30m.index[0]} to {df_30m.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Optimized Indicator Suite\n",
    "\n",
    "@njit(fastmath=True, cache=True, parallel=True)\n",
    "def calculate_all_indicators(close: np.ndarray, high: np.ndarray, low: np.ndarray) -> TypeDict:\n",
    "    \"\"\"Calculate all basic indicators in one pass\"\"\"\n",
    "    n = len(close)\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    ma5 = np.full(n, np.nan)\n",
    "    ma20 = np.full(n, np.nan)\n",
    "    rsi5 = np.full(n, 50.0)\n",
    "    rsi20 = np.full(n, 50.0)\n",
    "    atr = np.full(n, np.nan)\n",
    "    \n",
    "    # Weighted Moving Averages\n",
    "    weights5 = np.arange(1, 6, dtype=np.float64)\n",
    "    weights20 = np.arange(1, 21, dtype=np.float64)\n",
    "    sum_w5 = weights5.sum()\n",
    "    sum_w20 = weights20.sum()\n",
    "    \n",
    "    # Calculate WMAs in parallel chunks\n",
    "    for i in prange(4, n):\n",
    "        if i >= 4:\n",
    "            ma5[i] = np.dot(close[i-4:i+1], weights5) / sum_w5\n",
    "        if i >= 19:\n",
    "            ma20[i] = np.dot(close[i-19:i+1], weights20) / sum_w20\n",
    "    \n",
    "    # RSI calculation\n",
    "    deltas = np.diff(close)\n",
    "    gains = np.maximum(deltas, 0)\n",
    "    losses = -np.minimum(deltas, 0)\n",
    "    \n",
    "    # RSI 5\n",
    "    avg_gain5 = np.mean(gains[:5]) if len(gains) >= 5 else 0\n",
    "    avg_loss5 = np.mean(losses[:5]) if len(losses) >= 5 else 0\n",
    "    \n",
    "    if avg_loss5 > 0:\n",
    "        rsi5[5] = 100 - (100 / (1 + avg_gain5 / avg_loss5))\n",
    "    else:\n",
    "        rsi5[5] = 100\n",
    "    \n",
    "    for i in range(5, n - 1):\n",
    "        avg_gain5 = (avg_gain5 * 4 + gains[i]) / 5\n",
    "        avg_loss5 = (avg_loss5 * 4 + losses[i]) / 5\n",
    "        if avg_loss5 > 0:\n",
    "            rsi5[i + 1] = 100 - (100 / (1 + avg_gain5 / avg_loss5))\n",
    "        else:\n",
    "            rsi5[i + 1] = 100\n",
    "    \n",
    "    # RSI 20\n",
    "    avg_gain20 = np.mean(gains[:20]) if len(gains) >= 20 else 0\n",
    "    avg_loss20 = np.mean(losses[:20]) if len(losses) >= 20 else 0\n",
    "    \n",
    "    if avg_loss20 > 0:\n",
    "        rsi20[20] = 100 - (100 / (1 + avg_gain20 / avg_loss20))\n",
    "    else:\n",
    "        rsi20[20] = 100\n",
    "    \n",
    "    for i in range(20, n - 1):\n",
    "        avg_gain20 = (avg_gain20 * 19 + gains[i]) / 20\n",
    "        avg_loss20 = (avg_loss20 * 19 + losses[i]) / 20\n",
    "        if avg_loss20 > 0:\n",
    "            rsi20[i + 1] = 100 - (100 / (1 + avg_gain20 / avg_loss20))\n",
    "        else:\n",
    "            rsi20[i + 1] = 100\n",
    "    \n",
    "    # ATR calculation\n",
    "    tr = np.maximum(high - low, np.maximum(abs(high - np.roll(close, 1)), abs(low - np.roll(close, 1))))\n",
    "    tr[0] = high[0] - low[0]\n",
    "    \n",
    "    for i in range(14, n):\n",
    "        atr[i] = np.mean(tr[i-13:i+1])\n",
    "    \n",
    "    return ma5, ma20, rsi5, rsi20, atr\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_fvg_optimized(high: np.ndarray, low: np.ndarray, atr: np.ndarray,\n",
    "                        multiplier: float = 1.5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Optimized FVG detection with ATR filtering\"\"\"\n",
    "    n = len(high)\n",
    "    bull_active = np.zeros(n, dtype=np.bool_)\n",
    "    bear_active = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    for i in prange(3, n):\n",
    "        if not np.isnan(atr[i]):\n",
    "            # Dynamic gap threshold based on ATR\n",
    "            gap_threshold = atr[i] * multiplier\n",
    "            \n",
    "            # Bullish FVG with ATR filter\n",
    "            gap_size = low[i] - high[i-3]\n",
    "            if gap_size > gap_threshold:\n",
    "                # Mark active zone\n",
    "                for j in range(i, min(i + 20, n)):\n",
    "                    if low[j] >= high[i-3]:\n",
    "                        bull_active[j] = True\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # Bearish FVG with ATR filter\n",
    "            gap_size = low[i-3] - high[i]\n",
    "            if gap_size > gap_threshold:\n",
    "                # Mark active zone\n",
    "                for j in range(i, min(i + 20, n)):\n",
    "                    if high[j] <= low[i-3]:\n",
    "                        bear_active[j] = True\n",
    "                    else:\n",
    "                        break\n",
    "    \n",
    "    return bull_active, bear_active\n",
    "\n",
    "print(\"\\nCalculating all indicators with parallel processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate 30-minute indicators\n",
    "close_30m = df_30m['Close'].values\n",
    "high_30m = df_30m['High'].values\n",
    "low_30m = df_30m['Low'].values\n",
    "\n",
    "ma5, ma20, rsi5, rsi20, atr_30m = calculate_all_indicators(close_30m, high_30m, low_30m)\n",
    "\n",
    "# Smooth RSI\n",
    "rsi5_smooth = np.convolve(rsi5, np.ones(20)/20, mode='same')\n",
    "rsi20_smooth = np.convolve(rsi20, np.ones(20)/20, mode='same')\n",
    "\n",
    "# Calculate 5-minute indicators\n",
    "close_5m = df_5m['Close'].values\n",
    "high_5m = df_5m['High'].values\n",
    "low_5m = df_5m['Low'].values\n",
    "\n",
    "_, _, _, _, atr_5m = calculate_all_indicators(close_5m, high_5m, low_5m)\n",
    "\n",
    "# Detect FVG with ATR filter\n",
    "fvg_bull, fvg_bear = detect_fvg_optimized(high_5m, low_5m, atr_5m)\n",
    "\n",
    "calc_time = time.time() - start_time\n",
    "print(f\"All indicators calculated in {calc_time:.3f} seconds\")\n",
    "print(f\"FVG zones detected - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Advanced MLMI with Adaptive KNN\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def adaptive_knn_predict(features: np.ndarray, labels: np.ndarray, query: np.ndarray,\n",
    "                        k_base: int, volatility: float, size: int) -> Tuple[float, float]:\n",
    "    \"\"\"Adaptive KNN that adjusts K based on market volatility\"\"\"\n",
    "    if size == 0:\n",
    "        return 0.0, 0.5\n",
    "    \n",
    "    # Adjust K based on volatility\n",
    "    k = max(3, min(k_base, int(k_base * (1 - volatility * 2))))\n",
    "    k = min(k, size)\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        dist = 0.0\n",
    "        for j in range(2):\n",
    "            diff = features[i, j] - query[j]\n",
    "            dist += diff * diff\n",
    "        distances[i] = np.sqrt(dist)\n",
    "    \n",
    "    # Find k nearest neighbors\n",
    "    indices = np.argpartition(distances, k-1)[:k]\n",
    "    \n",
    "    # Weighted voting based on distance\n",
    "    vote = 0.0\n",
    "    weight_sum = 0.0\n",
    "    \n",
    "    for i in range(k):\n",
    "        idx = indices[i]\n",
    "        if distances[idx] > 0:\n",
    "            weight = 1.0 / distances[idx]\n",
    "            vote += labels[idx] * weight\n",
    "            weight_sum += weight\n",
    "    \n",
    "    if weight_sum > 0:\n",
    "        prediction = vote / weight_sum\n",
    "        confidence = min(abs(prediction) / k, 1.0)\n",
    "    else:\n",
    "        prediction = 0.0\n",
    "        confidence = 0.0\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def calculate_mlmi_adaptive(ma_fast: np.ndarray, ma_slow: np.ndarray,\n",
    "                           rsi_fast_smooth: np.ndarray, rsi_slow_smooth: np.ndarray,\n",
    "                           close: np.ndarray, returns: np.ndarray,\n",
    "                           k_neighbors: int = 200) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"MLMI with adaptive KNN and confidence scores\"\"\"\n",
    "    n = len(close)\n",
    "    mlmi_values = np.zeros(n)\n",
    "    mlmi_confidence = np.zeros(n)\n",
    "    \n",
    "    # Pre-allocate KNN storage\n",
    "    max_size = min(10000, n)\n",
    "    features = np.zeros((max_size, 2))\n",
    "    labels = np.zeros(max_size)\n",
    "    data_size = 0\n",
    "    \n",
    "    # Calculate rolling volatility\n",
    "    volatility = np.zeros(n)\n",
    "    for i in range(20, n):\n",
    "        volatility[i] = np.std(returns[i-20:i])\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        # Detect crossovers\n",
    "        bull_cross = ma_fast[i] > ma_slow[i] and ma_fast[i-1] <= ma_slow[i-1]\n",
    "        bear_cross = ma_fast[i] < ma_slow[i] and ma_fast[i-1] >= ma_slow[i-1]\n",
    "        \n",
    "        if (bull_cross or bear_cross) and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            # Store pattern\n",
    "            if data_size >= max_size:\n",
    "                # Keep most recent 75%\n",
    "                keep_size = int(max_size * 0.75)\n",
    "                features[:keep_size] = features[-keep_size:]\n",
    "                labels[:keep_size] = labels[-keep_size:]\n",
    "                data_size = keep_size\n",
    "            \n",
    "            features[data_size, 0] = rsi_slow_smooth[i]\n",
    "            features[data_size, 1] = rsi_fast_smooth[i]\n",
    "            \n",
    "            if i < n - 1:\n",
    "                # Multi-bar forward return for better signal\n",
    "                fwd_ret = (close[min(i+5, n-1)] - close[i]) / close[i]\n",
    "                labels[data_size] = np.sign(fwd_ret) * min(abs(fwd_ret) * 100, 1.0)\n",
    "            else:\n",
    "                labels[data_size] = 0.0\n",
    "            \n",
    "            data_size += 1\n",
    "        \n",
    "        # Make prediction\n",
    "        if data_size > 10 and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            query = np.array([rsi_slow_smooth[i], rsi_fast_smooth[i]])\n",
    "            pred, conf = adaptive_knn_predict(features, labels, query,\n",
    "                                            k_neighbors, volatility[i], data_size)\n",
    "            mlmi_values[i] = pred * 100  # Scale for visibility\n",
    "            mlmi_confidence[i] = conf\n",
    "    \n",
    "    return mlmi_values, mlmi_confidence\n",
    "\n",
    "# Calculate MLMI with confidence\n",
    "print(\"\\nCalculating adaptive MLMI with confidence scores...\")\n",
    "start_time = time.time()\n",
    "\n",
    "returns_30m = df_30m['Returns'].values\n",
    "mlmi_values, mlmi_confidence = calculate_mlmi_adaptive(\n",
    "    ma5, ma20, rsi5_smooth, rsi20_smooth, close_30m, returns_30m\n",
    ")\n",
    "\n",
    "# Store in dataframe\n",
    "df_30m['mlmi'] = mlmi_values\n",
    "df_30m['mlmi_confidence'] = mlmi_confidence\n",
    "df_30m['mlmi_bull'] = (mlmi_values > 0) & (mlmi_confidence > 0.3)\n",
    "df_30m['mlmi_bear'] = (mlmi_values < 0) & (mlmi_confidence > 0.3)\n",
    "\n",
    "mlmi_time = time.time() - start_time\n",
    "print(f\"Adaptive MLMI calculated in {mlmi_time:.3f} seconds\")\n",
    "print(f\"MLMI range: [{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")\n",
    "print(f\"Average confidence: {mlmi_confidence.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Enhanced NW-RQK with Multiple Kernels\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def gaussian_kernel(x: float, h: float) -> float:\n",
    "    \"\"\"Gaussian kernel function\"\"\"\n",
    "    return np.exp(-(x * x) / (2.0 * h * h))\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def epanechnikov_kernel(x: float, h: float) -> float:\n",
    "    \"\"\"Epanechnikov kernel function\"\"\"\n",
    "    u = x / h\n",
    "    if abs(u) <= 1:\n",
    "        return 0.75 * (1 - u * u)\n",
    "    return 0.0\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def nadaraya_watson_ensemble(prices: np.ndarray, h: float, r: float,\n",
    "                           min_periods: int = 25) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Ensemble NW regression with multiple kernels\"\"\"\n",
    "    n = len(prices)\n",
    "    result_rq = np.full(n, np.nan)  # Rational Quadratic\n",
    "    result_gauss = np.full(n, np.nan)  # Gaussian\n",
    "    \n",
    "    for i in prange(min_periods, n):\n",
    "        # Rational Quadratic regression\n",
    "        weighted_sum_rq = 0.0\n",
    "        weight_sum_rq = 0.0\n",
    "        \n",
    "        # Gaussian regression\n",
    "        weighted_sum_gauss = 0.0\n",
    "        weight_sum_gauss = 0.0\n",
    "        \n",
    "        window_size = min(i + 1, 500)\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            if i - j >= 0:\n",
    "                # Rational Quadratic\n",
    "                weight_rq = (1.0 + (j * j) / (h * h * 2.0 * r)) ** (-r)\n",
    "                weighted_sum_rq += prices[i - j] * weight_rq\n",
    "                weight_sum_rq += weight_rq\n",
    "                \n",
    "                # Gaussian\n",
    "                weight_gauss = gaussian_kernel(float(j), h)\n",
    "                weighted_sum_gauss += prices[i - j] * weight_gauss\n",
    "                weight_sum_gauss += weight_gauss\n",
    "        \n",
    "        if weight_sum_rq > 0:\n",
    "            result_rq[i] = weighted_sum_rq / weight_sum_rq\n",
    "        if weight_sum_gauss > 0:\n",
    "            result_gauss[i] = weighted_sum_gauss / weight_sum_gauss\n",
    "    \n",
    "    # Ensemble: average of both kernels\n",
    "    ensemble = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if not np.isnan(result_rq[i]) and not np.isnan(result_gauss[i]):\n",
    "            ensemble[i] = (result_rq[i] + result_gauss[i]) / 2\n",
    "        elif not np.isnan(result_rq[i]):\n",
    "            ensemble[i] = result_rq[i]\n",
    "        elif not np.isnan(result_gauss[i]):\n",
    "            ensemble[i] = result_gauss[i]\n",
    "        else:\n",
    "            ensemble[i] = np.nan\n",
    "    \n",
    "    return ensemble, result_rq\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def detect_nwrqk_signals_enhanced(yhat1: np.ndarray, yhat2: np.ndarray,\n",
    "                                 prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Enhanced signal detection with strength measurement\"\"\"\n",
    "    n = len(yhat1)\n",
    "    bull_signals = np.zeros(n, dtype=np.bool_)\n",
    "    bear_signals = np.zeros(n, dtype=np.bool_)\n",
    "    signal_strength = np.zeros(n)\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        if not np.isnan(yhat1[i]) and not np.isnan(yhat1[i-1]) and not np.isnan(yhat1[i-2]):\n",
    "            # Trend changes\n",
    "            slope_prev = yhat1[i-1] - yhat1[i-2]\n",
    "            slope_curr = yhat1[i] - yhat1[i-1]\n",
    "            \n",
    "            # Acceleration\n",
    "            acceleration = slope_curr - slope_prev\n",
    "            \n",
    "            # Bullish: negative to positive slope with positive acceleration\n",
    "            if slope_prev < 0 and slope_curr > 0 and acceleration > 0:\n",
    "                bull_signals[i] = True\n",
    "                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n",
    "            \n",
    "            # Bearish: positive to negative slope with negative acceleration\n",
    "            elif slope_prev > 0 and slope_curr < 0 and acceleration < 0:\n",
    "                bear_signals[i] = True\n",
    "                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n",
    "        \n",
    "        # Crossovers with momentum\n",
    "        if i > 0 and not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]):\n",
    "            if not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n",
    "                # Price momentum filter\n",
    "                price_momentum = (prices[i] - prices[max(0, i-5)]) / prices[max(0, i-5)]\n",
    "                \n",
    "                if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1] and price_momentum > 0:\n",
    "                    bull_signals[i] = True\n",
    "                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n",
    "                elif yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1] and price_momentum < 0:\n",
    "                    bear_signals[i] = True\n",
    "                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n",
    "    \n",
    "    return bull_signals, bear_signals, signal_strength\n",
    "\n",
    "# Calculate enhanced NW-RQK\n",
    "print(\"\\nCalculating enhanced NW-RQK with ensemble kernels...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Parameters\n",
    "h = 8.0\n",
    "r = 8.0\n",
    "lag = 2\n",
    "\n",
    "# Calculate regression lines\n",
    "yhat1, yhat1_rq = nadaraya_watson_ensemble(close_30m, h, r)\n",
    "yhat2, yhat2_rq = nadaraya_watson_ensemble(close_30m, h - lag, r)\n",
    "\n",
    "# Detect signals with strength\n",
    "nwrqk_bull, nwrqk_bear, nwrqk_strength = detect_nwrqk_signals_enhanced(yhat1, yhat2, close_30m)\n",
    "\n",
    "# Store in dataframe\n",
    "df_30m['nwrqk_bull'] = nwrqk_bull\n",
    "df_30m['nwrqk_bear'] = nwrqk_bear\n",
    "df_30m['nwrqk_strength'] = nwrqk_strength\n",
    "df_30m['yhat1'] = yhat1\n",
    "df_30m['yhat2'] = yhat2\n",
    "\n",
    "nwrqk_time = time.time() - start_time\n",
    "print(f\"Enhanced NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n",
    "print(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")\n",
    "print(f\"Average signal strength: {nwrqk_strength[nwrqk_strength > 0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Smart Timeframe Alignment\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def create_alignment_map(timestamps_5m: np.ndarray, timestamps_30m: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Create efficient mapping between timeframes\"\"\"\n",
    "    n_5m = len(timestamps_5m)\n",
    "    mapping = np.zeros(n_5m, dtype=np.int64)\n",
    "    \n",
    "    j = 0\n",
    "    for i in prange(n_5m):\n",
    "        # Find the corresponding 30m bar\n",
    "        while j < len(timestamps_30m) - 1 and timestamps_30m[j + 1] <= timestamps_5m[i]:\n",
    "            j += 1\n",
    "        mapping[i] = j\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "print(\"\\nPerforming smart timeframe alignment...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create datetime arrays for mapping\n",
    "# Convert to numeric timestamps for Numba\n",
    "timestamps_5m = df_5m.index.astype(np.int64) // 10**9\n",
    "timestamps_30m = df_30m.index.astype(np.int64) // 10**9\n",
    "\n",
    "# Create mapping\n",
    "mapping = create_alignment_map(timestamps_5m, timestamps_30m)\n",
    "\n",
    "# Align all indicators efficiently\n",
    "df_5m_aligned = df_5m.copy()\n",
    "\n",
    "# MLMI alignment with confidence\n",
    "df_5m_aligned['mlmi'] = df_30m['mlmi'].values[mapping]\n",
    "df_5m_aligned['mlmi_confidence'] = df_30m['mlmi_confidence'].values[mapping]\n",
    "df_5m_aligned['mlmi_bull'] = df_30m['mlmi_bull'].values[mapping]\n",
    "df_5m_aligned['mlmi_bear'] = df_30m['mlmi_bear'].values[mapping]\n",
    "\n",
    "# NW-RQK alignment with strength\n",
    "df_5m_aligned['nwrqk_bull'] = df_30m['nwrqk_bull'].values[mapping]\n",
    "df_5m_aligned['nwrqk_bear'] = df_30m['nwrqk_bear'].values[mapping]\n",
    "df_5m_aligned['nwrqk_strength'] = df_30m['nwrqk_strength'].values[mapping]\n",
    "\n",
    "# FVG data\n",
    "df_5m_aligned['fvg_bull'] = fvg_bull\n",
    "df_5m_aligned['fvg_bear'] = fvg_bear\n",
    "\n",
    "# Add market regime detection\n",
    "df_5m_aligned['volatility'] = df_5m_aligned['Returns'].rolling(20).std()\n",
    "df_5m_aligned['trend_strength'] = abs(df_5m_aligned['Returns'].rolling(50).mean()) / df_5m_aligned['volatility']\n",
    "\n",
    "align_time = time.time() - start_time\n",
    "print(f\"Smart alignment completed in {align_time:.3f} seconds\")\n",
    "print(f\"Aligned {len(df_5m_aligned):,} 5-minute bars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: MLMI → NW-RQK → FVG Synergy Detection\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_mlmi_nwrqk_fvg_synergy(mlmi_bull: np.ndarray, mlmi_bear: np.ndarray,\n",
    "                                 mlmi_conf: np.ndarray, nwrqk_bull: np.ndarray,\n",
    "                                 nwrqk_bear: np.ndarray, nwrqk_strength: np.ndarray,\n",
    "                                 fvg_bull: np.ndarray, fvg_bear: np.ndarray,\n",
    "                                 volatility: np.ndarray, window: int = 30) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Advanced synergy detection with confidence scoring\"\"\"\n",
    "    n = len(mlmi_bull)\n",
    "    long_signals = np.zeros(n, dtype=np.bool_)\n",
    "    short_signals = np.zeros(n, dtype=np.bool_)\n",
    "    signal_quality = np.zeros(n)\n",
    "    \n",
    "    # State tracking\n",
    "    mlmi_active_bull = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_active_bear = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_confirmed_bull = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_confirmed_bear = np.zeros(n, dtype=np.bool_)\n",
    "    state_timer = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        # Carry forward states\n",
    "        mlmi_active_bull[i] = mlmi_active_bull[i-1]\n",
    "        mlmi_active_bear[i] = mlmi_active_bear[i-1]\n",
    "        nwrqk_confirmed_bull[i] = nwrqk_confirmed_bull[i-1]\n",
    "        nwrqk_confirmed_bear[i] = nwrqk_confirmed_bear[i-1]\n",
    "        state_timer[i] = state_timer[i-1] + 1\n",
    "        \n",
    "        # Volatility adjustment\n",
    "        vol_factor = 1.0 / (1.0 + volatility[i] * 10) if not np.isnan(volatility[i]) else 1.0\n",
    "        \n",
    "        # Reset on opposite signal or timeout\n",
    "        if mlmi_bear[i] or state_timer[i] > window:\n",
    "            mlmi_active_bull[i] = False\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            if mlmi_bear[i]:\n",
    "                state_timer[i] = 0\n",
    "        \n",
    "        if mlmi_bull[i] or state_timer[i] > window:\n",
    "            mlmi_active_bear[i] = False\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            if mlmi_bull[i]:\n",
    "                state_timer[i] = 0\n",
    "        \n",
    "        # Step 1: MLMI signal with confidence filter\n",
    "        if mlmi_bull[i] and not mlmi_bull[i-1] and mlmi_conf[i] > 0.3:\n",
    "            mlmi_active_bull[i] = True\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        if mlmi_bear[i] and not mlmi_bear[i-1] and mlmi_conf[i] > 0.3:\n",
    "            mlmi_active_bear[i] = True\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        # Step 2: NW-RQK confirmation with strength filter\n",
    "        if mlmi_active_bull[i] and not nwrqk_confirmed_bull[i] and nwrqk_bull[i] and nwrqk_strength[i] > 0.2:\n",
    "            nwrqk_confirmed_bull[i] = True\n",
    "        \n",
    "        if mlmi_active_bear[i] and not nwrqk_confirmed_bear[i] and nwrqk_bear[i] and nwrqk_strength[i] > 0.2:\n",
    "            nwrqk_confirmed_bear[i] = True\n",
    "        \n",
    "        # Step 3: FVG final confirmation\n",
    "        if nwrqk_confirmed_bull[i] and fvg_bull[i]:\n",
    "            long_signals[i] = True\n",
    "            # Calculate signal quality\n",
    "            signal_quality[i] = (mlmi_conf[i] + nwrqk_strength[i]) / 2 * vol_factor\n",
    "            # Reset states\n",
    "            mlmi_active_bull[i] = False\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        if nwrqk_confirmed_bear[i] and fvg_bear[i]:\n",
    "            short_signals[i] = True\n",
    "            # Calculate signal quality\n",
    "            signal_quality[i] = (mlmi_conf[i] + nwrqk_strength[i]) / 2 * vol_factor\n",
    "            # Reset states\n",
    "            mlmi_active_bear[i] = False\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            state_timer[i] = 0\n",
    "    \n",
    "    return long_signals, short_signals, signal_quality\n",
    "\n",
    "print(\"\\nDetecting MLMI → NW-RQK → FVG synergy signals...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract arrays\n",
    "mlmi_bull_arr = df_5m_aligned['mlmi_bull'].values\n",
    "mlmi_bear_arr = df_5m_aligned['mlmi_bear'].values\n",
    "mlmi_conf_arr = df_5m_aligned['mlmi_confidence'].values\n",
    "nwrqk_bull_arr = df_5m_aligned['nwrqk_bull'].values\n",
    "nwrqk_bear_arr = df_5m_aligned['nwrqk_bear'].values\n",
    "nwrqk_strength_arr = df_5m_aligned['nwrqk_strength'].values\n",
    "fvg_bull_arr = df_5m_aligned['fvg_bull'].values\n",
    "fvg_bear_arr = df_5m_aligned['fvg_bear'].values\n",
    "volatility_arr = df_5m_aligned['volatility'].fillna(0.01).values\n",
    "\n",
    "# Detect synergy\n",
    "long_entries, short_entries, signal_quality = detect_mlmi_nwrqk_fvg_synergy(\n",
    "    mlmi_bull_arr, mlmi_bear_arr, mlmi_conf_arr,\n",
    "    nwrqk_bull_arr, nwrqk_bear_arr, nwrqk_strength_arr,\n",
    "    fvg_bull_arr, fvg_bear_arr, volatility_arr\n",
    ")\n",
    "\n",
    "# Add to dataframe\n",
    "df_5m_aligned['long_entry'] = long_entries\n",
    "df_5m_aligned['short_entry'] = short_entries\n",
    "df_5m_aligned['signal_quality'] = signal_quality\n",
    "\n",
    "signal_time = time.time() - start_time\n",
    "print(f\"Synergy detection completed in {signal_time:.3f} seconds\")\n",
    "print(f\"Long entries: {long_entries.sum():,}\")\n",
    "print(f\"Short entries: {short_entries.sum():,}\")\n",
    "print(f\"Average signal quality: {signal_quality[signal_quality > 0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Advanced VectorBT Backtesting\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADVANCED VECTORBT BACKTESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data\n",
    "close_prices = df_5m_aligned['Close']\n",
    "entries = df_5m_aligned['long_entry'] | df_5m_aligned['short_entry']\n",
    "direction = np.where(df_5m_aligned['long_entry'], 1, \n",
    "                    np.where(df_5m_aligned['short_entry'], -1, 0))\n",
    "\n",
    "# Dynamic position sizing based on signal quality\n",
    "base_size = 100\n",
    "position_sizes = np.where(entries, base_size * (0.5 + df_5m_aligned['signal_quality'] * 0.5), base_size)\n",
    "\n",
    "# Advanced exit logic\n",
    "@njit(fastmath=True)\n",
    "def generate_exits(entries: np.ndarray, direction: np.ndarray, \n",
    "                  high: np.ndarray, low: np.ndarray, close: np.ndarray,\n",
    "                  atr: np.ndarray, max_bars: int = 100) -> np.ndarray:\n",
    "    \"\"\"Generate exits with stop loss and time-based exits\"\"\"\n",
    "    n = len(entries)\n",
    "    exits = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    in_position = False\n",
    "    entry_price = 0.0\n",
    "    entry_idx = 0\n",
    "    position_dir = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if not in_position and entries[i]:\n",
    "            # Enter position\n",
    "            in_position = True\n",
    "            entry_price = close[i]\n",
    "            entry_idx = i\n",
    "            position_dir = direction[i]\n",
    "        \n",
    "        elif in_position:\n",
    "            # Check exit conditions\n",
    "            bars_held = i - entry_idx\n",
    "            \n",
    "            # Stop loss (2 ATR)\n",
    "            if not np.isnan(atr[i]):\n",
    "                stop_distance = 2 * atr[i]\n",
    "                \n",
    "                if position_dir > 0:  # Long position\n",
    "                    if low[i] <= entry_price - stop_distance:\n",
    "                        exits[i] = True\n",
    "                        in_position = False\n",
    "                elif position_dir < 0:  # Short position\n",
    "                    if high[i] >= entry_price + stop_distance:\n",
    "                        exits[i] = True\n",
    "                        in_position = False\n",
    "            \n",
    "            # Time-based exit\n",
    "            if bars_held >= max_bars:\n",
    "                exits[i] = True\n",
    "                in_position = False\n",
    "            \n",
    "            # Exit on opposite signal\n",
    "            if entries[i] and direction[i] != position_dir:\n",
    "                exits[i] = True\n",
    "                in_position = False\n",
    "    \n",
    "    return exits\n",
    "\n",
    "# Generate exits\n",
    "print(\"\\nGenerating advanced exit signals...\")\n",
    "exits = generate_exits(\n",
    "    entries.values,\n",
    "    direction,\n",
    "    df_5m_aligned['High'].values,\n",
    "    df_5m_aligned['Low'].values,\n",
    "    close_prices.values,\n",
    "    atr_5m\n",
    ")\n",
    "\n",
    "print(\"\\nRunning advanced backtest...\")\n",
    "backtest_start = time.time()\n",
    "\n",
    "# Run backtest\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=close_prices,\n",
    "    entries=entries,\n",
    "    exits=exits,\n",
    "    direction=direction,\n",
    "    size=position_sizes,\n",
    "    size_type='amount',\n",
    "    init_cash=100000,\n",
    "    fees=0.0001,\n",
    "    slippage=0.0001,\n",
    "    freq='5T'\n",
    ")\n",
    "\n",
    "backtest_time = time.time() - backtest_start\n",
    "print(f\"\\nBacktest completed in {backtest_time:.3f} seconds!\")\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "stats = portfolio.stats()\n",
    "returns = portfolio.returns()\n",
    "trades = portfolio.trades.records_readable\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "print(f\"Annualized Return: {stats['Total Return [%]'] * (252*78/len(df_5m_aligned)):.2f}%\")\n",
    "print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "print(f\"Sortino Ratio: {stats['Sortino Ratio']:.2f}\")\n",
    "print(f\"Calmar Ratio: {stats['Calmar Ratio']:.2f}\")\n",
    "print(f\"Max Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "print(f\"Max Drawdown Duration: {stats['Max Drawdown Duration']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TRADE STATISTICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Trades: {stats['Total Trades']:,.0f}\")\n",
    "print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "print(f\"Profit Factor: {stats['Profit Factor']:.2f}\")\n",
    "print(f\"Expectancy: {stats['Expectancy [%]']:.3f}%\")\n",
    "print(f\"Average Win: {stats['Avg Winning Trade [%]']:.2f}%\")\n",
    "print(f\"Average Loss: {stats['Avg Losing Trade [%]']:.2f}%\")\n",
    "print(f\"Best Trade: {stats['Best Trade [%]']:.2f}%\")\n",
    "print(f\"Worst Trade: {stats['Worst Trade [%]']:.2f}%\")\n",
    "\n",
    "# Additional analysis\n",
    "if len(trades) > 0:\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"TRADE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    avg_duration = trades['Duration'].mean()\n",
    "    print(f\"Average Trade Duration: {avg_duration}\")\n",
    "    print(f\"Daily Trades: {len(trades) / (len(df_5m_aligned) / 78):.1f}\")\n",
    "    print(f\"Trade Frequency: Every {len(df_5m_aligned) / len(trades):.0f} bars\")\n",
    "    \n",
    "    # Win/Loss streaks\n",
    "    returns_array = trades['PnL %'].values\n",
    "    wins = returns_array > 0\n",
    "    max_win_streak = 0\n",
    "    max_loss_streak = 0\n",
    "    current_win_streak = 0\n",
    "    current_loss_streak = 0\n",
    "    \n",
    "    for win in wins:\n",
    "        if win:\n",
    "            current_win_streak += 1\n",
    "            current_loss_streak = 0\n",
    "            max_win_streak = max(max_win_streak, current_win_streak)\n",
    "        else:\n",
    "            current_loss_streak += 1\n",
    "            current_win_streak = 0\n",
    "            max_loss_streak = max(max_loss_streak, current_loss_streak)\n",
    "    \n",
    "    print(f\"Max Win Streak: {max_win_streak}\")\n",
    "    print(f\"Max Loss Streak: {max_loss_streak}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Professional Multi-Panel Visualization\n",
    "\n",
    "print(\"\\nGenerating professional multi-panel visualization...\")\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=2,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.03,\n",
    "    horizontal_spacing=0.05,\n",
    "    row_heights=[0.3, 0.2, 0.2, 0.2, 0.1],\n",
    "    column_widths=[0.7, 0.3],\n",
    "    subplot_titles=(\n",
    "        'Cumulative Returns', 'Monthly Returns Heatmap',\n",
    "        'Drawdown Analysis', 'Trade Distribution',\n",
    "        'Signal Quality', 'Win Rate by Signal Quality',\n",
    "        'Price Action with Signals', 'Trade Duration Distribution',\n",
    "        'Volume Profile', ''\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"secondary_y\": False}, {\"type\": \"heatmap\"}],\n",
    "        [{\"secondary_y\": False}, {\"type\": \"histogram\"}],\n",
    "        [{\"secondary_y\": False}, {\"type\": \"scatter\"}],\n",
    "        [{\"secondary_y\": True}, {\"type\": \"histogram\"}],\n",
    "        [{\"secondary_y\": False}, {\"type\": \"scatter\"}]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 1. Cumulative Returns\n",
    "cumulative_returns = (1 + returns).cumprod() - 1\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cumulative_returns.index,\n",
    "        y=cumulative_returns.values * 100,\n",
    "        mode='lines',\n",
    "        name='Strategy',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Benchmark (buy and hold)\n",
    "benchmark_returns = (close_prices / close_prices.iloc[0] - 1) * 100\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=benchmark_returns.index,\n",
    "        y=benchmark_returns.values,\n",
    "        mode='lines',\n",
    "        name='Buy & Hold',\n",
    "        line=dict(color='gray', width=1, dash='dash')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Monthly Returns Heatmap\n",
    "monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1) * 100\n",
    "monthly_matrix = monthly_returns.values.reshape(-1, 12)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=monthly_matrix,\n",
    "        colorscale='RdYlGn',\n",
    "        zmid=0,\n",
    "        text=np.round(monthly_matrix, 1),\n",
    "        texttemplate='%{text}%',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Drawdown\n",
    "drawdown = portfolio.drawdown() * 100\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=drawdown.index,\n",
    "        y=-drawdown.values,\n",
    "        mode='lines',\n",
    "        name='Drawdown',\n",
    "        fill='tozeroy',\n",
    "        line=dict(color='red', width=1)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Trade Returns Distribution\n",
    "if len(trades) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=trades['PnL %'],\n",
    "            nbinsx=50,\n",
    "            name='Returns',\n",
    "            marker_color=np.where(trades['PnL %'] > 0, 'green', 'red')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# 5. Signal Quality over time\n",
    "signal_points = df_5m_aligned[df_5m_aligned['signal_quality'] > 0]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=signal_points.index,\n",
    "        y=signal_points['signal_quality'],\n",
    "        mode='markers',\n",
    "        name='Signal Quality',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=signal_points['signal_quality'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=False\n",
    "        )\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Win Rate by Signal Quality bins\n",
    "if len(trades) > 0 and 'signal_quality' in df_5m_aligned.columns:\n",
    "    # Match trades with signal quality\n",
    "    quality_bins = pd.qcut(signal_points['signal_quality'], q=5, duplicates='drop')\n",
    "    quality_win_rate = signal_points.groupby(quality_bins).apply(\n",
    "        lambda x: (x['long_entry'] | x['short_entry']).sum()\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(quality_win_rate))),\n",
    "            y=quality_win_rate.values,\n",
    "            mode='lines+markers',\n",
    "            name='Trade Count by Quality',\n",
    "            line=dict(color='purple', width=2)\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "\n",
    "# 7. Price Action with Signals (zoomed to recent 500 bars)\n",
    "recent_bars = min(500, len(df_5m_aligned))\n",
    "recent_df = df_5m_aligned.tail(recent_bars)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Candlestick(\n",
    "        x=recent_df.index,\n",
    "        open=recent_df['Open'],\n",
    "        high=recent_df['High'],\n",
    "        low=recent_df['Low'],\n",
    "        close=recent_df['Close'],\n",
    "        name='Price',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "# Add signals\n",
    "long_signals = recent_df[recent_df['long_entry']]\n",
    "short_signals = recent_df[recent_df['short_entry']]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=long_signals.index,\n",
    "        y=long_signals['Low'] * 0.995,\n",
    "        mode='markers',\n",
    "        name='Long',\n",
    "        marker=dict(symbol='triangle-up', size=8, color='green')\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=short_signals.index,\n",
    "        y=short_signals['High'] * 1.005,\n",
    "        mode='markers',\n",
    "        name='Short',\n",
    "        marker=dict(symbol='triangle-down', size=8, color='red')\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "# 8. Trade Duration Distribution\n",
    "if len(trades) > 0:\n",
    "    durations = trades['Duration'].dt.total_seconds() / 3600  # Convert to hours\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=durations,\n",
    "            nbinsx=30,\n",
    "            name='Duration (hours)',\n",
    "            marker_color='orange'\n",
    "        ),\n",
    "        row=4, col=2\n",
    "    )\n",
    "\n",
    "# 9. Volume Profile\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=recent_df.index,\n",
    "        y=recent_df['Volume'],\n",
    "        name='Volume',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=5, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='MLMI → NW-RQK → FVG Synergy Strategy - Comprehensive Analysis',\n",
    "    height=1600,\n",
    "    showlegend=True,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Signal Quality\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Price\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Volume\", row=5, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Statistical Validation and Robustness Testing\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def bootstrap_confidence_intervals(returns: np.ndarray, n_bootstrap: int = 10000,\n",
    "                                  confidence: float = 0.95) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Bootstrap confidence intervals for key metrics\"\"\"\n",
    "    n = len(returns)\n",
    "    \n",
    "    # Arrays to store bootstrap results\n",
    "    boot_returns = np.zeros(n_bootstrap)\n",
    "    boot_sharpes = np.zeros(n_bootstrap)\n",
    "    boot_max_dd = np.zeros(n_bootstrap)\n",
    "    boot_win_rates = np.zeros(n_bootstrap)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    clean_returns = returns[~np.isnan(returns)]\n",
    "    n_clean = len(clean_returns)\n",
    "    \n",
    "    if n_clean == 0:\n",
    "        return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n",
    "    \n",
    "    # Bootstrap iterations\n",
    "    for i in prange(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        indices = np.random.randint(0, n_clean, size=n_clean)\n",
    "        sample = clean_returns[indices]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        boot_returns[i] = np.prod(1 + sample) - 1\n",
    "        \n",
    "        mean_ret = np.mean(sample)\n",
    "        std_ret = np.std(sample)\n",
    "        if std_ret > 0:\n",
    "            boot_sharpes[i] = mean_ret / std_ret * np.sqrt(252 * 78)\n",
    "        \n",
    "        # Max drawdown\n",
    "        cum_ret = np.cumprod(1 + sample)\n",
    "        running_max = np.maximum.accumulate(cum_ret)\n",
    "        dd = (cum_ret - running_max) / running_max\n",
    "        boot_max_dd[i] = np.min(dd)\n",
    "        \n",
    "        # Win rate\n",
    "        boot_win_rates[i] = np.mean(sample > 0)\n",
    "    \n",
    "    return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL VALIDATION & ROBUSTNESS TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Bootstrap analysis\n",
    "print(\"\\nRunning bootstrap analysis (10,000 iterations)...\")\n",
    "boot_start = time.time()\n",
    "\n",
    "returns_array = returns.values\n",
    "boot_returns, boot_sharpes, boot_max_dd, boot_win_rates = bootstrap_confidence_intervals(returns_array)\n",
    "\n",
    "boot_time = time.time() - boot_start\n",
    "print(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n",
    "\n",
    "# Calculate confidence intervals\n",
    "def calculate_ci(data, confidence=0.95):\n",
    "    lower = np.percentile(data, (1 - confidence) / 2 * 100)\n",
    "    upper = np.percentile(data, (1 + confidence) / 2 * 100)\n",
    "    return lower, upper\n",
    "\n",
    "# Display results\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "ret_lower, ret_upper = calculate_ci(boot_returns)\n",
    "print(f\"Total Return: [{ret_lower*100:.2f}%, {ret_upper*100:.2f}%]\")\n",
    "\n",
    "sharpe_lower, sharpe_upper = calculate_ci(boot_sharpes)\n",
    "print(f\"Sharpe Ratio: [{sharpe_lower:.2f}, {sharpe_upper:.2f}]\")\n",
    "\n",
    "dd_lower, dd_upper = calculate_ci(boot_max_dd)\n",
    "print(f\"Max Drawdown: [{dd_lower*100:.2f}%, {dd_upper*100:.2f}%]\")\n",
    "\n",
    "wr_lower, wr_upper = calculate_ci(boot_win_rates)\n",
    "print(f\"Win Rate: [{wr_lower*100:.2f}%, {wr_upper*100:.2f}%]\")\n",
    "\n",
    "# Statistical significance tests\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"STATISTICAL SIGNIFICANCE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test if returns are significantly different from zero\n",
    "t_stat = np.mean(returns_array[~np.isnan(returns_array)]) / (np.std(returns_array[~np.isnan(returns_array)]) / np.sqrt(len(returns_array[~np.isnan(returns_array)])))\n",
    "p_value = 2 * (1 - scipy.stats.norm.cdf(abs(t_stat))) if 'scipy' in globals() else 0.05\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"Returns significantly positive: {'Yes' if t_stat > 1.96 else 'No'}\")\n",
    "\n",
    "# Risk-adjusted performance percentiles\n",
    "actual_sharpe = stats['Sharpe Ratio']\n",
    "sharpe_percentile = np.sum(boot_sharpes <= actual_sharpe) / len(boot_sharpes) * 100\n",
    "\n",
    "print(f\"\\nStrategy Sharpe ratio percentile: {sharpe_percentile:.1f}%\")\n",
    "print(f\"Performance assessment: \", end=\"\")\n",
    "if sharpe_percentile > 90:\n",
    "    print(\"EXCELLENT - Top 10% performance\")\n",
    "elif sharpe_percentile > 75:\n",
    "    print(\"VERY GOOD - Top 25% performance\")\n",
    "elif sharpe_percentile > 50:\n",
    "    print(\"GOOD - Above median performance\")\n",
    "else:\n",
    "    print(\"NEEDS IMPROVEMENT - Below median performance\")\n",
    "\n",
    "# Stability analysis\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"STABILITY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Rolling performance\n",
    "window = 252 * 5  # 1 year of 5-minute bars\n",
    "rolling_returns = returns.rolling(window).apply(lambda x: (1 + x).prod() - 1)\n",
    "rolling_sharpe = returns.rolling(window).apply(lambda x: x.mean() / x.std() * np.sqrt(252 * 78) if x.std() > 0 else 0)\n",
    "\n",
    "print(f\"Rolling 1-year return volatility: {rolling_returns.std()*100:.2f}%\")\n",
    "print(f\"Rolling Sharpe stability: {rolling_sharpe.std():.2f}\")\n",
    "print(f\"Minimum rolling Sharpe: {rolling_sharpe.min():.2f}\")\n",
    "print(f\"Maximum rolling Sharpe: {rolling_sharpe.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Final Summary and Recommendations\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY - MLMI → NW-RQK → FVG SYNERGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nPERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "print(f\"Total Trades: {stats['Total Trades']:,.0f}\")\n",
    "print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "print(f\"Average Trade: {stats['Expectancy [%]']:.3f}%\")\n",
    "\n",
    "# Execution summary\n",
    "total_time = calc_time + mlmi_time + nwrqk_time + align_time + signal_time + backtest_time + boot_time\n",
    "print(\"\\nEXECUTION PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "print(f\"Bars processed per second: {len(df_5m_aligned) / total_time:,.0f}\")\n",
    "print(f\"Signals detected per second: {(long_entries.sum() + short_entries.sum()) / signal_time:,.0f}\")\n",
    "\n",
    "# Signal analysis\n",
    "print(\"\\nSIGNAL CHARACTERISTICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Base indicators (30m):\")\n",
    "print(f\"  - MLMI signals: {df_30m['mlmi_bull'].sum() + df_30m['mlmi_bear'].sum():,}\")\n",
    "print(f\"  - NW-RQK signals: {df_30m['nwrqk_bull'].sum() + df_30m['nwrqk_bear'].sum():,}\")\n",
    "print(f\"FVG zones (5m): {fvg_bull.sum() + fvg_bear.sum():,}\")\n",
    "print(f\"\\nSynergy signals: {long_entries.sum() + short_entries.sum():,}\")\n",
    "print(f\"Signal reduction: {((1 - (long_entries.sum() + short_entries.sum()) / (df_30m['mlmi_bull'].sum() + df_30m['mlmi_bear'].sum())) * 100):.1f}%\")\n",
    "\n",
    "# Strengths and weaknesses\n",
    "print(\"\\nKEY STRENGTHS:\")\n",
    "print(\"-\" * 50)\n",
    "strengths = []\n",
    "if stats['Sharpe Ratio'] > 1.0:\n",
    "    strengths.append(f\"Strong risk-adjusted returns (Sharpe: {stats['Sharpe Ratio']:.2f})\")\n",
    "if stats['Win Rate [%]'] > 45:\n",
    "    strengths.append(f\"Solid win rate ({stats['Win Rate [%]']:.1f}%)\")\n",
    "if stats['Total Trades'] > 1000:\n",
    "    strengths.append(f\"Good trade frequency ({stats['Total Trades']:,.0f} trades)\")\n",
    "if abs(stats['Max Drawdown [%]']) < 20:\n",
    "    strengths.append(f\"Controlled drawdown ({stats['Max Drawdown [%]']:.1f}%)\")\n",
    "if total_time < 10:\n",
    "    strengths.append(f\"Ultra-fast execution ({total_time:.1f} seconds)\")\n",
    "\n",
    "for i, strength in enumerate(strengths, 1):\n",
    "    print(f\"{i}. {strength}\")\n",
    "\n",
    "print(\"\\nAREAS FOR IMPROVEMENT:\")\n",
    "print(\"-\" * 50)\n",
    "improvements = []\n",
    "if stats['Sharpe Ratio'] < 0.5:\n",
    "    improvements.append(\"Improve risk-adjusted returns\")\n",
    "if stats['Win Rate [%]'] < 40:\n",
    "    improvements.append(\"Increase win rate through better entry timing\")\n",
    "if stats['Total Trades'] < 500:\n",
    "    improvements.append(\"Consider relaxing signal criteria for more opportunities\")\n",
    "if abs(stats['Max Drawdown [%]']) > 30:\n",
    "    improvements.append(\"Implement better risk management to reduce drawdowns\")\n",
    "\n",
    "for i, improvement in enumerate(improvements, 1):\n",
    "    print(f\"{i}. {improvement}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Parameter optimization:\")\n",
    "print(\"   - Test different MLMI k-neighbors (100-300)\")\n",
    "print(\"   - Optimize NW-RQK kernel parameters (h: 5-15, r: 5-15)\")\n",
    "print(\"   - Adjust FVG ATR multiplier (1.0-2.0)\")\n",
    "print(\"\\n2. Risk management enhancements:\")\n",
    "print(\"   - Implement dynamic position sizing based on volatility\")\n",
    "print(\"   - Add trailing stops for trend-following trades\")\n",
    "print(\"   - Consider correlation-based portfolio allocation\")\n",
    "print(\"\\n3. Further testing:\")\n",
    "print(\"   - Walk-forward analysis across different market regimes\")\n",
    "print(\"   - Out-of-sample testing on different assets\")\n",
    "print(\"   - Stress testing during high volatility periods\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}