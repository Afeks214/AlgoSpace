{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 2: MLMI → NW-RQK → FVG Trading Strategy\n",
    "\n",
    "**Ultra-Fast Backtesting with VectorBT and Numba JIT Compilation**\n",
    "\n",
    "This notebook implements the second synergy pattern where:\n",
    "1. MLMI provides the primary trend signal\n",
    "2. NW-RQK confirms the trend direction\n",
    "3. FVG validates the final entry zone\n",
    "\n",
    "Key differences from Synergy 1:\n",
    "- NW-RQK confirmation comes before FVG\n",
    "- May capture different market dynamics\n",
    "- Expected to generate similar trade counts but with different timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Environment Setup and Imports\n\n# Standard library imports\nimport os\nimport sys\nimport gc\nimport json\nimport time\nimport logging\nimport warnings\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Tuple, Dict as TypeDict, Optional, List, Union, Any\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\nimport pickle\n\n# Scientific computing imports\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.spatial import cKDTree\n\n# Visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Trading and backtesting imports\nimport vectorbt as vbt\n\n# Performance optimization imports\nfrom numba import njit, prange, typed, types\nfrom numba.typed import Dict\nimport numba\n\n# Progress tracking\nfrom tqdm import tqdm\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout),\n        logging.FileHandler('synergy_strategy.log')\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Configure Numba for maximum performance\nnumba.config.THREADING_LAYER = 'threadsafe'\nnumba.config.NUMBA_NUM_THREADS = numba.config.NUMBA_DEFAULT_NUM_THREADS\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_rows', 100)\n\n# Version checks\nlogger.info(\"Environment Setup\")\nlogger.info(f\"Python version: {sys.version}\")\nlogger.info(f\"NumPy version: {np.__version__}\")\nlogger.info(f\"Pandas version: {pd.__version__}\")\nlogger.info(f\"VectorBT version: {vbt.__version__}\")\nlogger.info(f\"Numba version: {numba.__version__}\")\nlogger.info(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\n\nprint(\"Synergy 2: MLMI → NW-RQK → FVG Strategy\")\nprint(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\nprint(f\"VectorBT version: {vbt.__version__}\")\nprint(\"Environment ready for ultra-fast backtesting!\")\n\n# Configuration dataclass\n@dataclass\nclass StrategyConfig:\n    \"\"\"Configuration for the trading strategy\"\"\"\n    # Data paths\n    data_path_5m: str = \"/home/QuantNova/AlgoSpace-Strategy-1/@NQ - 5 min - ETH.csv\"\n    data_path_30m: str = \"/home/QuantNova/AlgoSpace-Strategy-1/NQ - 30 min - ETH.csv\"\n    \n    # MLMI parameters\n    mlmi_k_neighbors: int = 200\n    mlmi_confidence_threshold: float = 0.3\n    mlmi_forward_bars: int = 5\n    \n    # NW-RQK parameters\n    nwrqk_h: float = 8.0\n    nwrqk_r: float = 8.0\n    nwrqk_lag: int = 2\n    nwrqk_strength_threshold: float = 0.2\n    \n    # FVG parameters\n    fvg_atr_multiplier: float = 1.5\n    fvg_active_bars: int = 20\n    \n    # Signal parameters\n    synergy_window: int = 30\n    \n    # Backtesting parameters\n    initial_capital: float = 100000\n    position_size_base: float = 100\n    stop_loss_atr: float = 2.0\n    max_holding_bars: int = 100\n    fees: float = 0.0001\n    slippage: float = 0.0001\n    \n    # Performance parameters\n    chunk_size: int = 10000\n    max_memory_gb: float = 8.0\n    \n    # Output parameters\n    save_results: bool = True\n    results_path: str = \"./results\"\n    checkpoint_interval: int = 1000\n\n# Create default configuration\nconfig = StrategyConfig()\n\n# Memory management utilities\ndef check_memory_usage():\n    \"\"\"Check current memory usage\"\"\"\n    try:\n        import psutil\n        process = psutil.Process(os.getpid())\n        mem_gb = process.memory_info().rss / 1024 / 1024 / 1024\n        return mem_gb\n    except ImportError:\n        logger.warning(\"psutil not installed, memory monitoring disabled\")\n        return 0.0\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection\"\"\"\n    gc.collect()\n    logger.info(f\"Memory after cleanup: {check_memory_usage():.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Data Loading with Robust Error Handling\n\nclass DataLoadingError(Exception):\n    \"\"\"Custom exception for data loading errors\"\"\"\n    pass\n\nclass DataValidationError(Exception):\n    \"\"\"Custom exception for data validation errors\"\"\"\n    pass\n\ndef validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> None:\n    \"\"\"Validate dataframe has required columns and valid data\"\"\"\n    # Check for required columns\n    missing_columns = set(required_columns) - set(df.columns)\n    if missing_columns:\n        raise DataValidationError(f\"Missing required columns: {missing_columns}\")\n    \n    # Check for empty dataframe\n    if len(df) == 0:\n        raise DataValidationError(\"Dataframe is empty\")\n    \n    # Check for sufficient data\n    if len(df) < 100:\n        logger.warning(f\"Limited data: only {len(df)} rows available\")\n    \n    # Check for NaN values in critical columns\n    critical_columns = ['Open', 'High', 'Low', 'Close']\n    nan_counts = df[critical_columns].isna().sum()\n    if nan_counts.any():\n        logger.warning(f\"NaN values found: {nan_counts.to_dict()}\")\n\ndef load_data_optimized(file_path: str, timeframe: str = '5m', \n                       config: Optional[StrategyConfig] = None) -> pd.DataFrame:\n    \"\"\"Load and prepare data with comprehensive error handling\"\"\"\n    start_time = time.time()\n    logger.info(f\"Loading {timeframe} data from {file_path}\")\n    \n    try:\n        # Check if file exists\n        if not os.path.exists(file_path):\n            raise DataLoadingError(f\"Data file not found: {file_path}\")\n        \n        # Check file size\n        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n        if file_size_mb > 1000:\n            logger.warning(f\"Large file detected: {file_size_mb:.1f} MB\")\n        \n        # Read CSV with optimized settings\n        df = pd.read_csv(\n            file_path,\n            parse_dates=['Timestamp'],\n            infer_datetime_format=True,\n            date_parser=lambda x: pd.to_datetime(x, dayfirst=True, errors='coerce'),\n            index_col='Timestamp',\n            low_memory=False\n        )\n        \n        # Validate required columns\n        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n        validate_dataframe(df, required_columns)\n        \n        # Ensure numeric types for fast operations\n        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n        for col in numeric_cols:\n            if col in df.columns:\n                df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float64)\n        \n        # Remove any rows with invalid timestamps\n        df = df[df.index.notnull()]\n        \n        # Remove any NaN values in OHLC\n        initial_len = len(df)\n        df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n        if len(df) < initial_len:\n            logger.warning(f\"Dropped {initial_len - len(df)} rows with NaN values\")\n        \n        # Validate OHLC relationships\n        invalid_candles = (\n            (df['High'] < df['Low']) |\n            (df['High'] < df['Open']) |\n            (df['High'] < df['Close']) |\n            (df['Low'] > df['Open']) |\n            (df['Low'] > df['Close'])\n        )\n        if invalid_candles.any():\n            logger.warning(f\"Found {invalid_candles.sum()} invalid candles, fixing...\")\n            df.loc[invalid_candles, 'High'] = df.loc[invalid_candles, ['Open', 'Close', 'High']].max(axis=1)\n            df.loc[invalid_candles, 'Low'] = df.loc[invalid_candles, ['Open', 'Close', 'Low']].min(axis=1)\n        \n        # Sort index for faster operations\n        df.sort_index(inplace=True)\n        \n        # Check for duplicate timestamps\n        duplicates = df.index.duplicated()\n        if duplicates.any():\n            logger.warning(f\"Found {duplicates.sum()} duplicate timestamps, keeping first\")\n            df = df[~duplicates]\n        \n        # Pre-calculate commonly used features with safe operations\n        df['Returns'] = df['Close'].pct_change().fillna(0)\n        df['LogReturns'] = np.where(\n            df['Close'].shift(1) > 0,\n            np.log(df['Close'] / df['Close'].shift(1)),\n            0\n        )\n        df['HL_Range'] = df['High'] - df['Low']\n        df['OC_Range'] = abs(df['Open'] - df['Close'])\n        \n        # Add data quality metrics\n        df['DataQuality'] = 1.0\n        df.loc[df['Volume'] == 0, 'DataQuality'] *= 0.8\n        df.loc[df['HL_Range'] == 0, 'DataQuality'] *= 0.9\n        \n        load_time = time.time() - start_time\n        logger.info(f\"Successfully loaded {len(df):,} rows in {load_time:.2f} seconds\")\n        logger.info(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n        logger.info(f\"Average data quality: {df['DataQuality'].mean():.3f}\")\n        \n        # Memory optimization\n        df = df.astype({col: 'float32' for col in numeric_cols if col in df.columns})\n        \n        return df\n        \n    except pd.errors.ParserError as e:\n        raise DataLoadingError(f\"Failed to parse CSV file: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error loading data: {str(e)}\")\n        raise DataLoadingError(f\"Failed to load data: {str(e)}\")\n\n# Pre-compile all Numba functions\nprint(\"Pre-compiling Numba functions for maximum speed...\")\n\n@njit(cache=True)\ndef dummy_compile():\n    \"\"\"Dummy function to trigger compilation\"\"\"\n    return np.array([1.0, 2.0, 3.0]).sum()\n\n_ = dummy_compile()  # Trigger compilation\n\n# Load data files with error handling\nprint(\"\\nLoading data files...\")\n\ntry:\n    # Check if config paths exist, otherwise try alternative paths\n    data_paths_5m = [\n        config.data_path_5m,\n        \"./data/@NQ - 5 min - ETH.csv\",\n        \"../data/@NQ - 5 min - ETH.csv\",\n        \"data/@NQ - 5 min - ETH.csv\"\n    ]\n    \n    data_paths_30m = [\n        config.data_path_30m,\n        \"./data/NQ - 30 min - ETH.csv\",\n        \"../data/NQ - 30 min - ETH.csv\",\n        \"data/NQ - 30 min - ETH.csv\"\n    ]\n    \n    # Try to load 5m data\n    df_5m = None\n    for path in data_paths_5m:\n        if os.path.exists(path):\n            df_5m = load_data_optimized(path, '5m', config)\n            break\n    \n    if df_5m is None:\n        raise DataLoadingError(f\"Could not find 5m data file. Tried: {data_paths_5m}\")\n    \n    # Try to load 30m data\n    df_30m = None\n    for path in data_paths_30m:\n        if os.path.exists(path):\n            df_30m = load_data_optimized(path, '30m', config)\n            break\n    \n    if df_30m is None:\n        raise DataLoadingError(f\"Could not find 30m data file. Tried: {data_paths_30m}\")\n    \n    # Ensure time alignment\n    common_start = max(df_5m.index[0], df_30m.index[0])\n    common_end = min(df_5m.index[-1], df_30m.index[-1])\n    \n    df_5m = df_5m.loc[common_start:common_end]\n    df_30m = df_30m.loc[common_start:common_end]\n    \n    print(f\"\\n5-minute data: {df_5m.index[0]} to {df_5m.index[-1]} ({len(df_5m):,} bars)\")\n    print(f\"30-minute data: {df_30m.index[0]} to {df_30m.index[-1]} ({len(df_30m):,} bars)\")\n    print(f\"Memory usage: {check_memory_usage():.2f} GB\")\n    \n    # Save checkpoint\n    if config.save_results:\n        os.makedirs(config.results_path, exist_ok=True)\n        checkpoint_path = os.path.join(config.results_path, 'data_checkpoint.pkl')\n        with open(checkpoint_path, 'wb') as f:\n            pickle.dump({\n                'df_5m_shape': df_5m.shape,\n                'df_30m_shape': df_30m.shape,\n                'date_range': (df_5m.index[0], df_5m.index[-1])\n            }, f)\n        logger.info(f\"Saved data checkpoint to {checkpoint_path}\")\n    \nexcept DataLoadingError as e:\n    logger.error(f\"Data loading failed: {e}\")\n    print(f\"\\nERROR: {e}\")\n    print(\"\\nPlease ensure data files are in the correct location.\")\n    print(\"You can update the file paths in the StrategyConfig class in Cell 1.\")\n    raise\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Optimized Indicator Suite with Robust Error Handling\n\n@njit(fastmath=True, cache=True, parallel=True)\ndef calculate_all_indicators(close: np.ndarray, high: np.ndarray, low: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate all basic indicators with comprehensive error handling\"\"\"\n    n = len(close)\n    \n    # Pre-allocate arrays with default values\n    ma5 = np.full(n, np.nan, dtype=np.float64)\n    ma20 = np.full(n, np.nan, dtype=np.float64)\n    rsi5 = np.full(n, 50.0, dtype=np.float64)\n    rsi20 = np.full(n, 50.0, dtype=np.float64)\n    atr = np.full(n, np.nan, dtype=np.float64)\n    \n    # Input validation\n    if n == 0:\n        return ma5, ma20, rsi5, rsi20, atr\n    \n    # Weighted Moving Averages with safe calculations\n    weights5 = np.arange(1, 6, dtype=np.float64)\n    weights20 = np.arange(1, 21, dtype=np.float64)\n    sum_w5 = weights5.sum()\n    sum_w20 = weights20.sum()\n    \n    # Calculate WMAs in parallel chunks with bounds checking\n    for i in prange(n):\n        # 5-period WMA\n        if i >= 4:\n            window_data = close[i-4:i+1]\n            if not np.any(np.isnan(window_data)):\n                ma5[i] = np.dot(window_data, weights5) / sum_w5\n        \n        # 20-period WMA\n        if i >= 19:\n            window_data = close[i-19:i+1]\n            if not np.any(np.isnan(window_data)):\n                ma20[i] = np.dot(window_data, weights20) / sum_w20\n    \n    # RSI calculation with safe division\n    if n > 1:\n        deltas = np.diff(close)\n        gains = np.maximum(deltas, 0)\n        losses = -np.minimum(deltas, 0)\n        \n        # RSI 5\n        if len(gains) >= 5:\n            avg_gain5 = np.mean(gains[:5])\n            avg_loss5 = np.mean(losses[:5])\n            \n            if avg_loss5 > 0:\n                rs5 = avg_gain5 / avg_loss5\n                rsi5[5] = 100 - (100 / (1 + rs5))\n            else:\n                rsi5[5] = 100 if avg_gain5 > 0 else 50\n            \n            # Calculate remaining RSI values\n            for i in range(5, min(n - 1, len(gains))):\n                avg_gain5 = (avg_gain5 * 4 + gains[i]) / 5\n                avg_loss5 = (avg_loss5 * 4 + losses[i]) / 5\n                \n                if avg_loss5 > 0:\n                    rs5 = avg_gain5 / avg_loss5\n                    rsi5[i + 1] = 100 - (100 / (1 + rs5))\n                else:\n                    rsi5[i + 1] = 100 if avg_gain5 > 0 else 50\n        \n        # RSI 20\n        if len(gains) >= 20:\n            avg_gain20 = np.mean(gains[:20])\n            avg_loss20 = np.mean(losses[:20])\n            \n            if avg_loss20 > 0:\n                rs20 = avg_gain20 / avg_loss20\n                rsi20[20] = 100 - (100 / (1 + rs20))\n            else:\n                rsi20[20] = 100 if avg_gain20 > 0 else 50\n            \n            # Calculate remaining RSI values\n            for i in range(20, min(n - 1, len(gains))):\n                avg_gain20 = (avg_gain20 * 19 + gains[i]) / 20\n                avg_loss20 = (avg_loss20 * 19 + losses[i]) / 20\n                \n                if avg_loss20 > 0:\n                    rs20 = avg_gain20 / avg_loss20\n                    rsi20[i + 1] = 100 - (100 / (1 + rs20))\n                else:\n                    rsi20[i + 1] = 100 if avg_gain20 > 0 else 50\n    \n    # ATR calculation with safe operations\n    if n > 1:\n        # Calculate true range\n        tr = np.zeros(n, dtype=np.float64)\n        tr[0] = high[0] - low[0] if not np.isnan(high[0]) and not np.isnan(low[0]) else 0\n        \n        for i in range(1, n):\n            if not np.isnan(high[i]) and not np.isnan(low[i]) and not np.isnan(close[i-1]):\n                hl = high[i] - low[i]\n                hc = abs(high[i] - close[i-1])\n                lc = abs(low[i] - close[i-1])\n                tr[i] = max(hl, hc, lc)\n            else:\n                tr[i] = 0\n        \n        # Calculate ATR\n        for i in range(14, n):\n            window = tr[i-13:i+1]\n            valid_values = window[window > 0]\n            if len(valid_values) > 0:\n                atr[i] = np.mean(valid_values)\n    \n    return ma5, ma20, rsi5, rsi20, atr\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef detect_fvg_optimized(high: np.ndarray, low: np.ndarray, atr: np.ndarray,\n                        multiplier: float = 1.5, min_gap_pct: float = 0.001,\n                        active_bars: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Optimized FVG detection with ATR filtering and safety checks\"\"\"\n    n = len(high)\n    bull_active = np.zeros(n, dtype=np.bool_)\n    bear_active = np.zeros(n, dtype=np.bool_)\n    \n    if n < 4:  # Need at least 4 bars for FVG\n        return bull_active, bear_active\n    \n    for i in prange(3, n):\n        # Skip if ATR is invalid\n        if np.isnan(atr[i]) or atr[i] <= 0:\n            continue\n        \n        # Dynamic gap threshold based on ATR\n        gap_threshold = max(atr[i] * multiplier, low[i] * min_gap_pct)\n        \n        # Bullish FVG with safety checks\n        if (not np.isnan(low[i]) and not np.isnan(high[i-3]) and \n            low[i] > high[i-3]):\n            gap_size = low[i] - high[i-3]\n            if gap_size > gap_threshold:\n                # Mark active zone\n                for j in range(i, min(i + active_bars, n)):\n                    if not np.isnan(low[j]) and low[j] >= high[i-3]:\n                        bull_active[j] = True\n                    else:\n                        break\n        \n        # Bearish FVG with safety checks\n        if (not np.isnan(high[i]) and not np.isnan(low[i-3]) and \n            high[i] < low[i-3]):\n            gap_size = low[i-3] - high[i]\n            if gap_size > gap_threshold:\n                # Mark active zone\n                for j in range(i, min(i + active_bars, n)):\n                    if not np.isnan(high[j]) and high[j] <= low[i-3]:\n                        bear_active[j] = True\n                    else:\n                        break\n    \n    return bull_active, bear_active\n\n# Safe smoothing function\ndef safe_smooth(data: np.ndarray, window: int = 20) -> np.ndarray:\n    \"\"\"Apply smoothing with NaN handling\"\"\"\n    if len(data) < window:\n        return data\n    \n    # Replace NaN with forward fill for smoothing\n    filled_data = pd.Series(data).fillna(method='ffill').fillna(method='bfill').values\n    \n    # Apply convolution\n    kernel = np.ones(window) / window\n    smoothed = np.convolve(filled_data, kernel, mode='same')\n    \n    # Restore NaN where original data had NaN\n    smoothed[np.isnan(data)] = np.nan\n    \n    return smoothed\n\nprint(\"\\nCalculating all indicators with parallel processing...\")\nlogger.info(\"Starting indicator calculations\")\nstart_time = time.time()\n\ntry:\n    # Calculate 30-minute indicators\n    close_30m = df_30m['Close'].values.astype(np.float64)\n    high_30m = df_30m['High'].values.astype(np.float64)\n    low_30m = df_30m['Low'].values.astype(np.float64)\n    \n    ma5, ma20, rsi5, rsi20, atr_30m = calculate_all_indicators(\n        close_30m, high_30m, low_30m\n    )\n    \n    # Smooth RSI with safety\n    rsi5_smooth = safe_smooth(rsi5, 20)\n    rsi20_smooth = safe_smooth(rsi20, 20)\n    \n    # Calculate 5-minute indicators\n    close_5m = df_5m['Close'].values.astype(np.float64)\n    high_5m = df_5m['High'].values.astype(np.float64)\n    low_5m = df_5m['Low'].values.astype(np.float64)\n    \n    _, _, _, _, atr_5m = calculate_all_indicators(\n        close_5m, high_5m, low_5m\n    )\n    \n    # Detect FVG with ATR filter\n    fvg_bull, fvg_bear = detect_fvg_optimized(\n        high_5m, low_5m, atr_5m, \n        config.fvg_atr_multiplier,\n        0.001,\n        config.fvg_active_bars\n    )\n    \n    calc_time = time.time() - start_time\n    \n    # Log statistics\n    logger.info(f\"Indicators calculated in {calc_time:.3f} seconds\")\n    logger.info(f\"MA5 valid values: {(~np.isnan(ma5)).sum()}/{len(ma5)}\")\n    logger.info(f\"MA20 valid values: {(~np.isnan(ma20)).sum()}/{len(ma20)}\")\n    logger.info(f\"RSI5 range: [{np.nanmin(rsi5):.1f}, {np.nanmax(rsi5):.1f}]\")\n    logger.info(f\"RSI20 range: [{np.nanmin(rsi20):.1f}, {np.nanmax(rsi20):.1f}]\")\n    logger.info(f\"ATR 30m valid: {(~np.isnan(atr_30m)).sum()}/{len(atr_30m)}\")\n    logger.info(f\"ATR 5m valid: {(~np.isnan(atr_5m)).sum()}/{len(atr_5m)}\")\n    logger.info(f\"FVG zones - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")\n    \n    print(f\"All indicators calculated in {calc_time:.3f} seconds\")\n    print(f\"FVG zones detected - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")\n    \n    # Memory cleanup\n    if check_memory_usage() > config.max_memory_gb * 0.8:\n        cleanup_memory()\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating indicators: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Advanced MLMI with Adaptive KNN\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def adaptive_knn_predict(features: np.ndarray, labels: np.ndarray, query: np.ndarray,\n",
    "                        k_base: int, volatility: float, size: int) -> Tuple[float, float]:\n",
    "    \"\"\"Adaptive KNN that adjusts K based on market volatility\"\"\"\n",
    "    if size == 0:\n",
    "        return 0.0, 0.5\n",
    "    \n",
    "    # Adjust K based on volatility\n",
    "    k = max(3, min(k_base, int(k_base * (1 - volatility * 2))))\n",
    "    k = min(k, size)\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        dist = 0.0\n",
    "        for j in range(2):\n",
    "            diff = features[i, j] - query[j]\n",
    "            dist += diff * diff\n",
    "        distances[i] = np.sqrt(dist)\n",
    "    \n",
    "    # Find k nearest neighbors\n",
    "    indices = np.argpartition(distances, k-1)[:k]\n",
    "    \n",
    "    # Weighted voting based on distance\n",
    "    vote = 0.0\n",
    "    weight_sum = 0.0\n",
    "    \n",
    "    for i in range(k):\n",
    "        idx = indices[i]\n",
    "        if distances[idx] > 0:\n",
    "            weight = 1.0 / distances[idx]\n",
    "            vote += labels[idx] * weight\n",
    "            weight_sum += weight\n",
    "    \n",
    "    if weight_sum > 0:\n",
    "        prediction = vote / weight_sum\n",
    "        confidence = min(abs(prediction) / k, 1.0)\n",
    "    else:\n",
    "        prediction = 0.0\n",
    "        confidence = 0.0\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def calculate_mlmi_adaptive(ma_fast: np.ndarray, ma_slow: np.ndarray,\n",
    "                           rsi_fast_smooth: np.ndarray, rsi_slow_smooth: np.ndarray,\n",
    "                           close: np.ndarray, returns: np.ndarray,\n",
    "                           k_neighbors: int = 200) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"MLMI with adaptive KNN and confidence scores\"\"\"\n",
    "    n = len(close)\n",
    "    mlmi_values = np.zeros(n)\n",
    "    mlmi_confidence = np.zeros(n)\n",
    "    \n",
    "    # Pre-allocate KNN storage\n",
    "    max_size = min(10000, n)\n",
    "    features = np.zeros((max_size, 2))\n",
    "    labels = np.zeros(max_size)\n",
    "    data_size = 0\n",
    "    \n",
    "    # Calculate rolling volatility\n",
    "    volatility = np.zeros(n)\n",
    "    for i in range(20, n):\n",
    "        volatility[i] = np.std(returns[i-20:i])\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        # Detect crossovers\n",
    "        bull_cross = ma_fast[i] > ma_slow[i] and ma_fast[i-1] <= ma_slow[i-1]\n",
    "        bear_cross = ma_fast[i] < ma_slow[i] and ma_fast[i-1] >= ma_slow[i-1]\n",
    "        \n",
    "        if (bull_cross or bear_cross) and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            # Store pattern\n",
    "            if data_size >= max_size:\n",
    "                # Keep most recent 75%\n",
    "                keep_size = int(max_size * 0.75)\n",
    "                features[:keep_size] = features[-keep_size:]\n",
    "                labels[:keep_size] = labels[-keep_size:]\n",
    "                data_size = keep_size\n",
    "            \n",
    "            features[data_size, 0] = rsi_slow_smooth[i]\n",
    "            features[data_size, 1] = rsi_fast_smooth[i]\n",
    "            \n",
    "            if i < n - 1:\n",
    "                # Multi-bar forward return for better signal\n",
    "                fwd_ret = (close[min(i+5, n-1)] - close[i]) / close[i]\n",
    "                labels[data_size] = np.sign(fwd_ret) * min(abs(fwd_ret) * 100, 1.0)\n",
    "            else:\n",
    "                labels[data_size] = 0.0\n",
    "            \n",
    "            data_size += 1\n",
    "        \n",
    "        # Make prediction\n",
    "        if data_size > 10 and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            query = np.array([rsi_slow_smooth[i], rsi_fast_smooth[i]])\n",
    "            pred, conf = adaptive_knn_predict(features, labels, query,\n",
    "                                            k_neighbors, volatility[i], data_size)\n",
    "            mlmi_values[i] = pred * 100  # Scale for visibility\n",
    "            mlmi_confidence[i] = conf\n",
    "    \n",
    "    return mlmi_values, mlmi_confidence\n",
    "\n",
    "# Calculate MLMI with confidence\n",
    "print(\"\\nCalculating adaptive MLMI with confidence scores...\")\n",
    "start_time = time.time()\n",
    "\n",
    "returns_30m = df_30m['Returns'].values\n",
    "mlmi_values, mlmi_confidence = calculate_mlmi_adaptive(\n",
    "    ma5, ma20, rsi5_smooth, rsi20_smooth, close_30m, returns_30m\n",
    ")\n",
    "\n",
    "# Store in dataframe\n",
    "df_30m['mlmi'] = mlmi_values\n",
    "df_30m['mlmi_confidence'] = mlmi_confidence\n",
    "df_30m['mlmi_bull'] = (mlmi_values > 0) & (mlmi_confidence > 0.3)\n",
    "df_30m['mlmi_bear'] = (mlmi_values < 0) & (mlmi_confidence > 0.3)\n",
    "\n",
    "mlmi_time = time.time() - start_time\n",
    "print(f\"Adaptive MLMI calculated in {mlmi_time:.3f} seconds\")\n",
    "print(f\"MLMI range: [{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")\n",
    "print(f\"Average confidence: {mlmi_confidence.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Advanced MLMI with Adaptive KNN and Error Handling\n\n@njit(fastmath=True, cache=True)\ndef adaptive_knn_predict(features: np.ndarray, labels: np.ndarray, query: np.ndarray,\n                        k_base: int, volatility: float, size: int) -> Tuple[float, float]:\n    \"\"\"Adaptive KNN with safe distance calculations\"\"\"\n    if size == 0:\n        return 0.0, 0.5\n    \n    # Adjust K based on volatility with bounds checking\n    vol_factor = max(0, min(1, 1 - volatility * 2))\n    k = max(3, min(k_base, int(k_base * vol_factor)))\n    k = min(k, size)\n    \n    # Calculate distances with numerical stability\n    distances = np.zeros(size)\n    for i in range(size):\n        dist = 0.0\n        for j in range(min(2, features.shape[1])):  # Ensure we don't exceed feature dimensions\n            diff = features[i, j] - query[j]\n            dist += diff * diff\n        distances[i] = np.sqrt(max(0, dist))  # Ensure non-negative\n    \n    # Find k nearest neighbors\n    if k <= size:\n        indices = np.argpartition(distances, k-1)[:k]\n    else:\n        indices = np.arange(size)\n    \n    # Weighted voting based on distance\n    vote = 0.0\n    weight_sum = 0.0\n    \n    for i in range(len(indices)):\n        idx = indices[i]\n        if distances[idx] > 1e-10:  # Avoid division by very small numbers\n            weight = 1.0 / distances[idx]\n            vote += labels[idx] * weight\n            weight_sum += weight\n        else:\n            # Handle exact matches\n            vote += labels[idx] * 100\n            weight_sum += 100\n    \n    if weight_sum > 1e-10:\n        prediction = vote / weight_sum\n        confidence = min(abs(prediction) / max(1, k), 1.0)\n    else:\n        prediction = 0.0\n        confidence = 0.0\n    \n    return prediction, confidence\n\n@njit(fastmath=True, cache=True)\ndef calculate_mlmi_adaptive(ma_fast: np.ndarray, ma_slow: np.ndarray,\n                           rsi_fast_smooth: np.ndarray, rsi_slow_smooth: np.ndarray,\n                           close: np.ndarray, returns: np.ndarray,\n                           k_neighbors: int = 200,\n                           min_confidence: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"MLMI with comprehensive error handling\"\"\"\n    n = len(close)\n    mlmi_values = np.zeros(n)\n    mlmi_confidence = np.zeros(n)\n    \n    # Input validation\n    if n < 10:\n        return mlmi_values, mlmi_confidence\n    \n    # Pre-allocate KNN storage\n    max_size = min(10000, n)\n    features = np.zeros((max_size, 2))\n    labels = np.zeros(max_size)\n    data_size = 0\n    \n    # Calculate rolling volatility with safe operations\n    volatility = np.zeros(n)\n    for i in range(20, n):\n        window_returns = returns[max(0, i-20):i]\n        valid_returns = window_returns[~np.isnan(window_returns)]\n        if len(valid_returns) > 1:\n            volatility[i] = np.std(valid_returns)\n        else:\n            volatility[i] = 0.01  # Default volatility\n    \n    for i in range(1, n):\n        # Detect crossovers with NaN checks\n        if (np.isnan(ma_fast[i]) or np.isnan(ma_slow[i]) or \n            np.isnan(ma_fast[i-1]) or np.isnan(ma_slow[i-1])):\n            continue\n            \n        bull_cross = ma_fast[i] > ma_slow[i] and ma_fast[i-1] <= ma_slow[i-1]\n        bear_cross = ma_fast[i] < ma_slow[i] and ma_fast[i-1] >= ma_slow[i-1]\n        \n        if (bull_cross or bear_cross) and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n            # Store pattern\n            if data_size >= max_size:\n                # Keep most recent 75%\n                keep_size = int(max_size * 0.75)\n                features[:keep_size] = features[-keep_size:]\n                labels[:keep_size] = labels[-keep_size:]\n                data_size = keep_size\n            \n            features[data_size, 0] = rsi_slow_smooth[i]\n            features[data_size, 1] = rsi_fast_smooth[i]\n            \n            if i < n - 5:  # Ensure we have forward data\n                # Multi-bar forward return with safety\n                fwd_idx = min(i + 5, n - 1)\n                if close[i] > 0:\n                    fwd_ret = (close[fwd_idx] - close[i]) / close[i]\n                    # Clip extreme values\n                    fwd_ret = max(-0.1, min(0.1, fwd_ret))\n                    labels[data_size] = np.sign(fwd_ret) * min(abs(fwd_ret) * 100, 1.0)\n                else:\n                    labels[data_size] = 0.0\n            else:\n                labels[data_size] = 0.0\n            \n            data_size += 1\n        \n        # Make prediction\n        if (data_size > 10 and not np.isnan(rsi_fast_smooth[i]) and \n            not np.isnan(rsi_slow_smooth[i])):\n            query = np.array([rsi_slow_smooth[i], rsi_fast_smooth[i]])\n            pred, conf = adaptive_knn_predict(\n                features[:data_size, :], \n                labels[:data_size], \n                query,\n                k_neighbors, \n                volatility[i], \n                data_size\n            )\n            \n            # Apply confidence threshold\n            if conf >= min_confidence:\n                mlmi_values[i] = pred * 100  # Scale for visibility\n                mlmi_confidence[i] = conf\n            else:\n                mlmi_values[i] = 0.0\n                mlmi_confidence[i] = 0.0\n    \n    return mlmi_values, mlmi_confidence\n\n# Calculate MLMI with confidence\nprint(\"\\nCalculating adaptive MLMI with confidence scores...\")\nlogger.info(\"Starting MLMI calculation\")\nstart_time = time.time()\n\ntry:\n    # Ensure we have valid data\n    if 'Returns' not in df_30m.columns:\n        df_30m['Returns'] = df_30m['Close'].pct_change().fillna(0)\n    \n    returns_30m = df_30m['Returns'].values\n    \n    # Calculate MLMI\n    mlmi_values, mlmi_confidence = calculate_mlmi_adaptive(\n        ma5, ma20, rsi5_smooth, rsi20_smooth, close_30m, returns_30m,\n        config.mlmi_k_neighbors, config.mlmi_confidence_threshold\n    )\n    \n    # Store in dataframe with validation\n    df_30m['mlmi'] = mlmi_values\n    df_30m['mlmi_confidence'] = mlmi_confidence\n    df_30m['mlmi_bull'] = (mlmi_values > 0) & (mlmi_confidence > config.mlmi_confidence_threshold)\n    df_30m['mlmi_bear'] = (mlmi_values < 0) & (mlmi_confidence > config.mlmi_confidence_threshold)\n    \n    mlmi_time = time.time() - start_time\n    \n    # Log statistics\n    valid_mlmi = mlmi_values[mlmi_values != 0]\n    if len(valid_mlmi) > 0:\n        logger.info(f\"MLMI calculated in {mlmi_time:.3f} seconds\")\n        logger.info(f\"MLMI range: [{valid_mlmi.min():.1f}, {valid_mlmi.max():.1f}]\")\n        logger.info(f\"Average confidence: {mlmi_confidence[mlmi_confidence > 0].mean():.3f}\")\n        logger.info(f\"Bull signals: {df_30m['mlmi_bull'].sum()}, Bear signals: {df_30m['mlmi_bear'].sum()}\")\n    \n    print(f\"Adaptive MLMI calculated in {mlmi_time:.3f} seconds\")\n    print(f\"MLMI range: [{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")\n    print(f\"Average confidence: {mlmi_confidence.mean():.3f}\")\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating MLMI: {str(e)}\")\n    # Set default values on error\n    df_30m['mlmi'] = 0\n    df_30m['mlmi_confidence'] = 0\n    df_30m['mlmi_bull'] = False\n    df_30m['mlmi_bear'] = False\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Enhanced NW-RQK with Multiple Kernels and Error Handling\n\n@njit(fastmath=True, cache=True)\ndef gaussian_kernel(x: float, h: float) -> float:\n    \"\"\"Gaussian kernel with numerical stability\"\"\"\n    if h <= 0:\n        return 0.0\n    arg = -(x * x) / (2.0 * h * h)\n    # Prevent underflow\n    if arg < -50:\n        return 0.0\n    return np.exp(arg)\n\n@njit(fastmath=True, cache=True)\ndef epanechnikov_kernel(x: float, h: float) -> float:\n    \"\"\"Epanechnikov kernel with bounds checking\"\"\"\n    if h <= 0:\n        return 0.0\n    u = x / h\n    if abs(u) <= 1:\n        return 0.75 * (1 - u * u)\n    return 0.0\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef nadaraya_watson_ensemble(prices: np.ndarray, h: float, r: float,\n                           min_periods: int = 25) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Ensemble NW regression with robust calculations\"\"\"\n    n = len(prices)\n    result_rq = np.full(n, np.nan)  # Rational Quadratic\n    result_gauss = np.full(n, np.nan)  # Gaussian\n    \n    # Input validation\n    if n < min_periods or h <= 0 or r <= 0:\n        return result_rq, result_gauss\n    \n    for i in prange(min_periods, n):\n        # Skip if price is invalid\n        if np.isnan(prices[i]):\n            continue\n            \n        # Rational Quadratic regression\n        weighted_sum_rq = 0.0\n        weight_sum_rq = 0.0\n        \n        # Gaussian regression\n        weighted_sum_gauss = 0.0\n        weight_sum_gauss = 0.0\n        \n        window_size = min(i + 1, 500)\n        \n        for j in range(window_size):\n            if i - j >= 0 and not np.isnan(prices[i - j]):\n                # Rational Quadratic with numerical stability\n                denominator = h * h * 2.0 * r\n                if denominator > 0:\n                    weight_rq = (1.0 + (j * j) / denominator) ** (-r)\n                    weighted_sum_rq += prices[i - j] * weight_rq\n                    weight_sum_rq += weight_rq\n                \n                # Gaussian\n                weight_gauss = gaussian_kernel(float(j), h)\n                if weight_gauss > 1e-10:\n                    weighted_sum_gauss += prices[i - j] * weight_gauss\n                    weight_sum_gauss += weight_gauss\n        \n        # Calculate results with minimum weight threshold\n        if weight_sum_rq > 1e-10:\n            result_rq[i] = weighted_sum_rq / weight_sum_rq\n        if weight_sum_gauss > 1e-10:\n            result_gauss[i] = weighted_sum_gauss / weight_sum_gauss\n    \n    # Ensemble: average of both kernels\n    ensemble = np.zeros(n)\n    for i in range(n):\n        count = 0\n        sum_val = 0.0\n        \n        if not np.isnan(result_rq[i]):\n            sum_val += result_rq[i]\n            count += 1\n        if not np.isnan(result_gauss[i]):\n            sum_val += result_gauss[i]\n            count += 1\n            \n        if count > 0:\n            ensemble[i] = sum_val / count\n        else:\n            ensemble[i] = np.nan\n    \n    return ensemble, result_rq\n\n@njit(fastmath=True, cache=True)\ndef detect_nwrqk_signals_enhanced(yhat1: np.ndarray, yhat2: np.ndarray,\n                                 prices: np.ndarray,\n                                 min_slope_change: float = 1e-6) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Enhanced signal detection with numerical stability\"\"\"\n    n = len(yhat1)\n    bull_signals = np.zeros(n, dtype=np.bool_)\n    bear_signals = np.zeros(n, dtype=np.bool_)\n    signal_strength = np.zeros(n)\n    \n    if n < 3:\n        return bull_signals, bear_signals, signal_strength\n    \n    for i in range(2, n):\n        if not np.isnan(yhat1[i]) and not np.isnan(yhat1[i-1]) and not np.isnan(yhat1[i-2]):\n            # Calculate slopes with safety\n            slope_prev = yhat1[i-1] - yhat1[i-2]\n            slope_curr = yhat1[i] - yhat1[i-1]\n            \n            # Acceleration\n            acceleration = slope_curr - slope_prev\n            \n            # Bullish: negative to positive slope with positive acceleration\n            if slope_prev < -min_slope_change and slope_curr > min_slope_change and acceleration > min_slope_change:\n                bull_signals[i] = True\n                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n            \n            # Bearish: positive to negative slope with negative acceleration\n            elif slope_prev > min_slope_change and slope_curr < -min_slope_change and acceleration < -min_slope_change:\n                bear_signals[i] = True\n                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n        \n        # Crossovers with momentum\n        if i > 5 and not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]):\n            if not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]) and not np.isnan(prices[i]):\n                # Price momentum filter with safety\n                if prices[max(0, i-5)] > 0:\n                    price_momentum = (prices[i] - prices[max(0, i-5)]) / prices[max(0, i-5)]\n                    price_momentum = max(-0.5, min(0.5, price_momentum))  # Clip extreme values\n                else:\n                    price_momentum = 0.0\n                \n                # Crossover detection with threshold\n                cross_threshold = abs(yhat1[i] - yhat2[i]) * 0.001\n                \n                if yhat2[i] > yhat1[i] + cross_threshold and yhat2[i-1] <= yhat1[i-1] and price_momentum > 0:\n                    bull_signals[i] = True\n                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n                elif yhat2[i] < yhat1[i] - cross_threshold and yhat2[i-1] >= yhat1[i-1] and price_momentum < 0:\n                    bear_signals[i] = True\n                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n    \n    return bull_signals, bear_signals, signal_strength\n\n# Calculate enhanced NW-RQK\nprint(\"\\nCalculating enhanced NW-RQK with ensemble kernels...\")\nlogger.info(\"Starting NW-RQK calculation\")\nstart_time = time.time()\n\ntry:\n    # Validate parameters\n    h = max(1.0, config.nwrqk_h)\n    r = max(1.0, config.nwrqk_r)\n    lag = max(1, config.nwrqk_lag)\n    \n    # Calculate regression lines\n    yhat1, yhat1_rq = nadaraya_watson_ensemble(close_30m, h, r)\n    yhat2, yhat2_rq = nadaraya_watson_ensemble(close_30m, h - lag, r)\n    \n    # Detect signals with strength\n    nwrqk_bull, nwrqk_bear, nwrqk_strength = detect_nwrqk_signals_enhanced(\n        yhat1, yhat2, close_30m\n    )\n    \n    # Store in dataframe\n    df_30m['nwrqk_bull'] = nwrqk_bull\n    df_30m['nwrqk_bear'] = nwrqk_bear\n    df_30m['nwrqk_strength'] = nwrqk_strength\n    df_30m['yhat1'] = yhat1\n    df_30m['yhat2'] = yhat2\n    \n    nwrqk_time = time.time() - start_time\n    \n    # Log statistics\n    logger.info(f\"NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n    logger.info(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")\n    \n    valid_strength = nwrqk_strength[nwrqk_strength > 0]\n    if len(valid_strength) > 0:\n        logger.info(f\"Average signal strength: {valid_strength.mean():.3f}\")\n        logger.info(f\"Max signal strength: {valid_strength.max():.3f}\")\n    \n    print(f\"Enhanced NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n    print(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")\n    print(f\"Average signal strength: {nwrqk_strength[nwrqk_strength > 0].mean():.3f}\")\n    \n    # Memory cleanup\n    if check_memory_usage() > config.max_memory_gb * 0.8:\n        cleanup_memory()\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating NW-RQK: {str(e)}\")\n    # Set default values on error\n    df_30m['nwrqk_bull'] = False\n    df_30m['nwrqk_bear'] = False\n    df_30m['nwrqk_strength'] = 0.0\n    df_30m['yhat1'] = np.nan\n    df_30m['yhat2'] = np.nan\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Enhanced NW-RQK with Multiple Kernels\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def gaussian_kernel(x: float, h: float) -> float:\n",
    "    \"\"\"Gaussian kernel function\"\"\"\n",
    "    return np.exp(-(x * x) / (2.0 * h * h))\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def epanechnikov_kernel(x: float, h: float) -> float:\n",
    "    \"\"\"Epanechnikov kernel function\"\"\"\n",
    "    u = x / h\n",
    "    if abs(u) <= 1:\n",
    "        return 0.75 * (1 - u * u)\n",
    "    return 0.0\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def nadaraya_watson_ensemble(prices: np.ndarray, h: float, r: float,\n",
    "                           min_periods: int = 25) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Ensemble NW regression with multiple kernels\"\"\"\n",
    "    n = len(prices)\n",
    "    result_rq = np.full(n, np.nan)  # Rational Quadratic\n",
    "    result_gauss = np.full(n, np.nan)  # Gaussian\n",
    "    \n",
    "    for i in prange(min_periods, n):\n",
    "        # Rational Quadratic regression\n",
    "        weighted_sum_rq = 0.0\n",
    "        weight_sum_rq = 0.0\n",
    "        \n",
    "        # Gaussian regression\n",
    "        weighted_sum_gauss = 0.0\n",
    "        weight_sum_gauss = 0.0\n",
    "        \n",
    "        window_size = min(i + 1, 500)\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            if i - j >= 0:\n",
    "                # Rational Quadratic\n",
    "                weight_rq = (1.0 + (j * j) / (h * h * 2.0 * r)) ** (-r)\n",
    "                weighted_sum_rq += prices[i - j] * weight_rq\n",
    "                weight_sum_rq += weight_rq\n",
    "                \n",
    "                # Gaussian\n",
    "                weight_gauss = gaussian_kernel(float(j), h)\n",
    "                weighted_sum_gauss += prices[i - j] * weight_gauss\n",
    "                weight_sum_gauss += weight_gauss\n",
    "        \n",
    "        if weight_sum_rq > 0:\n",
    "            result_rq[i] = weighted_sum_rq / weight_sum_rq\n",
    "        if weight_sum_gauss > 0:\n",
    "            result_gauss[i] = weighted_sum_gauss / weight_sum_gauss\n",
    "    \n",
    "    # Ensemble: average of both kernels\n",
    "    ensemble = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if not np.isnan(result_rq[i]) and not np.isnan(result_gauss[i]):\n",
    "            ensemble[i] = (result_rq[i] + result_gauss[i]) / 2\n",
    "        elif not np.isnan(result_rq[i]):\n",
    "            ensemble[i] = result_rq[i]\n",
    "        elif not np.isnan(result_gauss[i]):\n",
    "            ensemble[i] = result_gauss[i]\n",
    "        else:\n",
    "            ensemble[i] = np.nan\n",
    "    \n",
    "    return ensemble, result_rq\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def detect_nwrqk_signals_enhanced(yhat1: np.ndarray, yhat2: np.ndarray,\n",
    "                                 prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Enhanced signal detection with strength measurement\"\"\"\n",
    "    n = len(yhat1)\n",
    "    bull_signals = np.zeros(n, dtype=np.bool_)\n",
    "    bear_signals = np.zeros(n, dtype=np.bool_)\n",
    "    signal_strength = np.zeros(n)\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        if not np.isnan(yhat1[i]) and not np.isnan(yhat1[i-1]) and not np.isnan(yhat1[i-2]):\n",
    "            # Trend changes\n",
    "            slope_prev = yhat1[i-1] - yhat1[i-2]\n",
    "            slope_curr = yhat1[i] - yhat1[i-1]\n",
    "            \n",
    "            # Acceleration\n",
    "            acceleration = slope_curr - slope_prev\n",
    "            \n",
    "            # Bullish: negative to positive slope with positive acceleration\n",
    "            if slope_prev < 0 and slope_curr > 0 and acceleration > 0:\n",
    "                bull_signals[i] = True\n",
    "                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n",
    "            \n",
    "            # Bearish: positive to negative slope with negative acceleration\n",
    "            elif slope_prev > 0 and slope_curr < 0 and acceleration < 0:\n",
    "                bear_signals[i] = True\n",
    "                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n",
    "        \n",
    "        # Crossovers with momentum\n",
    "        if i > 0 and not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]):\n",
    "            if not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n",
    "                # Price momentum filter\n",
    "                price_momentum = (prices[i] - prices[max(0, i-5)]) / prices[max(0, i-5)]\n",
    "                \n",
    "                if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1] and price_momentum > 0:\n",
    "                    bull_signals[i] = True\n",
    "                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n",
    "                elif yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1] and price_momentum < 0:\n",
    "                    bear_signals[i] = True\n",
    "                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n",
    "    \n",
    "    return bull_signals, bear_signals, signal_strength\n",
    "\n",
    "# Calculate enhanced NW-RQK\n",
    "print(\"\\nCalculating enhanced NW-RQK with ensemble kernels...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Parameters\n",
    "h = 8.0\n",
    "r = 8.0\n",
    "lag = 2\n",
    "\n",
    "# Calculate regression lines\n",
    "yhat1, yhat1_rq = nadaraya_watson_ensemble(close_30m, h, r)\n",
    "yhat2, yhat2_rq = nadaraya_watson_ensemble(close_30m, h - lag, r)\n",
    "\n",
    "# Detect signals with strength\n",
    "nwrqk_bull, nwrqk_bear, nwrqk_strength = detect_nwrqk_signals_enhanced(yhat1, yhat2, close_30m)\n",
    "\n",
    "# Store in dataframe\n",
    "df_30m['nwrqk_bull'] = nwrqk_bull\n",
    "df_30m['nwrqk_bear'] = nwrqk_bear\n",
    "df_30m['nwrqk_strength'] = nwrqk_strength\n",
    "df_30m['yhat1'] = yhat1\n",
    "df_30m['yhat2'] = yhat2\n",
    "\n",
    "nwrqk_time = time.time() - start_time\n",
    "print(f\"Enhanced NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n",
    "print(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")\n",
    "print(f\"Average signal strength: {nwrqk_strength[nwrqk_strength > 0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Smart Timeframe Alignment\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def create_alignment_map(timestamps_5m: np.ndarray, timestamps_30m: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Create efficient mapping between timeframes\"\"\"\n",
    "    n_5m = len(timestamps_5m)\n",
    "    mapping = np.zeros(n_5m, dtype=np.int64)\n",
    "    \n",
    "    j = 0\n",
    "    for i in prange(n_5m):\n",
    "        # Find the corresponding 30m bar\n",
    "        while j < len(timestamps_30m) - 1 and timestamps_30m[j + 1] <= timestamps_5m[i]:\n",
    "            j += 1\n",
    "        mapping[i] = j\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "print(\"\\nPerforming smart timeframe alignment...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create datetime arrays for mapping\n",
    "# Convert to numeric timestamps for Numba\n",
    "timestamps_5m = df_5m.index.astype(np.int64) // 10**9\n",
    "timestamps_30m = df_30m.index.astype(np.int64) // 10**9\n",
    "\n",
    "# Create mapping\n",
    "mapping = create_alignment_map(timestamps_5m, timestamps_30m)\n",
    "\n",
    "# Align all indicators efficiently\n",
    "df_5m_aligned = df_5m.copy()\n",
    "\n",
    "# MLMI alignment with confidence\n",
    "df_5m_aligned['mlmi'] = df_30m['mlmi'].values[mapping]\n",
    "df_5m_aligned['mlmi_confidence'] = df_30m['mlmi_confidence'].values[mapping]\n",
    "df_5m_aligned['mlmi_bull'] = df_30m['mlmi_bull'].values[mapping]\n",
    "df_5m_aligned['mlmi_bear'] = df_30m['mlmi_bear'].values[mapping]\n",
    "\n",
    "# NW-RQK alignment with strength\n",
    "df_5m_aligned['nwrqk_bull'] = df_30m['nwrqk_bull'].values[mapping]\n",
    "df_5m_aligned['nwrqk_bear'] = df_30m['nwrqk_bear'].values[mapping]\n",
    "df_5m_aligned['nwrqk_strength'] = df_30m['nwrqk_strength'].values[mapping]\n",
    "\n",
    "# FVG data\n",
    "df_5m_aligned['fvg_bull'] = fvg_bull\n",
    "df_5m_aligned['fvg_bear'] = fvg_bear\n",
    "\n",
    "# Add market regime detection\n",
    "df_5m_aligned['volatility'] = df_5m_aligned['Returns'].rolling(20).std()\n",
    "df_5m_aligned['trend_strength'] = abs(df_5m_aligned['Returns'].rolling(50).mean()) / df_5m_aligned['volatility']\n",
    "\n",
    "align_time = time.time() - start_time\n",
    "print(f\"Smart alignment completed in {align_time:.3f} seconds\")\n",
    "print(f\"Aligned {len(df_5m_aligned):,} 5-minute bars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: MLMI → NW-RQK → FVG Synergy Detection\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_mlmi_nwrqk_fvg_synergy(mlmi_bull: np.ndarray, mlmi_bear: np.ndarray,\n",
    "                                 mlmi_conf: np.ndarray, nwrqk_bull: np.ndarray,\n",
    "                                 nwrqk_bear: np.ndarray, nwrqk_strength: np.ndarray,\n",
    "                                 fvg_bull: np.ndarray, fvg_bear: np.ndarray,\n",
    "                                 volatility: np.ndarray, window: int = 30) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Advanced synergy detection with confidence scoring\"\"\"\n",
    "    n = len(mlmi_bull)\n",
    "    long_signals = np.zeros(n, dtype=np.bool_)\n",
    "    short_signals = np.zeros(n, dtype=np.bool_)\n",
    "    signal_quality = np.zeros(n)\n",
    "    \n",
    "    # State tracking\n",
    "    mlmi_active_bull = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_active_bear = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_confirmed_bull = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_confirmed_bear = np.zeros(n, dtype=np.bool_)\n",
    "    state_timer = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        # Carry forward states\n",
    "        mlmi_active_bull[i] = mlmi_active_bull[i-1]\n",
    "        mlmi_active_bear[i] = mlmi_active_bear[i-1]\n",
    "        nwrqk_confirmed_bull[i] = nwrqk_confirmed_bull[i-1]\n",
    "        nwrqk_confirmed_bear[i] = nwrqk_confirmed_bear[i-1]\n",
    "        state_timer[i] = state_timer[i-1] + 1\n",
    "        \n",
    "        # Volatility adjustment\n",
    "        vol_factor = 1.0 / (1.0 + volatility[i] * 10) if not np.isnan(volatility[i]) else 1.0\n",
    "        \n",
    "        # Reset on opposite signal or timeout\n",
    "        if mlmi_bear[i] or state_timer[i] > window:\n",
    "            mlmi_active_bull[i] = False\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            if mlmi_bear[i]:\n",
    "                state_timer[i] = 0\n",
    "        \n",
    "        if mlmi_bull[i] or state_timer[i] > window:\n",
    "            mlmi_active_bear[i] = False\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            if mlmi_bull[i]:\n",
    "                state_timer[i] = 0\n",
    "        \n",
    "        # Step 1: MLMI signal with confidence filter\n",
    "        if mlmi_bull[i] and not mlmi_bull[i-1] and mlmi_conf[i] > 0.3:\n",
    "            mlmi_active_bull[i] = True\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        if mlmi_bear[i] and not mlmi_bear[i-1] and mlmi_conf[i] > 0.3:\n",
    "            mlmi_active_bear[i] = True\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        # Step 2: NW-RQK confirmation with strength filter\n",
    "        if mlmi_active_bull[i] and not nwrqk_confirmed_bull[i] and nwrqk_bull[i] and nwrqk_strength[i] > 0.2:\n",
    "            nwrqk_confirmed_bull[i] = True\n",
    "        \n",
    "        if mlmi_active_bear[i] and not nwrqk_confirmed_bear[i] and nwrqk_bear[i] and nwrqk_strength[i] > 0.2:\n",
    "            nwrqk_confirmed_bear[i] = True\n",
    "        \n",
    "        # Step 3: FVG final confirmation\n",
    "        if nwrqk_confirmed_bull[i] and fvg_bull[i]:\n",
    "            long_signals[i] = True\n",
    "            # Calculate signal quality\n",
    "            signal_quality[i] = (mlmi_conf[i] + nwrqk_strength[i]) / 2 * vol_factor\n",
    "            # Reset states\n",
    "            mlmi_active_bull[i] = False\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        if nwrqk_confirmed_bear[i] and fvg_bear[i]:\n",
    "            short_signals[i] = True\n",
    "            # Calculate signal quality\n",
    "            signal_quality[i] = (mlmi_conf[i] + nwrqk_strength[i]) / 2 * vol_factor\n",
    "            # Reset states\n",
    "            mlmi_active_bear[i] = False\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            state_timer[i] = 0\n",
    "    \n",
    "    return long_signals, short_signals, signal_quality\n",
    "\n",
    "print(\"\\nDetecting MLMI → NW-RQK → FVG synergy signals...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract arrays\n",
    "mlmi_bull_arr = df_5m_aligned['mlmi_bull'].values\n",
    "mlmi_bear_arr = df_5m_aligned['mlmi_bear'].values\n",
    "mlmi_conf_arr = df_5m_aligned['mlmi_confidence'].values\n",
    "nwrqk_bull_arr = df_5m_aligned['nwrqk_bull'].values\n",
    "nwrqk_bear_arr = df_5m_aligned['nwrqk_bear'].values\n",
    "nwrqk_strength_arr = df_5m_aligned['nwrqk_strength'].values\n",
    "fvg_bull_arr = df_5m_aligned['fvg_bull'].values\n",
    "fvg_bear_arr = df_5m_aligned['fvg_bear'].values\n",
    "volatility_arr = df_5m_aligned['volatility'].fillna(0.01).values\n",
    "\n",
    "# Detect synergy\n",
    "long_entries, short_entries, signal_quality = detect_mlmi_nwrqk_fvg_synergy(\n",
    "    mlmi_bull_arr, mlmi_bear_arr, mlmi_conf_arr,\n",
    "    nwrqk_bull_arr, nwrqk_bear_arr, nwrqk_strength_arr,\n",
    "    fvg_bull_arr, fvg_bear_arr, volatility_arr\n",
    ")\n",
    "\n",
    "# Add to dataframe\n",
    "df_5m_aligned['long_entry'] = long_entries\n",
    "df_5m_aligned['short_entry'] = short_entries\n",
    "df_5m_aligned['signal_quality'] = signal_quality\n",
    "\n",
    "signal_time = time.time() - start_time\n",
    "print(f\"Synergy detection completed in {signal_time:.3f} seconds\")\n",
    "print(f\"Long entries: {long_entries.sum():,}\")\n",
    "print(f\"Short entries: {short_entries.sum():,}\")\n",
    "print(f\"Average signal quality: {signal_quality[signal_quality > 0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Statistical Validation and Robustness Testing\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef bootstrap_confidence_intervals(returns: np.ndarray, n_bootstrap: int = 10000,\n                                  confidence: float = 0.95) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Bootstrap confidence intervals with robust statistics\"\"\"\n    n = len(returns)\n    \n    # Arrays to store bootstrap results\n    boot_returns = np.zeros(n_bootstrap)\n    boot_sharpes = np.zeros(n_bootstrap)\n    boot_max_dd = np.zeros(n_bootstrap)\n    boot_win_rates = np.zeros(n_bootstrap)\n    \n    # Remove NaN values\n    clean_returns = returns[~np.isnan(returns)]\n    n_clean = len(clean_returns)\n    \n    if n_clean == 0:\n        return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n    \n    # Bootstrap iterations\n    for i in prange(n_bootstrap):\n        # Resample with replacement\n        np.random.seed(i)  # For reproducibility\n        indices = np.random.randint(0, n_clean, size=n_clean)\n        sample = clean_returns[indices]\n        \n        # Calculate metrics with safety checks\n        boot_returns[i] = np.prod(1 + sample) - 1\n        \n        mean_ret = np.mean(sample)\n        std_ret = np.std(sample)\n        if std_ret > 1e-10:\n            boot_sharpes[i] = mean_ret / std_ret * np.sqrt(252 * 78)\n        else:\n            boot_sharpes[i] = 0.0\n        \n        # Max drawdown\n        cum_ret = np.cumprod(1 + sample)\n        running_max = np.maximum.accumulate(cum_ret)\n        dd = np.where(running_max > 0, (cum_ret - running_max) / running_max, 0)\n        boot_max_dd[i] = np.min(dd)\n        \n        # Win rate\n        boot_win_rates[i] = np.mean(sample > 0)\n    \n    return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STATISTICAL VALIDATION & ROBUSTNESS TESTING\")\nprint(\"=\" * 80)\n\n# Bootstrap analysis\nprint(\"\\nRunning bootstrap analysis (10,000 iterations)...\")\nlogger.info(\"Starting bootstrap analysis\")\nboot_start = time.time()\n\ntry:\n    returns_array = returns.values\n    boot_returns, boot_sharpes, boot_max_dd, boot_win_rates = bootstrap_confidence_intervals(returns_array)\n    \n    boot_time = time.time() - boot_start\n    logger.info(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n    print(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n    \n    # Calculate confidence intervals\n    def calculate_ci(data, confidence=0.95):\n        \"\"\"Calculate confidence interval with safety checks\"\"\"\n        valid_data = data[~np.isnan(data)]\n        if len(valid_data) == 0:\n            return 0.0, 0.0\n        lower = np.percentile(valid_data, (1 - confidence) / 2 * 100)\n        upper = np.percentile(valid_data, (1 + confidence) / 2 * 100)\n        return lower, upper\n    \n    # Display results\n    print(\"\\n95% Confidence Intervals:\")\n    print(\"-\" * 50)\n    \n    ret_lower, ret_upper = calculate_ci(boot_returns)\n    print(f\"Total Return: [{ret_lower*100:.2f}%, {ret_upper*100:.2f}%]\")\n    \n    sharpe_lower, sharpe_upper = calculate_ci(boot_sharpes)\n    print(f\"Sharpe Ratio: [{sharpe_lower:.2f}, {sharpe_upper:.2f}]\")\n    \n    dd_lower, dd_upper = calculate_ci(boot_max_dd)\n    print(f\"Max Drawdown: [{dd_lower*100:.2f}%, {dd_upper*100:.2f}%]\")\n    \n    wr_lower, wr_upper = calculate_ci(boot_win_rates)\n    print(f\"Win Rate: [{wr_lower*100:.2f}%, {wr_upper*100:.2f}%]\")\n    \n    # Statistical significance tests\n    print(\"\\n\" + \"-\" * 50)\n    print(\"STATISTICAL SIGNIFICANCE\")\n    print(\"-\" * 50)\n    \n    # Test if returns are significantly different from zero\n    clean_returns = returns_array[~np.isnan(returns_array)]\n    if len(clean_returns) > 1:\n        mean_return = np.mean(clean_returns)\n        std_return = np.std(clean_returns)\n        n_returns = len(clean_returns)\n        \n        if std_return > 0:\n            t_stat = mean_return / (std_return / np.sqrt(n_returns))\n            # Approximate p-value using normal distribution\n            p_value_approx = 2 * (1 - stats.norm.cdf(abs(t_stat)))\n            \n            print(f\"T-statistic: {t_stat:.3f}\")\n            print(f\"Approx p-value: {p_value_approx:.4f}\")\n            print(f\"Returns significantly positive: {'Yes' if t_stat > 1.96 else 'No'}\")\n        else:\n            print(\"Cannot calculate t-statistic: zero standard deviation\")\n    \n    # Risk-adjusted performance percentiles\n    actual_sharpe = stats['Sharpe Ratio'] if 'stats' in globals() else 0\n    sharpe_percentile = np.sum(boot_sharpes <= actual_sharpe) / len(boot_sharpes) * 100\n    \n    print(f\"\\nStrategy Sharpe ratio percentile: {sharpe_percentile:.1f}%\")\n    print(f\"Performance assessment: \", end=\"\")\n    if sharpe_percentile > 90:\n        print(\"EXCELLENT - Top 10% performance\")\n    elif sharpe_percentile > 75:\n        print(\"VERY GOOD - Top 25% performance\")\n    elif sharpe_percentile > 50:\n        print(\"GOOD - Above median performance\")\n    else:\n        print(\"NEEDS IMPROVEMENT - Below median performance\")\n    \n    # Stability analysis\n    print(\"\\n\" + \"-\" * 50)\n    print(\"STABILITY ANALYSIS\")\n    print(\"-\" * 50)\n    \n    # Rolling performance\n    window = min(252 * 5, len(returns) // 2)  # 1 year of 5-minute bars or half the data\n    if window > 100:\n        rolling_returns = returns.rolling(window).apply(lambda x: (1 + x).prod() - 1)\n        rolling_sharpe = returns.rolling(window).apply(\n            lambda x: x.mean() / x.std() * np.sqrt(252 * 78) if x.std() > 0 else 0\n        )\n        \n        print(f\"Rolling 1-year return volatility: {rolling_returns.std()*100:.2f}%\")\n        print(f\"Rolling Sharpe stability: {rolling_sharpe.std():.2f}\")\n        print(f\"Minimum rolling Sharpe: {rolling_sharpe.min():.2f}\")\n        print(f\"Maximum rolling Sharpe: {rolling_sharpe.max():.2f}\")\n    else:\n        print(\"Insufficient data for rolling analysis\")\n    \n    # Save validation results\n    if config.save_results:\n        validation_results = {\n            'confidence_intervals': {\n                'return': (ret_lower, ret_upper),\n                'sharpe': (sharpe_lower, sharpe_upper),\n                'max_dd': (dd_lower, dd_upper),\n                'win_rate': (wr_lower, wr_upper)\n            },\n            'significance': {\n                't_stat': t_stat if 't_stat' in locals() else None,\n                'significant': t_stat > 1.96 if 't_stat' in locals() else False\n            },\n            'percentiles': {\n                'sharpe_percentile': sharpe_percentile\n            }\n        }\n        \n        validation_path = os.path.join(config.results_path, 'validation_results.json')\n        with open(validation_path, 'w') as f:\n            json.dump(validation_results, f, indent=2, default=str)\n        logger.info(f\"Saved validation results to {validation_path}\")\n    \nexcept Exception as e:\n    logger.error(f\"Error in statistical validation: {str(e)}\")\n    print(f\"\\nError in statistical validation: {str(e)}\")\n    print(\"Continuing with limited validation...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Final Summary and Production Deployment\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL SUMMARY - MLMI → NW-RQK → FVG SYNERGY\")\nprint(\"=\" * 80)\n\n# Execution summary\ntotal_time = calc_time + mlmi_time + nwrqk_time + align_time + signal_time + backtest_time + boot_time\n\ntry:\n    # Performance summary\n    print(\"\\nPERFORMANCE SUMMARY:\")\n    print(\"-\" * 50)\n    print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n    print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n    print(f\"Total Trades: {stats['Total Trades']:,.0f}\")\n    print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n    print(f\"Average Trade: {stats['Expectancy [%]']:.3f}%\")\n    \n    print(\"\\nEXECUTION PERFORMANCE:\")\n    print(\"-\" * 50)\n    print(f\"Total execution time: {total_time:.2f} seconds\")\n    print(f\"Bars processed per second: {len(df_5m_aligned) / total_time:,.0f}\")\n    print(f\"Signals detected per second: {(long_entries.sum() + short_entries.sum()) / signal_time:,.0f}\")\n    \n    # Signal analysis\n    print(\"\\nSIGNAL CHARACTERISTICS:\")\n    print(\"-\" * 50)\n    print(f\"Base indicators (30m):\")\n    print(f\"  - MLMI signals: {df_30m['mlmi_bull'].sum() + df_30m['mlmi_bear'].sum():,}\")\n    print(f\"  - NW-RQK signals: {df_30m['nwrqk_bull'].sum() + df_30m['nwrqk_bear'].sum():,}\")\n    print(f\"FVG zones (5m): {fvg_bull.sum() + fvg_bear.sum():,}\")\n    print(f\"\\nSynergy signals: {long_entries.sum() + short_entries.sum():,}\")\n    \n    mlmi_signals = df_30m['mlmi_bull'].sum() + df_30m['mlmi_bear'].sum()\n    if mlmi_signals > 0:\n        signal_reduction = ((1 - (long_entries.sum() + short_entries.sum()) / mlmi_signals) * 100)\n        print(f\"Signal reduction: {signal_reduction:.1f}%\")\n    \n    # Strengths and weaknesses\n    print(\"\\nKEY STRENGTHS:\")\n    print(\"-\" * 50)\n    strengths = []\n    if stats['Sharpe Ratio'] > 1.0:\n        strengths.append(f\"Strong risk-adjusted returns (Sharpe: {stats['Sharpe Ratio']:.2f})\")\n    if stats['Win Rate [%]'] > 45:\n        strengths.append(f\"Solid win rate ({stats['Win Rate [%]']:.1f}%)\")\n    if stats['Total Trades'] > 1000:\n        strengths.append(f\"Good trade frequency ({stats['Total Trades']:,.0f} trades)\")\n    if abs(stats['Max Drawdown [%]']) < 20:\n        strengths.append(f\"Controlled drawdown ({stats['Max Drawdown [%]']:.1f}%)\")\n    if total_time < 10:\n        strengths.append(f\"Ultra-fast execution ({total_time:.1f} seconds)\")\n    \n    for i, strength in enumerate(strengths, 1):\n        print(f\"{i}. {strength}\")\n    \n    print(\"\\nAREAS FOR IMPROVEMENT:\")\n    print(\"-\" * 50)\n    improvements = []\n    if stats['Sharpe Ratio'] < 0.5:\n        improvements.append(\"Improve risk-adjusted returns\")\n    if stats['Win Rate [%]'] < 40:\n        improvements.append(\"Increase win rate through better entry timing\")\n    if stats['Total Trades'] < 500:\n        improvements.append(\"Consider relaxing signal criteria for more opportunities\")\n    if abs(stats['Max Drawdown [%]']) > 30:\n        improvements.append(\"Implement better risk management to reduce drawdowns\")\n    \n    for i, improvement in enumerate(improvements, 1):\n        print(f\"{i}. {improvement}\")\n    \n    # Save final results\n    if config.save_results:\n        final_results = {\n            'performance': {\n                'total_return': stats['Total Return [%]'],\n                'sharpe_ratio': stats['Sharpe Ratio'],\n                'win_rate': stats['Win Rate [%]'],\n                'total_trades': stats['Total Trades'],\n                'max_drawdown': stats['Max Drawdown [%]']\n            },\n            'execution': {\n                'total_time': total_time,\n                'bars_per_second': len(df_5m_aligned) / total_time\n            },\n            'signals': {\n                'mlmi_count': mlmi_signals,\n                'nwrqk_count': df_30m['nwrqk_bull'].sum() + df_30m['nwrqk_bear'].sum(),\n                'fvg_count': fvg_bull.sum() + fvg_bear.sum(),\n                'final_count': long_entries.sum() + short_entries.sum()\n            },\n            'config': config.__dict__\n        }\n        \n        results_path = os.path.join(config.results_path, 'final_results.json')\n        with open(results_path, 'w') as f:\n            json.dump(final_results, f, indent=2, default=str)\n        \n        # Save trade records\n        if len(trades) > 0:\n            trades_path = os.path.join(config.results_path, 'trades.csv')\n            trades.to_csv(trades_path)\n            logger.info(f\"Saved {len(trades)} trades to {trades_path}\")\n        \n        # Save signals\n        signals_df = df_5m_aligned[['long_entry', 'short_entry', 'signal_quality']].copy()\n        signals_df = signals_df[signals_df['long_entry'] | signals_df['short_entry']]\n        if len(signals_df) > 0:\n            signals_path = os.path.join(config.results_path, 'signals.csv')\n            signals_df.to_csv(signals_path)\n            logger.info(f\"Saved {len(signals_df)} signals to {signals_path}\")\n        \n        print(f\"\\nResults saved to {config.results_path}\")\n        \nexcept Exception as e:\n    logger.error(f\"Error in final summary: {str(e)}\")\n    print(f\"\\nError generating summary: {str(e)}\")\n\n# Recommendations\nprint(\"\\nRECOMMENDATIONS:\")\nprint(\"-\" * 50)\nprint(\"1. Parameter optimization:\")\nprint(\"   - Test different MLMI k-neighbors (100-300)\")\nprint(\"   - Optimize NW-RQK kernel parameters (h: 5-15, r: 5-15)\")\nprint(\"   - Adjust FVG ATR multiplier (1.0-2.0)\")\nprint(\"\\n2. Risk management enhancements:\")\nprint(\"   - Implement dynamic position sizing based on volatility\")\nprint(\"   - Add trailing stops for trend-following trades\")\nprint(\"   - Consider correlation-based portfolio allocation\")\nprint(\"\\n3. Further testing:\")\nprint(\"   - Walk-forward analysis across different market regimes\")\nprint(\"   - Out-of-sample testing on different assets\")\nprint(\"   - Stress testing during high volatility periods\")\nprint(\"\\n4. Production deployment:\")\nprint(\"   - Set up real-time data feeds\")\nprint(\"   - Implement order execution with slippage control\")\nprint(\"   - Add monitoring and alerting systems\")\nprint(\"   - Create automated rebalancing schedules\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ANALYSIS COMPLETE - NOTEBOOK PRODUCTION READY\")\nprint(\"=\" * 80)\n\n# Clean up memory one final time\ncleanup_memory()\nlogger.info(\"Final memory cleanup completed\")\nprint(f\"\\nFinal memory usage: {check_memory_usage():.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Statistical Validation and Robustness Testing\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def bootstrap_confidence_intervals(returns: np.ndarray, n_bootstrap: int = 10000,\n",
    "                                  confidence: float = 0.95) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Bootstrap confidence intervals for key metrics\"\"\"\n",
    "    n = len(returns)\n",
    "    \n",
    "    # Arrays to store bootstrap results\n",
    "    boot_returns = np.zeros(n_bootstrap)\n",
    "    boot_sharpes = np.zeros(n_bootstrap)\n",
    "    boot_max_dd = np.zeros(n_bootstrap)\n",
    "    boot_win_rates = np.zeros(n_bootstrap)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    clean_returns = returns[~np.isnan(returns)]\n",
    "    n_clean = len(clean_returns)\n",
    "    \n",
    "    if n_clean == 0:\n",
    "        return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n",
    "    \n",
    "    # Bootstrap iterations\n",
    "    for i in prange(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        indices = np.random.randint(0, n_clean, size=n_clean)\n",
    "        sample = clean_returns[indices]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        boot_returns[i] = np.prod(1 + sample) - 1\n",
    "        \n",
    "        mean_ret = np.mean(sample)\n",
    "        std_ret = np.std(sample)\n",
    "        if std_ret > 0:\n",
    "            boot_sharpes[i] = mean_ret / std_ret * np.sqrt(252 * 78)\n",
    "        \n",
    "        # Max drawdown\n",
    "        cum_ret = np.cumprod(1 + sample)\n",
    "        running_max = np.maximum.accumulate(cum_ret)\n",
    "        dd = (cum_ret - running_max) / running_max\n",
    "        boot_max_dd[i] = np.min(dd)\n",
    "        \n",
    "        # Win rate\n",
    "        boot_win_rates[i] = np.mean(sample > 0)\n",
    "    \n",
    "    return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL VALIDATION & ROBUSTNESS TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Bootstrap analysis\n",
    "print(\"\\nRunning bootstrap analysis (10,000 iterations)...\")\n",
    "boot_start = time.time()\n",
    "\n",
    "returns_array = returns.values\n",
    "boot_returns, boot_sharpes, boot_max_dd, boot_win_rates = bootstrap_confidence_intervals(returns_array)\n",
    "\n",
    "boot_time = time.time() - boot_start\n",
    "print(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n",
    "\n",
    "# Calculate confidence intervals\n",
    "def calculate_ci(data, confidence=0.95):\n",
    "    lower = np.percentile(data, (1 - confidence) / 2 * 100)\n",
    "    upper = np.percentile(data, (1 + confidence) / 2 * 100)\n",
    "    return lower, upper\n",
    "\n",
    "# Display results\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "ret_lower, ret_upper = calculate_ci(boot_returns)\n",
    "print(f\"Total Return: [{ret_lower*100:.2f}%, {ret_upper*100:.2f}%]\")\n",
    "\n",
    "sharpe_lower, sharpe_upper = calculate_ci(boot_sharpes)\n",
    "print(f\"Sharpe Ratio: [{sharpe_lower:.2f}, {sharpe_upper:.2f}]\")\n",
    "\n",
    "dd_lower, dd_upper = calculate_ci(boot_max_dd)\n",
    "print(f\"Max Drawdown: [{dd_lower*100:.2f}%, {dd_upper*100:.2f}%]\")\n",
    "\n",
    "wr_lower, wr_upper = calculate_ci(boot_win_rates)\n",
    "print(f\"Win Rate: [{wr_lower*100:.2f}%, {wr_upper*100:.2f}%]\")\n",
    "\n",
    "# Statistical significance tests\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"STATISTICAL SIGNIFICANCE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test if returns are significantly different from zero\n",
    "t_stat = np.mean(returns_array[~np.isnan(returns_array)]) / (np.std(returns_array[~np.isnan(returns_array)]) / np.sqrt(len(returns_array[~np.isnan(returns_array)])))\n",
    "p_value = 2 * (1 - scipy.stats.norm.cdf(abs(t_stat))) if 'scipy' in globals() else 0.05\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"Returns significantly positive: {'Yes' if t_stat > 1.96 else 'No'}\")\n",
    "\n",
    "# Risk-adjusted performance percentiles\n",
    "actual_sharpe = stats['Sharpe Ratio']\n",
    "sharpe_percentile = np.sum(boot_sharpes <= actual_sharpe) / len(boot_sharpes) * 100\n",
    "\n",
    "print(f\"\\nStrategy Sharpe ratio percentile: {sharpe_percentile:.1f}%\")\n",
    "print(f\"Performance assessment: \", end=\"\")\n",
    "if sharpe_percentile > 90:\n",
    "    print(\"EXCELLENT - Top 10% performance\")\n",
    "elif sharpe_percentile > 75:\n",
    "    print(\"VERY GOOD - Top 25% performance\")\n",
    "elif sharpe_percentile > 50:\n",
    "    print(\"GOOD - Above median performance\")\n",
    "else:\n",
    "    print(\"NEEDS IMPROVEMENT - Below median performance\")\n",
    "\n",
    "# Stability analysis\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"STABILITY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Rolling performance\n",
    "window = 252 * 5  # 1 year of 5-minute bars\n",
    "rolling_returns = returns.rolling(window).apply(lambda x: (1 + x).prod() - 1)\n",
    "rolling_sharpe = returns.rolling(window).apply(lambda x: x.mean() / x.std() * np.sqrt(252 * 78) if x.std() > 0 else 0)\n",
    "\n",
    "print(f\"Rolling 1-year return volatility: {rolling_returns.std()*100:.2f}%\")\n",
    "print(f\"Rolling Sharpe stability: {rolling_sharpe.std():.2f}\")\n",
    "print(f\"Minimum rolling Sharpe: {rolling_sharpe.min():.2f}\")\n",
    "print(f\"Maximum rolling Sharpe: {rolling_sharpe.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Final Summary and Recommendations\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY - MLMI → NW-RQK → FVG SYNERGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nPERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "print(f\"Total Trades: {stats['Total Trades']:,.0f}\")\n",
    "print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "print(f\"Average Trade: {stats['Expectancy [%]']:.3f}%\")\n",
    "\n",
    "# Execution summary\n",
    "total_time = calc_time + mlmi_time + nwrqk_time + align_time + signal_time + backtest_time + boot_time\n",
    "print(\"\\nEXECUTION PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "print(f\"Bars processed per second: {len(df_5m_aligned) / total_time:,.0f}\")\n",
    "print(f\"Signals detected per second: {(long_entries.sum() + short_entries.sum()) / signal_time:,.0f}\")\n",
    "\n",
    "# Signal analysis\n",
    "print(\"\\nSIGNAL CHARACTERISTICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Base indicators (30m):\")\n",
    "print(f\"  - MLMI signals: {df_30m['mlmi_bull'].sum() + df_30m['mlmi_bear'].sum():,}\")\n",
    "print(f\"  - NW-RQK signals: {df_30m['nwrqk_bull'].sum() + df_30m['nwrqk_bear'].sum():,}\")\n",
    "print(f\"FVG zones (5m): {fvg_bull.sum() + fvg_bear.sum():,}\")\n",
    "print(f\"\\nSynergy signals: {long_entries.sum() + short_entries.sum():,}\")\n",
    "print(f\"Signal reduction: {((1 - (long_entries.sum() + short_entries.sum()) / (df_30m['mlmi_bull'].sum() + df_30m['mlmi_bear'].sum())) * 100):.1f}%\")\n",
    "\n",
    "# Strengths and weaknesses\n",
    "print(\"\\nKEY STRENGTHS:\")\n",
    "print(\"-\" * 50)\n",
    "strengths = []\n",
    "if stats['Sharpe Ratio'] > 1.0:\n",
    "    strengths.append(f\"Strong risk-adjusted returns (Sharpe: {stats['Sharpe Ratio']:.2f})\")\n",
    "if stats['Win Rate [%]'] > 45:\n",
    "    strengths.append(f\"Solid win rate ({stats['Win Rate [%]']:.1f}%)\")\n",
    "if stats['Total Trades'] > 1000:\n",
    "    strengths.append(f\"Good trade frequency ({stats['Total Trades']:,.0f} trades)\")\n",
    "if abs(stats['Max Drawdown [%]']) < 20:\n",
    "    strengths.append(f\"Controlled drawdown ({stats['Max Drawdown [%]']:.1f}%)\")\n",
    "if total_time < 10:\n",
    "    strengths.append(f\"Ultra-fast execution ({total_time:.1f} seconds)\")\n",
    "\n",
    "for i, strength in enumerate(strengths, 1):\n",
    "    print(f\"{i}. {strength}\")\n",
    "\n",
    "print(\"\\nAREAS FOR IMPROVEMENT:\")\n",
    "print(\"-\" * 50)\n",
    "improvements = []\n",
    "if stats['Sharpe Ratio'] < 0.5:\n",
    "    improvements.append(\"Improve risk-adjusted returns\")\n",
    "if stats['Win Rate [%]'] < 40:\n",
    "    improvements.append(\"Increase win rate through better entry timing\")\n",
    "if stats['Total Trades'] < 500:\n",
    "    improvements.append(\"Consider relaxing signal criteria for more opportunities\")\n",
    "if abs(stats['Max Drawdown [%]']) > 30:\n",
    "    improvements.append(\"Implement better risk management to reduce drawdowns\")\n",
    "\n",
    "for i, improvement in enumerate(improvements, 1):\n",
    "    print(f\"{i}. {improvement}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Parameter optimization:\")\n",
    "print(\"   - Test different MLMI k-neighbors (100-300)\")\n",
    "print(\"   - Optimize NW-RQK kernel parameters (h: 5-15, r: 5-15)\")\n",
    "print(\"   - Adjust FVG ATR multiplier (1.0-2.0)\")\n",
    "print(\"\\n2. Risk management enhancements:\")\n",
    "print(\"   - Implement dynamic position sizing based on volatility\")\n",
    "print(\"   - Add trailing stops for trend-following trades\")\n",
    "print(\"   - Consider correlation-based portfolio allocation\")\n",
    "print(\"\\n3. Further testing:\")\n",
    "print(\"   - Walk-forward analysis across different market regimes\")\n",
    "print(\"   - Out-of-sample testing on different assets\")\n",
    "print(\"   - Stress testing during high volatility periods\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}