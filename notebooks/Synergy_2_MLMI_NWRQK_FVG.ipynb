{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 2: MLMI → NW-RQK → FVG Trading Strategy\n",
    "\n",
    "**Ultra-Fast Backtesting with VectorBT and Numba JIT Compilation**\n",
    "\n",
    "This notebook implements the second synergy pattern where:\n",
    "1. MLMI provides the primary trend signal\n",
    "2. NW-RQK confirms the trend direction\n",
    "3. FVG validates the final entry zone\n",
    "\n",
    "Key differences from Synergy 1:\n",
    "- NW-RQK confirmation comes before FVG\n",
    "- May capture different market dynamics\n",
    "- Expected to generate similar trade counts but with different timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Environment Setup and Imports\n\n# Standard library imports\nimport os\nimport sys\nimport gc\nimport json\nimport time\nimport logging\nimport warnings\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Tuple, Dict as TypeDict, Optional, List, Union, Any\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\nimport pickle\n\n# Scientific computing imports\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.spatial import cKDTree\n\n# Visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Trading and backtesting imports\nimport vectorbt as vbt\n\n# Performance optimization imports\nfrom numba import njit, prange, typed, types\nfrom numba.typed import Dict\nimport numba\n\n# Progress tracking\nfrom tqdm import tqdm\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout),\n        logging.FileHandler('synergy_strategy.log')\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Configure Numba for maximum performance\nnumba.config.THREADING_LAYER = 'threadsafe'\nnumba.config.NUMBA_NUM_THREADS = numba.config.NUMBA_DEFAULT_NUM_THREADS\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_rows', 100)\n\n# Version checks\nlogger.info(\"Environment Setup\")\nlogger.info(f\"Python version: {sys.version}\")\nlogger.info(f\"NumPy version: {np.__version__}\")\nlogger.info(f\"Pandas version: {pd.__version__}\")\nlogger.info(f\"VectorBT version: {vbt.__version__}\")\nlogger.info(f\"Numba version: {numba.__version__}\")\nlogger.info(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\n\nprint(\"Synergy 2: MLMI → NW-RQK → FVG Strategy\")\nprint(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\nprint(f\"VectorBT version: {vbt.__version__}\")\nprint(\"Environment ready for ultra-fast backtesting!\")\n\n# Configuration dataclass\n@dataclass\nclass StrategyConfig:\n    \"\"\"Configuration for the trading strategy\"\"\"\n    # Data paths\n    data_path_5m: str = \"/home/QuantNova/AlgoSpace-Strategy-1/@NQ - 5 min - ETH.csv\"\n    data_path_30m: str = \"/home/QuantNova/AlgoSpace-Strategy-1/NQ - 30 min - ETH.csv\"\n    \n    # MLMI parameters\n    mlmi_k_neighbors: int = 200\n    mlmi_confidence_threshold: float = 0.3\n    mlmi_forward_bars: int = 5\n    \n    # NW-RQK parameters\n    nwrqk_h: float = 8.0\n    nwrqk_r: float = 8.0\n    nwrqk_lag: int = 2\n    nwrqk_strength_threshold: float = 0.2\n    \n    # FVG parameters\n    fvg_atr_multiplier: float = 1.5\n    fvg_active_bars: int = 20\n    \n    # Signal parameters\n    synergy_window: int = 30\n    \n    # Backtesting parameters\n    initial_capital: float = 100000\n    position_size_base: float = 100\n    stop_loss_atr: float = 2.0\n    max_holding_bars: int = 100\n    fees: float = 0.0001\n    slippage: float = 0.0001\n    \n    # Performance parameters\n    chunk_size: int = 10000\n    max_memory_gb: float = 8.0\n    \n    # Output parameters\n    save_results: bool = True\n    results_path: str = \"./results\"\n    checkpoint_interval: int = 1000\n\n# Create default configuration\nconfig = StrategyConfig()\n\n# Memory management utilities\ndef check_memory_usage():\n    \"\"\"Check current memory usage\"\"\"\n    try:\n        import psutil\n        process = psutil.Process(os.getpid())\n        mem_gb = process.memory_info().rss / 1024 / 1024 / 1024\n        return mem_gb\n    except ImportError:\n        logger.warning(\"psutil not installed, memory monitoring disabled\")\n        return 0.0\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection\"\"\"\n    gc.collect()\n    logger.info(f\"Memory after cleanup: {check_memory_usage():.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Data Loading with Robust Error Handling\n\nclass DataLoadingError(Exception):\n    \"\"\"Custom exception for data loading errors\"\"\"\n    pass\n\nclass DataValidationError(Exception):\n    \"\"\"Custom exception for data validation errors\"\"\"\n    pass\n\ndef validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> None:\n    \"\"\"Validate dataframe has required columns and valid data\"\"\"\n    # Check for required columns\n    missing_columns = set(required_columns) - set(df.columns)\n    if missing_columns:\n        raise DataValidationError(f\"Missing required columns: {missing_columns}\")\n    \n    # Check for empty dataframe\n    if len(df) == 0:\n        raise DataValidationError(\"Dataframe is empty\")\n    \n    # Check for sufficient data\n    if len(df) < 100:\n        logger.warning(f\"Limited data: only {len(df)} rows available\")\n    \n    # Check for NaN values in critical columns\n    critical_columns = ['Open', 'High', 'Low', 'Close']\n    nan_counts = df[critical_columns].isna().sum()\n    if nan_counts.any():\n        logger.warning(f\"NaN values found: {nan_counts.to_dict()}\")\n\ndef load_data_optimized(file_path: str, timeframe: str = '5m', \n                       config: Optional[StrategyConfig] = None) -> pd.DataFrame:\n    \"\"\"Load and prepare data with comprehensive error handling\"\"\"\n    start_time = time.time()\n    logger.info(f\"Loading {timeframe} data from {file_path}\")\n    \n    try:\n        # Check if file exists\n        if not os.path.exists(file_path):\n            raise DataLoadingError(f\"Data file not found: {file_path}\")\n        \n        # Check file size\n        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n        if file_size_mb > 1000:\n            logger.warning(f\"Large file detected: {file_size_mb:.1f} MB\")\n        \n        # Read CSV with optimized settings\n        df = pd.read_csv(\n            file_path,\n            parse_dates=['Timestamp'],\n            index_col='Timestamp'\n        )\n        \n        # Ensure datetime index\n        if not isinstance(df.index, pd.DatetimeIndex):\n            df.index = pd.to_datetime(df.index, dayfirst=True, errors='coerce')\n        \n        # Validate required columns\n        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n        validate_dataframe(df, required_columns)\n        \n        # Ensure numeric types for fast operations\n        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n        for col in numeric_cols:\n            if col in df.columns:\n                df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float64)\n        \n        # Remove any rows with invalid timestamps\n        df = df[df.index.notnull()]\n        \n        # Remove any NaN values in OHLC\n        initial_len = len(df)\n        df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n        if len(df) < initial_len:\n            logger.warning(f\"Dropped {initial_len - len(df)} rows with NaN values\")\n        \n        # Validate OHLC relationships\n        invalid_candles = (\n            (df['High'] < df['Low']) |\n            (df['High'] < df['Open']) |\n            (df['High'] < df['Close']) |\n            (df['Low'] > df['Open']) |\n            (df['Low'] > df['Close'])\n        )\n        if invalid_candles.any():\n            logger.warning(f\"Found {invalid_candles.sum()} invalid candles, fixing...\")\n            df.loc[invalid_candles, 'High'] = df.loc[invalid_candles, ['Open', 'Close', 'High']].max(axis=1)\n            df.loc[invalid_candles, 'Low'] = df.loc[invalid_candles, ['Open', 'Close', 'Low']].min(axis=1)\n        \n        # Sort index for faster operations\n        df.sort_index(inplace=True)\n        \n        # Check for duplicate timestamps\n        duplicates = df.index.duplicated()\n        if duplicates.any():\n            logger.warning(f\"Found {duplicates.sum()} duplicate timestamps, keeping first\")\n            df = df[~duplicates]\n        \n        # Pre-calculate commonly used features with safe operations\n        df['Returns'] = df['Close'].pct_change().fillna(0)\n        df['LogReturns'] = np.where(\n            df['Close'].shift(1) > 0,\n            np.log(df['Close'] / df['Close'].shift(1)),\n            0\n        )\n        df['HL_Range'] = df['High'] - df['Low']\n        df['OC_Range'] = abs(df['Open'] - df['Close'])\n        \n        # Add data quality metrics\n        df['DataQuality'] = 1.0\n        df.loc[df['Volume'] == 0, 'DataQuality'] *= 0.8\n        df.loc[df['HL_Range'] == 0, 'DataQuality'] *= 0.9\n        \n        load_time = time.time() - start_time\n        logger.info(f\"Successfully loaded {len(df):,} rows in {load_time:.2f} seconds\")\n        logger.info(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n        logger.info(f\"Average data quality: {df['DataQuality'].mean():.3f}\")\n        \n        # Memory optimization\n        df = df.astype({col: 'float32' for col in numeric_cols if col in df.columns})\n        \n        return df\n        \n    except pd.errors.ParserError as e:\n        raise DataLoadingError(f\"Failed to parse CSV file: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error loading data: {str(e)}\")\n        raise DataLoadingError(f\"Failed to load data: {str(e)}\")\n\n# Pre-compile all Numba functions\nprint(\"Pre-compiling Numba functions for maximum speed...\")\n\n@njit(cache=True)\ndef dummy_compile():\n    \"\"\"Dummy function to trigger compilation\"\"\"\n    return np.array([1.0, 2.0, 3.0]).sum()\n\n_ = dummy_compile()  # Trigger compilation\n\n# Load data files with error handling\nprint(\"\\nLoading data files...\")\n\ntry:\n    # Check if config paths exist, otherwise try alternative paths\n    data_paths_5m = [\n        config.data_path_5m,\n        \"./data/@NQ - 5 min - ETH.csv\",\n        \"../data/@NQ - 5 min - ETH.csv\",\n        \"data/@NQ - 5 min - ETH.csv\"\n    ]\n    \n    data_paths_30m = [\n        config.data_path_30m,\n        \"./data/NQ - 30 min - ETH.csv\",\n        \"../data/NQ - 30 min - ETH.csv\",\n        \"data/NQ - 30 min - ETH.csv\"\n    ]\n    \n    # Try to load 5m data\n    df_5m = None\n    for path in data_paths_5m:\n        if os.path.exists(path):\n            df_5m = load_data_optimized(path, '5m', config)\n            break\n    \n    if df_5m is None:\n        raise DataLoadingError(f\"Could not find 5m data file. Tried: {data_paths_5m}\")\n    \n    # Try to load 30m data\n    df_30m = None\n    for path in data_paths_30m:\n        if os.path.exists(path):\n            df_30m = load_data_optimized(path, '30m', config)\n            break\n    \n    if df_30m is None:\n        raise DataLoadingError(f\"Could not find 30m data file. Tried: {data_paths_30m}\")\n    \n    # Ensure time alignment\n    common_start = max(df_5m.index[0], df_30m.index[0])\n    common_end = min(df_5m.index[-1], df_30m.index[-1])\n    \n    df_5m = df_5m.loc[common_start:common_end]\n    df_30m = df_30m.loc[common_start:common_end]\n    \n    print(f\"\\n5-minute data: {df_5m.index[0]} to {df_5m.index[-1]} ({len(df_5m):,} bars)\")\n    print(f\"30-minute data: {df_30m.index[0]} to {df_30m.index[-1]} ({len(df_30m):,} bars)\")\n    print(f\"Memory usage: {check_memory_usage():.2f} GB\")\n    \n    # Save checkpoint\n    if config.save_results:\n        os.makedirs(config.results_path, exist_ok=True)\n        checkpoint_path = os.path.join(config.results_path, 'data_checkpoint.pkl')\n        with open(checkpoint_path, 'wb') as f:\n            pickle.dump({\n                'df_5m_shape': df_5m.shape,\n                'df_30m_shape': df_30m.shape,\n                'date_range': (df_5m.index[0], df_5m.index[-1])\n            }, f)\n        logger.info(f\"Saved data checkpoint to {checkpoint_path}\")\n    \nexcept DataLoadingError as e:\n    logger.error(f\"Data loading failed: {e}\")\n    print(f\"\\nERROR: {e}\")\n    print(\"\\nPlease ensure data files are in the correct location.\")\n    print(\"You can update the file paths in the StrategyConfig class in Cell 1.\")\n    raise\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Optimized Indicator Suite with Robust Error Handling\n\n@njit(fastmath=True, cache=True, parallel=True)\ndef calculate_all_indicators(close: np.ndarray, high: np.ndarray, low: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate all basic indicators with comprehensive error handling\"\"\"\n    n = len(close)\n    \n    # Pre-allocate arrays with default values\n    ma5 = np.full(n, np.nan, dtype=np.float64)\n    ma20 = np.full(n, np.nan, dtype=np.float64)\n    rsi5 = np.full(n, 50.0, dtype=np.float64)\n    rsi20 = np.full(n, 50.0, dtype=np.float64)\n    atr = np.full(n, np.nan, dtype=np.float64)\n    \n    # Input validation\n    if n == 0:\n        return ma5, ma20, rsi5, rsi20, atr\n    \n    # Weighted Moving Averages with safe calculations\n    weights5 = np.arange(1, 6, dtype=np.float64)\n    weights20 = np.arange(1, 21, dtype=np.float64)\n    sum_w5 = weights5.sum()\n    sum_w20 = weights20.sum()\n    \n    # Calculate WMAs in parallel chunks with bounds checking\n    for i in prange(n):\n        # 5-period WMA\n        if i >= 4:\n            window_data = close[i-4:i+1]\n            if not np.any(np.isnan(window_data)):\n                ma5[i] = np.dot(window_data, weights5) / sum_w5\n        \n        # 20-period WMA\n        if i >= 19:\n            window_data = close[i-19:i+1]\n            if not np.any(np.isnan(window_data)):\n                ma20[i] = np.dot(window_data, weights20) / sum_w20\n    \n    # RSI calculation with safe division\n    if n > 1:\n        deltas = np.diff(close)\n        gains = np.maximum(deltas, 0)\n        losses = -np.minimum(deltas, 0)\n        \n        # RSI 5\n        if len(gains) >= 5:\n            avg_gain5 = np.mean(gains[:5])\n            avg_loss5 = np.mean(losses[:5])\n            \n            if avg_loss5 > 0:\n                rs5 = avg_gain5 / avg_loss5\n                rsi5[5] = 100 - (100 / (1 + rs5))\n            else:\n                rsi5[5] = 100 if avg_gain5 > 0 else 50\n            \n            # Calculate remaining RSI values\n            for i in range(5, min(n - 1, len(gains))):\n                avg_gain5 = (avg_gain5 * 4 + gains[i]) / 5\n                avg_loss5 = (avg_loss5 * 4 + losses[i]) / 5\n                \n                if avg_loss5 > 0:\n                    rs5 = avg_gain5 / avg_loss5\n                    rsi5[i + 1] = 100 - (100 / (1 + rs5))\n                else:\n                    rsi5[i + 1] = 100 if avg_gain5 > 0 else 50\n        \n        # RSI 20\n        if len(gains) >= 20:\n            avg_gain20 = np.mean(gains[:20])\n            avg_loss20 = np.mean(losses[:20])\n            \n            if avg_loss20 > 0:\n                rs20 = avg_gain20 / avg_loss20\n                rsi20[20] = 100 - (100 / (1 + rs20))\n            else:\n                rsi20[20] = 100 if avg_gain20 > 0 else 50\n            \n            # Calculate remaining RSI values\n            for i in range(20, min(n - 1, len(gains))):\n                avg_gain20 = (avg_gain20 * 19 + gains[i]) / 20\n                avg_loss20 = (avg_loss20 * 19 + losses[i]) / 20\n                \n                if avg_loss20 > 0:\n                    rs20 = avg_gain20 / avg_loss20\n                    rsi20[i + 1] = 100 - (100 / (1 + rs20))\n                else:\n                    rsi20[i + 1] = 100 if avg_gain20 > 0 else 50\n    \n    # ATR calculation with safe operations\n    if n > 1:\n        # Calculate true range\n        tr = np.zeros(n, dtype=np.float64)\n        tr[0] = high[0] - low[0] if not np.isnan(high[0]) and not np.isnan(low[0]) else 0\n        \n        for i in range(1, n):\n            if not np.isnan(high[i]) and not np.isnan(low[i]) and not np.isnan(close[i-1]):\n                hl = high[i] - low[i]\n                hc = abs(high[i] - close[i-1])\n                lc = abs(low[i] - close[i-1])\n                tr[i] = max(hl, hc, lc)\n            else:\n                tr[i] = 0\n        \n        # Calculate ATR\n        for i in range(14, n):\n            window = tr[i-13:i+1]\n            valid_values = window[window > 0]\n            if len(valid_values) > 0:\n                atr[i] = np.mean(valid_values)\n    \n    return ma5, ma20, rsi5, rsi20, atr\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef detect_fvg_optimized(high: np.ndarray, low: np.ndarray, atr: np.ndarray,\n                        multiplier: float = 1.5, min_gap_pct: float = 0.001,\n                        active_bars: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Optimized FVG detection with ATR filtering and safety checks\"\"\"\n    n = len(high)\n    bull_active = np.zeros(n, dtype=np.bool_)\n    bear_active = np.zeros(n, dtype=np.bool_)\n    \n    if n < 4:  # Need at least 4 bars for FVG\n        return bull_active, bear_active\n    \n    for i in prange(3, n):\n        # Skip if ATR is invalid\n        if np.isnan(atr[i]) or atr[i] <= 0:\n            continue\n        \n        # Dynamic gap threshold based on ATR\n        gap_threshold = max(atr[i] * multiplier, low[i] * min_gap_pct)\n        \n        # Bullish FVG with safety checks\n        if (not np.isnan(low[i]) and not np.isnan(high[i-3]) and \n            low[i] > high[i-3]):\n            gap_size = low[i] - high[i-3]\n            if gap_size > gap_threshold:\n                # Mark active zone\n                for j in range(i, min(i + active_bars, n)):\n                    if not np.isnan(low[j]) and low[j] >= high[i-3]:\n                        bull_active[j] = True\n                    else:\n                        break\n        \n        # Bearish FVG with safety checks\n        if (not np.isnan(high[i]) and not np.isnan(low[i-3]) and \n            high[i] < low[i-3]):\n            gap_size = low[i-3] - high[i]\n            if gap_size > gap_threshold:\n                # Mark active zone\n                for j in range(i, min(i + active_bars, n)):\n                    if not np.isnan(high[j]) and high[j] <= low[i-3]:\n                        bear_active[j] = True\n                    else:\n                        break\n    \n    return bull_active, bear_active\n\n# Safe smoothing function\ndef safe_smooth(data: np.ndarray, window: int = 20) -> np.ndarray:\n    \"\"\"Apply smoothing with NaN handling\"\"\"\n    if len(data) < window:\n        return data\n    \n    # Replace NaN with forward fill for smoothing\n    filled_data = pd.Series(data).ffill().bfill().values\n    \n    # Apply convolution\n    kernel = np.ones(window) / window\n    smoothed = np.convolve(filled_data, kernel, mode='same')\n    \n    # Restore NaN where original data had NaN\n    smoothed[np.isnan(data)] = np.nan\n    \n    return smoothed\n\nprint(\"\\nCalculating all indicators with parallel processing...\")\nlogger.info(\"Starting indicator calculations\")\nstart_time = time.time()\n\ntry:\n    # Calculate 30-minute indicators\n    close_30m = df_30m['Close'].values.astype(np.float64)\n    high_30m = df_30m['High'].values.astype(np.float64)\n    low_30m = df_30m['Low'].values.astype(np.float64)\n    \n    ma5, ma20, rsi5, rsi20, atr_30m = calculate_all_indicators(\n        close_30m, high_30m, low_30m\n    )\n    \n    # Smooth RSI with safety\n    rsi5_smooth = safe_smooth(rsi5, 20)\n    rsi20_smooth = safe_smooth(rsi20, 20)\n    \n    # Calculate 5-minute indicators\n    close_5m = df_5m['Close'].values.astype(np.float64)\n    high_5m = df_5m['High'].values.astype(np.float64)\n    low_5m = df_5m['Low'].values.astype(np.float64)\n    \n    _, _, _, _, atr_5m = calculate_all_indicators(\n        close_5m, high_5m, low_5m\n    )\n    \n    # Detect FVG with ATR filter\n    fvg_bull, fvg_bear = detect_fvg_optimized(\n        high_5m, low_5m, atr_5m, \n        config.fvg_atr_multiplier,\n        0.001,\n        config.fvg_active_bars\n    )\n    \n    calc_time = time.time() - start_time\n    \n    # Log statistics\n    logger.info(f\"Indicators calculated in {calc_time:.3f} seconds\")\n    logger.info(f\"MA5 valid values: {(~np.isnan(ma5)).sum()}/{len(ma5)}\")\n    logger.info(f\"MA20 valid values: {(~np.isnan(ma20)).sum()}/{len(ma20)}\")\n    logger.info(f\"RSI5 range: [{np.nanmin(rsi5):.1f}, {np.nanmax(rsi5):.1f}]\")\n    logger.info(f\"RSI20 range: [{np.nanmin(rsi20):.1f}, {np.nanmax(rsi20):.1f}]\")\n    logger.info(f\"ATR 30m valid: {(~np.isnan(atr_30m)).sum()}/{len(atr_30m)}\")\n    logger.info(f\"ATR 5m valid: {(~np.isnan(atr_5m)).sum()}/{len(atr_5m)}\")\n    logger.info(f\"FVG zones - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")\n    \n    print(f\"All indicators calculated in {calc_time:.3f} seconds\")\n    print(f\"FVG zones detected - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")\n    \n    # Memory cleanup\n    if check_memory_usage() > config.max_memory_gb * 0.8:\n        cleanup_memory()\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating indicators: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Advanced MLMI with Adaptive KNN\n\n@njit(fastmath=True, cache=True)\ndef adaptive_knn_predict(features: np.ndarray, labels: np.ndarray, query: np.ndarray,\n                        k_base: int, volatility: float, size: int) -> Tuple[float, float]:\n    \"\"\"Adaptive KNN with safe distance calculations\"\"\"\n    if size == 0:\n        return 0.0, 0.5\n    \n    # Adjust K based on volatility with bounds checking\n    vol_factor = max(0.0, min(1.0, 1.0 - volatility * 2.0))\n    k = max(3, min(k_base, int(k_base * vol_factor)))\n    k = min(k, size)\n    \n    # Calculate distances with numerical stability\n    distances = np.zeros(size)\n    for i in range(size):\n        dist = 0.0\n        for j in range(min(2, features.shape[1])):  # Ensure we don't exceed feature dimensions\n            diff = features[i, j] - query[j]\n            dist += diff * diff\n        distances[i] = np.sqrt(max(0.0, dist))  # Ensure non-negative\n    \n    # Find k nearest neighbors\n    if k <= size:\n        indices = np.argpartition(distances, k-1)[:k]\n    else:\n        indices = np.arange(size)\n    \n    # Weighted voting based on distance\n    vote = 0.0\n    weight_sum = 0.0\n    \n    for i in range(len(indices)):\n        idx = indices[i]\n        if distances[idx] > 1e-10:  # Avoid division by very small numbers\n            weight = 1.0 / distances[idx]\n            vote += labels[idx] * weight\n            weight_sum += weight\n        else:\n            # Handle exact matches\n            vote += labels[idx] * 100.0\n            weight_sum += 100.0\n    \n    if weight_sum > 1e-10:\n        prediction = vote / weight_sum\n        confidence = min(abs(prediction) / max(1.0, float(k)), 1.0)\n    else:\n        prediction = 0.0\n        confidence = 0.0\n    \n    return prediction, confidence\n\n@njit(fastmath=True, cache=True)\ndef calculate_mlmi_adaptive(ma_fast: np.ndarray, ma_slow: np.ndarray,\n                           rsi_fast_smooth: np.ndarray, rsi_slow_smooth: np.ndarray,\n                           close: np.ndarray, returns: np.ndarray,\n                           k_neighbors: int = 200,\n                           min_confidence: float = 0.1,\n                           forward_bars: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"MLMI with comprehensive error handling\"\"\"\n    n = len(close)\n    mlmi_values = np.zeros(n)\n    mlmi_confidence = np.zeros(n)\n    \n    # Input validation\n    if n < 10:\n        return mlmi_values, mlmi_confidence\n    \n    # Pre-allocate KNN storage\n    max_size = min(10000, n)\n    features = np.zeros((max_size, 2))\n    labels = np.zeros(max_size)\n    data_size = 0\n    \n    # Calculate rolling volatility with safe operations\n    volatility = np.zeros(n)\n    for i in range(20, n):\n        window_returns = returns[max(0, i-20):i]\n        valid_returns = window_returns[~np.isnan(window_returns)]\n        if len(valid_returns) > 1:\n            volatility[i] = np.std(valid_returns)\n        else:\n            volatility[i] = 0.01  # Default volatility\n    \n    for i in range(1, n):\n        # Detect crossovers with NaN checks\n        if (np.isnan(ma_fast[i]) or np.isnan(ma_slow[i]) or \n            np.isnan(ma_fast[i-1]) or np.isnan(ma_slow[i-1])):\n            continue\n            \n        bull_cross = ma_fast[i] > ma_slow[i] and ma_fast[i-1] <= ma_slow[i-1]\n        bear_cross = ma_fast[i] < ma_slow[i] and ma_fast[i-1] >= ma_slow[i-1]\n        \n        if (bull_cross or bear_cross) and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n            # Store pattern\n            if data_size >= max_size:\n                # Keep most recent 75%\n                keep_size = int(max_size * 0.75)\n                features[:keep_size] = features[-keep_size:]\n                labels[:keep_size] = labels[-keep_size:]\n                data_size = keep_size\n            \n            features[data_size, 0] = rsi_slow_smooth[i]\n            features[data_size, 1] = rsi_fast_smooth[i]\n            \n            if i < n - forward_bars:  # Ensure we have forward data\n                # Multi-bar forward return with safety\n                fwd_idx = min(i + forward_bars, n - 1)\n                if close[i] > 0:\n                    fwd_ret = (close[fwd_idx] - close[i]) / close[i]\n                    # Clip extreme values\n                    fwd_ret = max(-0.1, min(0.1, fwd_ret))\n                    labels[data_size] = np.sign(fwd_ret) * min(abs(fwd_ret) * 100.0, 1.0)\n                else:\n                    labels[data_size] = 0.0\n            else:\n                labels[data_size] = 0.0\n            \n            data_size += 1\n        \n        # Make prediction\n        if (data_size > 10 and not np.isnan(rsi_fast_smooth[i]) and \n            not np.isnan(rsi_slow_smooth[i])):\n            query = np.array([rsi_slow_smooth[i], rsi_fast_smooth[i]])\n            pred, conf = adaptive_knn_predict(\n                features[:data_size, :], \n                labels[:data_size], \n                query,\n                k_neighbors, \n                volatility[i], \n                data_size\n            )\n            \n            # Apply confidence threshold\n            if conf >= min_confidence:\n                mlmi_values[i] = pred * 100.0  # Scale for visibility\n                mlmi_confidence[i] = conf\n            else:\n                mlmi_values[i] = 0.0\n                mlmi_confidence[i] = 0.0\n    \n    return mlmi_values, mlmi_confidence\n\n# Calculate MLMI with confidence\nprint(\"\\nCalculating adaptive MLMI with confidence scores...\")\nstart_time = time.time()\n\nreturns_30m = df_30m['Returns'].values\nmlmi_values, mlmi_confidence = calculate_mlmi_adaptive(\n    ma5, ma20, rsi5_smooth, rsi20_smooth, close_30m, returns_30m,\n    config.mlmi_k_neighbors, config.mlmi_confidence_threshold, config.mlmi_forward_bars\n)\n\n# Store in dataframe\ndf_30m['mlmi'] = mlmi_values\ndf_30m['mlmi_confidence'] = mlmi_confidence\ndf_30m['mlmi_bull'] = (mlmi_values > 0) & (mlmi_confidence > config.mlmi_confidence_threshold)\ndf_30m['mlmi_bear'] = (mlmi_values < 0) & (mlmi_confidence > config.mlmi_confidence_threshold)\n\nmlmi_time = time.time() - start_time\nprint(f\"Adaptive MLMI calculated in {mlmi_time:.3f} seconds\")\nprint(f\"MLMI range: [{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")\nprint(f\"Average confidence: {mlmi_confidence.mean():.3f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Enhanced NW-RQK with Multiple Kernels and Error Handling\n\n@njit(fastmath=True, cache=True)\ndef gaussian_kernel(x: float, h: float) -> float:\n    \"\"\"Gaussian kernel with numerical stability\"\"\"\n    if h <= 0:\n        return 0.0\n    arg = -(x * x) / (2.0 * h * h)\n    # Prevent underflow\n    if arg < -50:\n        return 0.0\n    return np.exp(arg)\n\n@njit(fastmath=True, cache=True)\ndef epanechnikov_kernel(x: float, h: float) -> float:\n    \"\"\"Epanechnikov kernel with bounds checking\"\"\"\n    if h <= 0:\n        return 0.0\n    u = x / h\n    if abs(u) <= 1:\n        return 0.75 * (1 - u * u)\n    return 0.0\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef nadaraya_watson_ensemble(prices: np.ndarray, h: float, r: float,\n                           min_periods: int = 25) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Ensemble NW regression with robust calculations\"\"\"\n    n = len(prices)\n    result_rq = np.full(n, np.nan)  # Rational Quadratic\n    result_gauss = np.full(n, np.nan)  # Gaussian\n    \n    # Input validation\n    if n < min_periods or h <= 0 or r <= 0:\n        return result_rq, result_gauss\n    \n    for i in prange(min_periods, n):\n        # Skip if price is invalid\n        if np.isnan(prices[i]):\n            continue\n            \n        # Rational Quadratic regression\n        weighted_sum_rq = 0.0\n        weight_sum_rq = 0.0\n        \n        # Gaussian regression\n        weighted_sum_gauss = 0.0\n        weight_sum_gauss = 0.0\n        \n        window_size = min(i + 1, 500)\n        \n        for j in range(window_size):\n            if i - j >= 0 and not np.isnan(prices[i - j]):\n                # Rational Quadratic with numerical stability\n                denominator = h * h * 2.0 * r\n                if denominator > 0:\n                    weight_rq = (1.0 + (j * j) / denominator) ** (-r)\n                    weighted_sum_rq += prices[i - j] * weight_rq\n                    weight_sum_rq += weight_rq\n                \n                # Gaussian\n                weight_gauss = gaussian_kernel(float(j), h)\n                if weight_gauss > 1e-10:\n                    weighted_sum_gauss += prices[i - j] * weight_gauss\n                    weight_sum_gauss += weight_gauss\n        \n        # Calculate results with minimum weight threshold\n        if weight_sum_rq > 1e-10:\n            result_rq[i] = weighted_sum_rq / weight_sum_rq\n        if weight_sum_gauss > 1e-10:\n            result_gauss[i] = weighted_sum_gauss / weight_sum_gauss\n    \n    # Ensemble: average of both kernels\n    ensemble = np.zeros(n)\n    for i in range(n):\n        count = 0\n        sum_val = 0.0\n        \n        if not np.isnan(result_rq[i]):\n            sum_val += result_rq[i]\n            count += 1\n        if not np.isnan(result_gauss[i]):\n            sum_val += result_gauss[i]\n            count += 1\n            \n        if count > 0:\n            ensemble[i] = sum_val / count\n        else:\n            ensemble[i] = np.nan\n    \n    return ensemble, result_rq\n\n@njit(fastmath=True, cache=True)\ndef detect_nwrqk_signals_enhanced(yhat1: np.ndarray, yhat2: np.ndarray,\n                                 prices: np.ndarray,\n                                 min_slope_change: float = 1e-6) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Enhanced signal detection with numerical stability\"\"\"\n    n = len(yhat1)\n    bull_signals = np.zeros(n, dtype=np.bool_)\n    bear_signals = np.zeros(n, dtype=np.bool_)\n    signal_strength = np.zeros(n)\n    \n    if n < 3:\n        return bull_signals, bear_signals, signal_strength\n    \n    for i in range(2, n):\n        if not np.isnan(yhat1[i]) and not np.isnan(yhat1[i-1]) and not np.isnan(yhat1[i-2]):\n            # Calculate slopes with safety\n            slope_prev = yhat1[i-1] - yhat1[i-2]\n            slope_curr = yhat1[i] - yhat1[i-1]\n            \n            # Acceleration\n            acceleration = slope_curr - slope_prev\n            \n            # Bullish: negative to positive slope with positive acceleration\n            if slope_prev < -min_slope_change and slope_curr > min_slope_change and acceleration > min_slope_change:\n                bull_signals[i] = True\n                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n            \n            # Bearish: positive to negative slope with negative acceleration\n            elif slope_prev > min_slope_change and slope_curr < -min_slope_change and acceleration < -min_slope_change:\n                bear_signals[i] = True\n                signal_strength[i] = min(abs(acceleration) * 1000, 1.0)\n        \n        # Crossovers with momentum\n        if i > 5 and not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]):\n            if not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]) and not np.isnan(prices[i]):\n                # Price momentum filter with safety\n                if prices[max(0, i-5)] > 0:\n                    price_momentum = (prices[i] - prices[max(0, i-5)]) / prices[max(0, i-5)]\n                    price_momentum = max(-0.5, min(0.5, price_momentum))  # Clip extreme values\n                else:\n                    price_momentum = 0.0\n                \n                # Crossover detection with threshold\n                cross_threshold = abs(yhat1[i] - yhat2[i]) * 0.001\n                \n                if yhat2[i] > yhat1[i] + cross_threshold and yhat2[i-1] <= yhat1[i-1] and price_momentum > 0:\n                    bull_signals[i] = True\n                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n                elif yhat2[i] < yhat1[i] - cross_threshold and yhat2[i-1] >= yhat1[i-1] and price_momentum < 0:\n                    bear_signals[i] = True\n                    signal_strength[i] = max(signal_strength[i], min(abs(price_momentum) * 50, 1.0))\n    \n    return bull_signals, bear_signals, signal_strength\n\n# Calculate enhanced NW-RQK\nprint(\"\\nCalculating enhanced NW-RQK with ensemble kernels...\")\nlogger.info(\"Starting NW-RQK calculation\")\nstart_time = time.time()\n\ntry:\n    # Validate parameters\n    h = max(1.0, config.nwrqk_h)\n    r = max(1.0, config.nwrqk_r)\n    lag = max(1, config.nwrqk_lag)\n    \n    # Calculate regression lines\n    yhat1, yhat1_rq = nadaraya_watson_ensemble(close_30m, h, r)\n    yhat2, yhat2_rq = nadaraya_watson_ensemble(close_30m, h - lag, r)\n    \n    # Detect signals with strength\n    nwrqk_bull, nwrqk_bear, nwrqk_strength = detect_nwrqk_signals_enhanced(\n        yhat1, yhat2, close_30m\n    )\n    \n    # Store in dataframe\n    df_30m['nwrqk_bull'] = nwrqk_bull\n    df_30m['nwrqk_bear'] = nwrqk_bear\n    df_30m['nwrqk_strength'] = nwrqk_strength\n    df_30m['yhat1'] = yhat1\n    df_30m['yhat2'] = yhat2\n    \n    nwrqk_time = time.time() - start_time\n    \n    # Log statistics\n    logger.info(f\"NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n    logger.info(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")\n    \n    valid_strength = nwrqk_strength[nwrqk_strength > 0]\n    if len(valid_strength) > 0:\n        logger.info(f\"Average signal strength: {valid_strength.mean():.3f}\")\n        logger.info(f\"Max signal strength: {valid_strength.max():.3f}\")\n    \n    print(f\"Enhanced NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n    print(f\"Bull signals: {nwrqk_bull.sum():,}, Bear signals: {nwrqk_bear.sum():,}\")\n    print(f\"Average signal strength: {nwrqk_strength[nwrqk_strength > 0].mean():.3f}\")\n    \n    # Memory cleanup\n    if check_memory_usage() > config.max_memory_gb * 0.8:\n        cleanup_memory()\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating NW-RQK: {str(e)}\")\n    # Set default values on error\n    df_30m['nwrqk_bull'] = False\n    df_30m['nwrqk_bear'] = False\n    df_30m['nwrqk_strength'] = 0.0\n    df_30m['yhat1'] = np.nan\n    df_30m['yhat2'] = np.nan\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Smart Timeframe Alignment\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef create_alignment_map(timestamps_5m: np.ndarray, timestamps_30m: np.ndarray) -> np.ndarray:\n    \"\"\"Create efficient mapping between timeframes\"\"\"\n    n_5m = len(timestamps_5m)\n    mapping = np.zeros(n_5m, dtype=np.int64)\n    \n    j = 0\n    for i in prange(n_5m):\n        # Find the corresponding 30m bar\n        while j < len(timestamps_30m) - 1 and timestamps_30m[j + 1] <= timestamps_5m[i]:\n            j += 1\n        mapping[i] = j\n    \n    return mapping\n\nprint(\"\\nPerforming smart timeframe alignment...\")\nstart_time = time.time()\n\n# Create datetime arrays for mapping\n# Convert pandas DatetimeIndex to numpy array of timestamps\ntimestamps_5m = df_5m.index.values.astype(np.int64) // 10**9\ntimestamps_30m = df_30m.index.values.astype(np.int64) // 10**9\n\n# Create mapping\nmapping = create_alignment_map(timestamps_5m, timestamps_30m)\n\n# Align all indicators efficiently\ndf_5m_aligned = df_5m.copy()\n\n# MLMI alignment with confidence\ndf_5m_aligned['mlmi'] = df_30m['mlmi'].values[mapping]\ndf_5m_aligned['mlmi_confidence'] = df_30m['mlmi_confidence'].values[mapping]\ndf_5m_aligned['mlmi_bull'] = df_30m['mlmi_bull'].values[mapping]\ndf_5m_aligned['mlmi_bear'] = df_30m['mlmi_bear'].values[mapping]\n\n# NW-RQK alignment with strength\ndf_5m_aligned['nwrqk_bull'] = df_30m['nwrqk_bull'].values[mapping]\ndf_5m_aligned['nwrqk_bear'] = df_30m['nwrqk_bear'].values[mapping]\ndf_5m_aligned['nwrqk_strength'] = df_30m['nwrqk_strength'].values[mapping]\n\n# FVG data\ndf_5m_aligned['fvg_bull'] = fvg_bull\ndf_5m_aligned['fvg_bear'] = fvg_bear\n\n# Add market regime detection\ndf_5m_aligned['volatility'] = df_5m_aligned['Returns'].rolling(20).std()\ndf_5m_aligned['trend_strength'] = abs(df_5m_aligned['Returns'].rolling(50).mean()) / df_5m_aligned['volatility']\n\nalign_time = time.time() - start_time\nprint(f\"Smart alignment completed in {align_time:.3f} seconds\")\nprint(f\"Aligned {len(df_5m_aligned):,} 5-minute bars\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: MLMI → NW-RQK → FVG Synergy Detection\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_mlmi_nwrqk_fvg_synergy(mlmi_bull: np.ndarray, mlmi_bear: np.ndarray,\n",
    "                                 mlmi_conf: np.ndarray, nwrqk_bull: np.ndarray,\n",
    "                                 nwrqk_bear: np.ndarray, nwrqk_strength: np.ndarray,\n",
    "                                 fvg_bull: np.ndarray, fvg_bear: np.ndarray,\n",
    "                                 volatility: np.ndarray, window: int = 30) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Advanced synergy detection with confidence scoring\"\"\"\n",
    "    n = len(mlmi_bull)\n",
    "    long_signals = np.zeros(n, dtype=np.bool_)\n",
    "    short_signals = np.zeros(n, dtype=np.bool_)\n",
    "    signal_quality = np.zeros(n)\n",
    "    \n",
    "    # State tracking\n",
    "    mlmi_active_bull = np.zeros(n, dtype=np.bool_)\n",
    "    mlmi_active_bear = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_confirmed_bull = np.zeros(n, dtype=np.bool_)\n",
    "    nwrqk_confirmed_bear = np.zeros(n, dtype=np.bool_)\n",
    "    state_timer = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        # Carry forward states\n",
    "        mlmi_active_bull[i] = mlmi_active_bull[i-1]\n",
    "        mlmi_active_bear[i] = mlmi_active_bear[i-1]\n",
    "        nwrqk_confirmed_bull[i] = nwrqk_confirmed_bull[i-1]\n",
    "        nwrqk_confirmed_bear[i] = nwrqk_confirmed_bear[i-1]\n",
    "        state_timer[i] = state_timer[i-1] + 1\n",
    "        \n",
    "        # Volatility adjustment\n",
    "        vol_factor = 1.0 / (1.0 + volatility[i] * 10) if not np.isnan(volatility[i]) else 1.0\n",
    "        \n",
    "        # Reset on opposite signal or timeout\n",
    "        if mlmi_bear[i] or state_timer[i] > window:\n",
    "            mlmi_active_bull[i] = False\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            if mlmi_bear[i]:\n",
    "                state_timer[i] = 0\n",
    "        \n",
    "        if mlmi_bull[i] or state_timer[i] > window:\n",
    "            mlmi_active_bear[i] = False\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            if mlmi_bull[i]:\n",
    "                state_timer[i] = 0\n",
    "        \n",
    "        # Step 1: MLMI signal with confidence filter\n",
    "        if mlmi_bull[i] and not mlmi_bull[i-1] and mlmi_conf[i] > 0.3:\n",
    "            mlmi_active_bull[i] = True\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        if mlmi_bear[i] and not mlmi_bear[i-1] and mlmi_conf[i] > 0.3:\n",
    "            mlmi_active_bear[i] = True\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        # Step 2: NW-RQK confirmation with strength filter\n",
    "        if mlmi_active_bull[i] and not nwrqk_confirmed_bull[i] and nwrqk_bull[i] and nwrqk_strength[i] > 0.2:\n",
    "            nwrqk_confirmed_bull[i] = True\n",
    "        \n",
    "        if mlmi_active_bear[i] and not nwrqk_confirmed_bear[i] and nwrqk_bear[i] and nwrqk_strength[i] > 0.2:\n",
    "            nwrqk_confirmed_bear[i] = True\n",
    "        \n",
    "        # Step 3: FVG final confirmation\n",
    "        if nwrqk_confirmed_bull[i] and fvg_bull[i]:\n",
    "            long_signals[i] = True\n",
    "            # Calculate signal quality\n",
    "            signal_quality[i] = (mlmi_conf[i] + nwrqk_strength[i]) / 2 * vol_factor\n",
    "            # Reset states\n",
    "            mlmi_active_bull[i] = False\n",
    "            nwrqk_confirmed_bull[i] = False\n",
    "            state_timer[i] = 0\n",
    "        \n",
    "        if nwrqk_confirmed_bear[i] and fvg_bear[i]:\n",
    "            short_signals[i] = True\n",
    "            # Calculate signal quality\n",
    "            signal_quality[i] = (mlmi_conf[i] + nwrqk_strength[i]) / 2 * vol_factor\n",
    "            # Reset states\n",
    "            mlmi_active_bear[i] = False\n",
    "            nwrqk_confirmed_bear[i] = False\n",
    "            state_timer[i] = 0\n",
    "    \n",
    "    return long_signals, short_signals, signal_quality\n",
    "\n",
    "print(\"\\nDetecting MLMI → NW-RQK → FVG synergy signals...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract arrays\n",
    "mlmi_bull_arr = df_5m_aligned['mlmi_bull'].values\n",
    "mlmi_bear_arr = df_5m_aligned['mlmi_bear'].values\n",
    "mlmi_conf_arr = df_5m_aligned['mlmi_confidence'].values\n",
    "nwrqk_bull_arr = df_5m_aligned['nwrqk_bull'].values\n",
    "nwrqk_bear_arr = df_5m_aligned['nwrqk_bear'].values\n",
    "nwrqk_strength_arr = df_5m_aligned['nwrqk_strength'].values\n",
    "fvg_bull_arr = df_5m_aligned['fvg_bull'].values\n",
    "fvg_bear_arr = df_5m_aligned['fvg_bear'].values\n",
    "volatility_arr = df_5m_aligned['volatility'].fillna(0.01).values\n",
    "\n",
    "# Detect synergy\n",
    "long_entries, short_entries, signal_quality = detect_mlmi_nwrqk_fvg_synergy(\n",
    "    mlmi_bull_arr, mlmi_bear_arr, mlmi_conf_arr,\n",
    "    nwrqk_bull_arr, nwrqk_bear_arr, nwrqk_strength_arr,\n",
    "    fvg_bull_arr, fvg_bear_arr, volatility_arr\n",
    ")\n",
    "\n",
    "# Add to dataframe\n",
    "df_5m_aligned['long_entry'] = long_entries\n",
    "df_5m_aligned['short_entry'] = short_entries\n",
    "df_5m_aligned['signal_quality'] = signal_quality\n",
    "\n",
    "signal_time = time.time() - start_time\n",
    "print(f\"Synergy detection completed in {signal_time:.3f} seconds\")\n",
    "print(f\"Long entries: {long_entries.sum():,}\")\n",
    "print(f\"Short entries: {short_entries.sum():,}\")\n",
    "print(f\"Average signal quality: {signal_quality[signal_quality > 0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Ultra-Fast VectorBT Backtesting with Dynamic Position Sizing\n\n@njit(fastmath=True, cache=True)\ndef generate_exit_signals_advanced(entries: np.ndarray, direction: np.ndarray, \n                                  close: np.ndarray, atr: np.ndarray,\n                                  signal_quality: np.ndarray,\n                                  max_bars: int = 100, \n                                  stop_loss_atr: float = 2.0,\n                                  take_profit_atr: float = 4.0) -> np.ndarray:\n    \"\"\"Generate exit signals with ATR-based stops and signal quality adjustments\"\"\"\n    n = len(entries)\n    exits = np.zeros(n, dtype=np.bool_)\n    \n    position_open = False\n    position_dir = 0\n    entry_idx = -1\n    entry_price = 0.0\n    entry_atr = 0.0\n    entry_quality = 0.5\n    \n    for i in range(n):\n        if position_open:\n            bars_held = i - entry_idx\n            \n            # Dynamic exit levels based on signal quality\n            quality_factor = 0.5 + entry_quality\n            stop_distance = entry_atr * stop_loss_atr / quality_factor\n            target_distance = entry_atr * take_profit_atr * quality_factor\n            \n            # Check exit conditions\n            if position_dir == 1:  # Long position\n                stop_price = entry_price - stop_distance\n                target_price = entry_price + target_distance\n                \n                if (direction[i] == -1 or \n                    bars_held >= max_bars or \n                    close[i] <= stop_price or \n                    close[i] >= target_price):\n                    exits[i] = True\n                    position_open = False\n            \n            elif position_dir == -1:  # Short position\n                stop_price = entry_price + stop_distance\n                target_price = entry_price - target_distance\n                \n                if (direction[i] == 1 or \n                    bars_held >= max_bars or \n                    close[i] >= stop_price or \n                    close[i] <= target_price):\n                    exits[i] = True\n                    position_open = False\n        \n        # Check for new entry\n        if entries[i] and not position_open:\n            position_open = True\n            position_dir = direction[i]\n            entry_idx = i\n            entry_price = close[i]\n            entry_atr = atr[i] if not np.isnan(atr[i]) else close[i] * 0.01\n            entry_quality = signal_quality[i] if not np.isnan(signal_quality[i]) else 0.5\n    \n    return exits\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ULTRA-FAST VECTORBT BACKTESTING WITH DYNAMIC RISK MANAGEMENT\")\nprint(\"=\" * 80)\n\n# Prepare data for vectorbt\nclose_prices = df_5m_aligned['Close'].values\nentries = df_5m_aligned['long_entry'] | df_5m_aligned['short_entry']\nentries_array = entries.values\ndirection = np.where(df_5m_aligned['long_entry'], 1, \n                    np.where(df_5m_aligned['short_entry'], -1, 0))\nsignal_quality_array = df_5m_aligned['signal_quality'].values\n\n# Calculate ATR for dynamic stops (approximation for speed)\natr_approx = df_5m_aligned['HL_Range'].rolling(14).mean().values\n\n# Generate exit signals with advanced logic\nprint(\"\\nGenerating dynamic exit signals...\")\nexit_start = time.time()\n\nexits = generate_exit_signals_advanced(\n    entries_array, direction, close_prices, atr_approx, signal_quality_array,\n    max_bars=config.max_holding_bars,\n    stop_loss_atr=config.stop_loss_atr,\n    take_profit_atr=config.stop_loss_atr * 2  # 2:1 reward-risk ratio\n)\n\nexit_time = time.time() - exit_start\nprint(f\"Exit signals generated in {exit_time:.3f} seconds\")\nprint(f\"Total exits: {exits.sum():,}\")\n\n# Dynamic position sizing based on signal quality\nposition_sizes = np.where(\n    entries_array,\n    config.position_size_base * (0.5 + signal_quality_array),\n    0\n)\n\nprint(\"\\nRunning vectorized backtest with dynamic sizing...\")\nbacktest_start = time.time()\n\ntry:\n    # Run backtest with vectorbt\n    portfolio = vbt.Portfolio.from_signals(\n        close=df_5m_aligned['Close'],\n        entries=entries,\n        exits=exits,\n        direction=direction,\n        size=position_sizes,\n        size_type='amount',\n        init_cash=config.initial_capital,\n        fees=config.fees,\n        slippage=config.slippage,\n        freq='5T',\n        cash_sharing=True,\n        call_seq='auto'\n    )\n    \n    backtest_time = time.time() - backtest_start\n    print(f\"\\nBacktest completed in {backtest_time:.3f} seconds!\")\n    \n    # Calculate comprehensive metrics\n    portfolio_stats = portfolio.stats()\n    returns = portfolio.returns()\n    trades = portfolio.trades.records_readable\n    \n    print(\"\\n\" + \"-\" * 50)\n    print(\"PERFORMANCE METRICS\")\n    print(\"-\" * 50)\n    \n    # Core metrics\n    print(f\"Total Return: {portfolio_stats['Total Return [%]']:.2f}%\")\n    print(f\"Annualized Return: {portfolio_stats['Annualized Return [%]']:.2f}%\")\n    print(f\"Sharpe Ratio: {portfolio_stats['Sharpe Ratio']:.2f}\")\n    print(f\"Sortino Ratio: {portfolio_stats['Sortino Ratio']:.2f}\")\n    print(f\"Calmar Ratio: {portfolio_stats['Calmar Ratio']:.2f}\")\n    print(f\"Max Drawdown: {portfolio_stats['Max Drawdown [%]']:.2f}%\")\n    print(f\"Max Drawdown Duration: {portfolio_stats['Max Drawdown Duration']} days\")\n    \n    # Trade statistics\n    print(\"\\n\" + \"-\" * 50)\n    print(\"TRADE STATISTICS\")\n    print(\"-\" * 50)\n    print(f\"Total Trades: {portfolio_stats['Total Trades']:,.0f}\")\n    print(f\"Win Rate: {portfolio_stats['Win Rate [%]']:.2f}%\")\n    print(f\"Profit Factor: {portfolio_stats['Profit Factor']:.2f}\")\n    print(f\"Expectancy: {portfolio_stats['Expectancy [%]']:.3f}%\")\n    print(f\"Average Win: {portfolio_stats['Avg Winning Trade [%]']:.2f}%\")\n    print(f\"Average Loss: {portfolio_stats['Avg Losing Trade [%]']:.2f}%\")\n    print(f\"Best Trade: {portfolio_stats['Best Trade [%]']:.2f}%\")\n    print(f\"Worst Trade: {portfolio_stats['Worst Trade [%]']:.2f}%\")\n    \n    # Position sizing analysis\n    if len(trades) > 0:\n        avg_position_size = trades['Size'].mean()\n        print(f\"\\nAverage Position Size: {avg_position_size:.2f}\")\n        print(f\"Position Size StdDev: {trades['Size'].std():.2f}\")\n        \n    # Advanced metrics\n    print(\"\\n\" + \"-\" * 50)\n    print(\"ADVANCED METRICS\")\n    print(\"-\" * 50)\n    \n    # Calculate additional metrics\n    daily_returns = returns.resample('D').apply(lambda x: (1 + x).prod() - 1)\n    \n    if len(daily_returns) > 0:\n        # Value at Risk (95%)\n        var_95 = np.percentile(daily_returns.dropna(), 5)\n        print(f\"Daily VaR (95%): {var_95*100:.2f}%\")\n        \n        # Conditional VaR (CVaR)\n        cvar_95 = daily_returns[daily_returns <= var_95].mean()\n        print(f\"Daily CVaR (95%): {cvar_95*100:.2f}%\")\n        \n        # Information Ratio (assuming 0 benchmark)\n        ir = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n        print(f\"Information Ratio: {ir:.2f}\")\n    \n    # Quality of trades by signal strength\n    if len(trades) > 0 and 'signal_quality' in df_5m_aligned.columns:\n        print(\"\\n\" + \"-\" * 50)\n        print(\"TRADE QUALITY ANALYSIS\")\n        print(\"-\" * 50)\n        \n        # Match trades to signal quality\n        trade_qualities = []\n        for idx, trade in trades.iterrows():\n            entry_idx = df_5m_aligned.index.get_loc(trade['Entry Timestamp'])\n            quality = df_5m_aligned.iloc[entry_idx]['signal_quality']\n            trade_qualities.append(quality)\n        \n        trades['Signal Quality'] = trade_qualities\n        \n        # Analyze by quality quartiles\n        quality_quartiles = pd.qcut(trades['Signal Quality'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n        quality_analysis = trades.groupby(quality_quartiles)['PnL %'].agg(['mean', 'count', 'std'])\n        \n        print(\"\\nPerformance by Signal Quality:\")\n        print(quality_analysis)\n        \n    logger.info(f\"Backtest completed successfully with {len(trades)} trades\")\n    \nexcept Exception as e:\n    logger.error(f\"Error running backtest: {str(e)}\")\n    print(f\"\\nError running backtest: {str(e)}\")\n    portfolio_stats = {}\n    returns = pd.Series(dtype=float)\n    trades = pd.DataFrame()\n    portfolio = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Professional Visualizations\n\nprint(\"\\nGenerating professional visualizations...\")\nlogger.info(\"Creating performance visualizations\")\n\n# Create comprehensive dashboard\nfig = make_subplots(\n    rows=4, cols=2,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    horizontal_spacing=0.1,\n    row_heights=[0.3, 0.2, 0.2, 0.3],\n    column_widths=[0.7, 0.3],\n    subplot_titles=(\n        'Cumulative Returns', 'Returns Distribution',\n        'Drawdown', 'Trade Quality vs Returns',\n        'Signal Indicators', 'Monthly Returns Heatmap',\n        'Price & Signals (Sample)', 'Trade Duration Analysis'\n    ),\n    specs=[\n        [{\"secondary_y\": True}, {\"type\": \"histogram\"}],\n        [{\"secondary_y\": False}, {\"type\": \"scatter\"}],\n        [{\"secondary_y\": True}, {\"type\": \"heatmap\"}],\n        [{\"secondary_y\": False}, {\"type\": \"bar\"}]\n    ]\n)\n\n# 1. Cumulative Returns with benchmark\ncumulative_returns = (1 + returns).cumprod() - 1\nbenchmark_returns = df_5m_aligned['Close'].pct_change().fillna(0)\ncumulative_benchmark = (1 + benchmark_returns).cumprod() - 1\n\nfig.add_trace(\n    go.Scatter(\n        x=cumulative_returns.index,\n        y=cumulative_returns.values * 100,\n        mode='lines',\n        name='Strategy',\n        line=dict(color='blue', width=2)\n    ),\n    row=1, col=1, secondary_y=False\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=cumulative_benchmark.index,\n        y=cumulative_benchmark.values * 100,\n        mode='lines',\n        name='Buy & Hold',\n        line=dict(color='gray', width=1, dash='dash')\n    ),\n    row=1, col=1, secondary_y=False\n)\n\n# Add cumulative trades on secondary axis\nif 'portfolio' in globals() and portfolio is not None:\n    cumulative_trades = np.arange(len(trades))\n    trade_times = trades['Entry Timestamp']\n    \n    fig.add_trace(\n        go.Scatter(\n            x=trade_times,\n            y=cumulative_trades,\n            mode='lines',\n            name='Cumulative Trades',\n            line=dict(color='green', width=1),\n            yaxis='y2'\n        ),\n        row=1, col=1, secondary_y=True\n    )\n\n# 2. Returns Distribution\nif len(returns) > 0:\n    fig.add_trace(\n        go.Histogram(\n            x=returns.values * 100,\n            nbinsx=50,\n            name='Returns',\n            marker_color='lightblue',\n            showlegend=False\n        ),\n        row=1, col=2\n    )\n    \n    # Add normal distribution overlay\n    mean_ret = returns.mean() * 100\n    std_ret = returns.std() * 100\n    x_range = np.linspace(returns.min() * 100, returns.max() * 100, 100)\n    normal_dist = stats.norm.pdf(x_range, mean_ret, std_ret) * len(returns) * (returns.max() - returns.min()) * 100 / 50\n    \n    fig.add_trace(\n        go.Scatter(\n            x=x_range,\n            y=normal_dist,\n            mode='lines',\n            name='Normal',\n            line=dict(color='red', width=2)\n        ),\n        row=1, col=2\n    )\n\n# 3. Drawdown\nif 'portfolio' in globals() and portfolio is not None:\n    drawdown = portfolio.drawdown() * 100\n    fig.add_trace(\n        go.Scatter(\n            x=drawdown.index,\n            y=-drawdown.values,\n            mode='lines',\n            name='Drawdown',\n            fill='tozeroy',\n            line=dict(color='red', width=1)\n        ),\n        row=2, col=1\n    )\n\n# 4. Trade Quality vs Returns\nif len(trades) > 0 and 'Signal Quality' in trades.columns:\n    fig.add_trace(\n        go.Scatter(\n            x=trades['Signal Quality'],\n            y=trades['PnL %'],\n            mode='markers',\n            name='Trades',\n            marker=dict(\n                size=np.abs(trades['PnL %']) * 2,\n                color=trades['PnL %'],\n                colorscale='RdYlGn',\n                cmin=-5,\n                cmax=5,\n                showscale=True,\n                colorbar=dict(title=\"PnL %\", x=1.15)\n            )\n        ),\n        row=2, col=2\n    )\n\n# 5. Signal Indicators (30m data sample)\nsample_size = min(500, len(df_30m))\nsample_30m = df_30m.tail(sample_size)\n\nfig.add_trace(\n    go.Scatter(\n        x=sample_30m.index,\n        y=sample_30m['Close'],\n        mode='lines',\n        name='Close',\n        line=dict(color='black', width=1)\n    ),\n    row=3, col=1, secondary_y=False\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=sample_30m.index,\n        y=sample_30m['mlmi'],\n        mode='lines',\n        name='MLMI',\n        line=dict(color='blue', width=1),\n        yaxis='y2'\n    ),\n    row=3, col=1, secondary_y=True\n)\n\n# Add NW-RQK lines\nif 'yhat1' in sample_30m.columns:\n    fig.add_trace(\n        go.Scatter(\n            x=sample_30m.index,\n            y=sample_30m['yhat1'],\n            mode='lines',\n            name='NW-RQK',\n            line=dict(color='orange', width=1, dash='dash')\n        ),\n        row=3, col=1, secondary_y=False\n    )\n\n# 6. Monthly Returns Heatmap\nif len(returns) > 30:\n    monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n    years = monthly_returns.index.year.unique()\n    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    \n    heatmap_data = np.full((len(years), 12), np.nan)\n    for i, year in enumerate(years):\n        year_data = monthly_returns[monthly_returns.index.year == year]\n        for j, month_ret in enumerate(year_data.values):\n            month_idx = year_data.index[j].month - 1\n            heatmap_data[i, month_idx] = month_ret * 100\n    \n    fig.add_trace(\n        go.Heatmap(\n            z=heatmap_data,\n            x=months,\n            y=years,\n            colorscale='RdYlGn',\n            zmid=0,\n            text=np.round(heatmap_data, 1),\n            texttemplate='%{text}%',\n            textfont={\"size\": 10},\n            showscale=False\n        ),\n        row=3, col=2\n    )\n\n# 7. Price & Signals Sample\nsample_size_5m = min(1000, len(df_5m_aligned))\nsample_5m = df_5m_aligned.tail(sample_size_5m)\n\nfig.add_trace(\n    go.Candlestick(\n        x=sample_5m.index,\n        open=sample_5m['Open'],\n        high=sample_5m['High'],\n        low=sample_5m['Low'],\n        close=sample_5m['Close'],\n        name='Price',\n        showlegend=False\n    ),\n    row=4, col=1\n)\n\n# Add entry signals\nlong_entries_sample = sample_5m[sample_5m['long_entry']]\nshort_entries_sample = sample_5m[sample_5m['short_entry']]\n\nif len(long_entries_sample) > 0:\n    fig.add_trace(\n        go.Scatter(\n            x=long_entries_sample.index,\n            y=long_entries_sample['Low'] * 0.995,\n            mode='markers',\n            name='Long',\n            marker=dict(symbol='triangle-up', size=10, color='green')\n        ),\n        row=4, col=1\n    )\n\nif len(short_entries_sample) > 0:\n    fig.add_trace(\n        go.Scatter(\n            x=short_entries_sample.index,\n            y=short_entries_sample['High'] * 1.005,\n            mode='markers',\n            name='Short',\n            marker=dict(symbol='triangle-down', size=10, color='red')\n        ),\n        row=4, col=1\n    )\n\n# 8. Trade Duration Analysis\nif len(trades) > 0 and 'Duration' in trades.columns:\n    durations = trades['Duration'].dt.total_seconds() / 3600  # Convert to hours\n    \n    fig.add_trace(\n        go.Bar(\n            x=['< 1h', '1-2h', '2-4h', '4-8h', '8-24h', '> 24h'],\n            y=[\n                len(durations[durations < 1]),\n                len(durations[(durations >= 1) & (durations < 2)]),\n                len(durations[(durations >= 2) & (durations < 4)]),\n                len(durations[(durations >= 4) & (durations < 8)]),\n                len(durations[(durations >= 8) & (durations < 24)]),\n                len(durations[durations >= 24])\n            ],\n            name='Trade Count',\n            marker_color='lightblue'\n        ),\n        row=4, col=2\n    )\n\n# Update layout\nfig.update_layout(\n    title={\n        'text': 'MLMI → NW-RQK → FVG Synergy Strategy Performance Dashboard',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 20}\n    },\n    height=1600,\n    showlegend=True,\n    template='plotly_white',\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.02,\n        xanchor=\"right\",\n        x=1\n    )\n)\n\n# Update axes\nfig.update_yaxes(title_text=\"Return (%)\", row=1, col=1, secondary_y=False)\nfig.update_yaxes(title_text=\"Trades\", row=1, col=1, secondary_y=True)\nfig.update_xaxes(title_text=\"Return (%)\", row=1, col=2)\nfig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\nfig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\nfig.update_xaxes(title_text=\"Signal Quality\", row=2, col=2)\nfig.update_yaxes(title_text=\"Trade PnL (%)\", row=2, col=2)\nfig.update_yaxes(title_text=\"Price\", row=3, col=1, secondary_y=False)\nfig.update_yaxes(title_text=\"MLMI\", row=3, col=1, secondary_y=True)\nfig.update_yaxes(title_text=\"Price\", row=4, col=1)\nfig.update_xaxes(title_text=\"Duration\", row=4, col=2)\nfig.update_yaxes(title_text=\"Count\", row=4, col=2)\n\n# Show the figure\nfig.show()\n\nprint(\"\\nVisualization complete!\")\n\n# Save the figure if configured\nif config.save_results:\n    fig_path = os.path.join(config.results_path, 'performance_dashboard.html')\n    fig.write_html(fig_path)\n    logger.info(f\"Saved performance dashboard to {fig_path}\")\n    print(f\"Dashboard saved to {fig_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Statistical Validation and Robustness Testing\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef bootstrap_confidence_intervals(returns: np.ndarray, n_bootstrap: int = 10000,\n                                  confidence: float = 0.95) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Bootstrap confidence intervals with robust statistics\"\"\"\n    n = len(returns)\n    \n    # Arrays to store bootstrap results\n    boot_returns = np.zeros(n_bootstrap)\n    boot_sharpes = np.zeros(n_bootstrap)\n    boot_max_dd = np.zeros(n_bootstrap)\n    boot_win_rates = np.zeros(n_bootstrap)\n    \n    # Remove NaN values\n    clean_returns = returns[~np.isnan(returns)]\n    n_clean = len(clean_returns)\n    \n    if n_clean == 0:\n        return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n    \n    # Bootstrap iterations\n    for i in prange(n_bootstrap):\n        # Resample with replacement (without setting seed in parallel loop)\n        indices = np.random.randint(0, n_clean, size=n_clean)\n        sample = clean_returns[indices]\n        \n        # Calculate metrics with safety checks\n        boot_returns[i] = np.prod(1 + sample) - 1\n        \n        mean_ret = np.mean(sample)\n        std_ret = np.std(sample)\n        if std_ret > 1e-10:\n            boot_sharpes[i] = mean_ret / std_ret * np.sqrt(252 * 78)\n        else:\n            boot_sharpes[i] = 0.0\n        \n        # Max drawdown\n        cum_ret = np.cumprod(1 + sample)\n        running_max = np.maximum.accumulate(cum_ret)\n        dd = np.where(running_max > 0, (cum_ret - running_max) / running_max, 0)\n        boot_max_dd[i] = np.min(dd)\n        \n        # Win rate\n        boot_win_rates[i] = np.mean(sample > 0)\n    \n    return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STATISTICAL VALIDATION & ROBUSTNESS TESTING\")\nprint(\"=\" * 80)\n\n# Bootstrap analysis\nprint(\"\\nRunning bootstrap analysis (10,000 iterations)...\")\nlogger.info(\"Starting bootstrap analysis\")\nboot_start = time.time()\n\ntry:\n    returns_array = returns.values\n    boot_returns, boot_sharpes, boot_max_dd, boot_win_rates = bootstrap_confidence_intervals(returns_array)\n    \n    boot_time = time.time() - boot_start\n    logger.info(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n    print(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n    \n    # Calculate confidence intervals\n    def calculate_ci(data, confidence=0.95):\n        \"\"\"Calculate confidence interval with safety checks\"\"\"\n        valid_data = data[~np.isnan(data)]\n        if len(valid_data) == 0:\n            return 0.0, 0.0\n        lower = np.percentile(valid_data, (1 - confidence) / 2 * 100)\n        upper = np.percentile(valid_data, (1 + confidence) / 2 * 100)\n        return lower, upper\n    \n    # Display results\n    print(\"\\n95% Confidence Intervals:\")\n    print(\"-\" * 50)\n    \n    ret_lower, ret_upper = calculate_ci(boot_returns)\n    print(f\"Total Return: [{ret_lower*100:.2f}%, {ret_upper*100:.2f}%]\")\n    \n    sharpe_lower, sharpe_upper = calculate_ci(boot_sharpes)\n    print(f\"Sharpe Ratio: [{sharpe_lower:.2f}, {sharpe_upper:.2f}]\")\n    \n    dd_lower, dd_upper = calculate_ci(boot_max_dd)\n    print(f\"Max Drawdown: [{dd_lower*100:.2f}%, {dd_upper*100:.2f}%]\")\n    \n    wr_lower, wr_upper = calculate_ci(boot_win_rates)\n    print(f\"Win Rate: [{wr_lower*100:.2f}%, {wr_upper*100:.2f}%]\")\n    \n    # Statistical significance tests\n    print(\"\\n\" + \"-\" * 50)\n    print(\"STATISTICAL SIGNIFICANCE\")\n    print(\"-\" * 50)\n    \n    # Test if returns are significantly different from zero\n    clean_returns = returns_array[~np.isnan(returns_array)]\n    if len(clean_returns) > 1:\n        mean_return = np.mean(clean_returns)\n        std_return = np.std(clean_returns)\n        n_returns = len(clean_returns)\n        \n        if std_return > 0:\n            t_stat = mean_return / (std_return / np.sqrt(n_returns))\n            # Approximate p-value using normal distribution\n            p_value_approx = 2 * (1 - stats.norm.cdf(abs(t_stat)))\n            \n            print(f\"T-statistic: {t_stat:.3f}\")\n            print(f\"Approx p-value: {p_value_approx:.4f}\")\n            print(f\"Returns significantly positive: {'Yes' if t_stat > 1.96 else 'No'}\")\n        else:\n            print(\"Cannot calculate t-statistic: zero standard deviation\")\n    \n    # Risk-adjusted performance percentiles\n    if 'portfolio_stats' in globals() and portfolio_stats and 'Sharpe Ratio' in portfolio_stats:\n        actual_sharpe = portfolio_stats['Sharpe Ratio']\n    else:\n        actual_sharpe = 0\n        \n    sharpe_percentile = np.sum(boot_sharpes <= actual_sharpe) / len(boot_sharpes) * 100\n    \n    print(f\"\\nStrategy Sharpe ratio percentile: {sharpe_percentile:.1f}%\")\n    print(f\"Performance assessment: \", end=\"\")\n    if sharpe_percentile > 90:\n        print(\"EXCELLENT - Top 10% performance\")\n    elif sharpe_percentile > 75:\n        print(\"VERY GOOD - Top 25% performance\")\n    elif sharpe_percentile > 50:\n        print(\"GOOD - Above median performance\")\n    else:\n        print(\"NEEDS IMPROVEMENT - Below median performance\")\n    \n    # Stability analysis\n    print(\"\\n\" + \"-\" * 50)\n    print(\"STABILITY ANALYSIS\")\n    print(\"-\" * 50)\n    \n    # Rolling performance\n    window = min(252 * 5, len(returns) // 2)  # 1 year of 5-minute bars or half the data\n    if window > 100:\n        rolling_returns = returns.rolling(window).apply(lambda x: (1 + x).prod() - 1)\n        rolling_sharpe = returns.rolling(window).apply(\n            lambda x: x.mean() / x.std() * np.sqrt(252 * 78) if x.std() > 0 else 0\n        )\n        \n        print(f\"Rolling 1-year return volatility: {rolling_returns.std()*100:.2f}%\")\n        print(f\"Rolling Sharpe stability: {rolling_sharpe.std():.2f}\")\n        print(f\"Minimum rolling Sharpe: {rolling_sharpe.min():.2f}\")\n        print(f\"Maximum rolling Sharpe: {rolling_sharpe.max():.2f}\")\n    else:\n        print(\"Insufficient data for rolling analysis\")\n    \n    # Save validation results\n    if config.save_results:\n        validation_results = {\n            'confidence_intervals': {\n                'return': (ret_lower, ret_upper),\n                'sharpe': (sharpe_lower, sharpe_upper),\n                'max_dd': (dd_lower, dd_upper),\n                'win_rate': (wr_lower, wr_upper)\n            },\n            'significance': {\n                't_stat': t_stat if 't_stat' in locals() else None,\n                'significant': t_stat > 1.96 if 't_stat' in locals() else False\n            },\n            'percentiles': {\n                'sharpe_percentile': sharpe_percentile\n            }\n        }\n        \n        validation_path = os.path.join(config.results_path, 'validation_results.json')\n        with open(validation_path, 'w') as f:\n            json.dump(validation_results, f, indent=2, default=str)\n        logger.info(f\"Saved validation results to {validation_path}\")\n    \nexcept Exception as e:\n    logger.error(f\"Error in statistical validation: {str(e)}\")\n    print(f\"\\nError in statistical validation: {str(e)}\")\n    print(\"Continuing with limited validation...\")\n\n# Initialize boot_time if bootstrap failed\nif 'boot_time' not in locals():\n    boot_time = 0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}